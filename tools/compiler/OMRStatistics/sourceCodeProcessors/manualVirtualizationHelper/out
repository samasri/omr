/*******************************************************************************
* Copyright (c) 2000, 2018 IBM Corp. and others
*
* This program and the accompanying materials are made available under
* the terms of the Eclipse Public License 2.0 which accompanies this
* distribution and is available at http://eclipse.org/legal/epl-2.0
* or the Apache License, Version 2.0 which accompanies this distribution
* and is available at https://www.apache.org/licenses/LICENSE-2.0.
*
* This Source Code may also be made available under the following Secondary
* Licenses when the conditions for such availability set forth in the
* Eclipse Public License, v. 2.0 are satisfied: GNU General Public License,
* version 2 with the GNU Classpath Exception [1] and GNU General Public
* License, version 2 with the OpenJDK Assembly Exception [2].
*
* [1] https://www.gnu.org/software/classpath/license.html
* [2] http://openjdk.java.net/legal/assembly-exception.html
*
* SPDX-License-Identifier: EPL-2.0 OR Apache-2.0 OR GPL-2.0 WITH Classpath-exception-2.0 OR LicenseRef-GPL-2.0 WITH Assembly-exception
*******************************************************************************/

#ifndef OMR_POWER_CODEGENERATOR_INCL
#define OMR_POWER_CODEGENERATOR_INCL

/*
* The following #define and typedef must appear before any #includes in this file
*/
#ifndef OMR_CODEGENERATOR_CONNECTOR
#define OMR_CODEGENERATOR_CONNECTOR
namespace OMR { namespace Power { class CodeGenerator; } }
namespace OMR { typedef OMR::Power::CodeGenerator CodeGeneratorConnector; }
#else
#error OMR::Power::CodeGenerator expected to be a primary connector, but an OMR connector is already defined
#endif

#include "compiler/codegen/OMRCodeGenerator.hpp"

#include "codegen/InstOpCode.hpp"              // for InstOpCode, etc
#include "codegen/Machine.hpp"                 // for LOWER_IMMED, etc
#include "codegen/RealRegister.hpp"            // for RealRegister, etc
#include "codegen/ScratchRegisterManager.hpp"
#include "compile/SymbolReferenceTable.hpp"
#include "env/CPU.hpp"                         // for Cpu
#include "env/PersistentInfo.hpp"              // for PersistentInfo
#include "env/Processors.hpp"
#include "env/TypedAllocator.hpp"
#include "env/jittypes.h"                      // for intptrj_t
#include "infra/BitVector.hpp"                 // for TR_BitVector
#include "infra/TRlist.hpp"
#include "optimizer/DataFlowAnalysis.hpp"

#include "codegen/RegisterPair.hpp"
#include "codegen/MemoryReference.hpp"

class TR_BackingStore;
class TR_PPCLoadLabelItem;
class TR_PPCOutOfLineCodeSection;
class TR_PPCScratchRegisterManager;
namespace TR { class CodeGenerator; }
namespace TR { class ConstantDataSnippet; }
namespace TR { class PPCImmInstruction; }
namespace TR { class Snippet; }
namespace TR { struct PPCLinkageProperties; }

extern TR::Instruction *loadAddressConstantInSnippet(TR::CodeGenerator *cg,
TR::Node        *node,
intptrj_t      address,
TR::Register    *targetRegister,
TR::Register    *tempRegister,
TR::InstOpCode::Mnemonic  opCode,
bool           isUnloadablePicSite=false,
TR::Instruction *cursor=NULL);

extern TR::Instruction *loadAddressConstant(TR::CodeGenerator *cg,
TR::Node        *node,
intptrj_t         value,
TR::Register    *targetRegister,
TR::Instruction *cursor=NULL,
bool            isPicSite=false,
int16_t         typeAddress = -1);

extern TR::Instruction *loadActualConstant(TR::CodeGenerator *cg,
TR::Node        *node,
intptrj_t       value,
TR::Register    *targetRegister,
TR::Instruction *cursor=NULL,
bool            isPicSite=false);

extern TR::Instruction *loadConstant(TR::CodeGenerator *cg,
TR::Node        *node,
int32_t         value,
TR::Register    *targetRegister,
TR::Instruction *cursor=NULL,
bool            isPicSite=false);

extern TR::Instruction *loadConstant(TR::CodeGenerator *cg,
TR::Node        *node,
int64_t         value,
TR::Register    *targetRegister,
TR::Instruction *cursor=NULL,
bool            isPicSite=false,
bool            useTOC=true);

extern TR::Instruction *fixedSeqMemAccess(TR::CodeGenerator *cg,
TR::Node          *node,
intptrj_t         addr,
TR::Instruction  **nibbles,
TR::Register      *srcOrTrg,
TR::Register      *baseReg,
TR::InstOpCode::Mnemonic     opCode,
int32_t           opSize,
TR::Instruction   *cursor=NULL,
TR::Register      *tempReg=NULL);

extern uint8_t *storeArgumentItem(TR::InstOpCode::Mnemonic       op,
uint8_t            *buffer,
TR::RealRegister *reg,
int32_t             offset,
TR::CodeGenerator *cg);

extern uint8_t *loadArgumentItem(TR::InstOpCode::Mnemonic       op,
uint8_t            *buffer,
TR::RealRegister *reg,
int32_t             offset,
TR::CodeGenerator *cg);

extern intptrj_t findCCLocalPPCHelperTrampoline(int32_t helperIndex, void *target, void*callSite, void*fe);

#if defined(TR_HOST_POWER)
void ppcCodeSync(uint8_t * start, uint32_t size);
#endif


struct TR_PPCBinaryEncodingData : public TR_BinaryEncodingData
{
int32_t estimate;
TR::Instruction *cursorInstruction;
TR::Instruction *jitTojitStart;
TR::Instruction *preProcInstruction;
TR::Recompilation *recomp;
};

#include "il/Node.hpp"
#include "p/codegen/PPCOutOfLineCodeSection.hpp"


namespace OMR
{

namespace Power
{

class CodeGenerator;

class OMR_EXTENSIBLE CodeGenerator : public OMR::CodeGenerator
{

public:

List<TR_BackingStore> * conversionBuffer;
ListIterator<TR_BackingStore> * conversionBufferIt;
TR_BackingStore * allocateStackSlot();

CodeGenerator();

TR::Linkage *createLinkage(TR_LinkageConventions lc);

static bool supportsTransientPrefetch();

bool is64BitProcessor();

bool getSupportsIbyteswap();

void generateBinaryEncodingPrologue(TR_PPCBinaryEncodingData *data);

void beginInstructionSelection();
void endInstructionSelection();
void doRegisterAssignment(TR_RegisterKinds kindsToAssign);
void doBinaryEncoding();
void doPeephole();
virtual TR_RegisterPressureSummary *calculateRegisterPressure();
void deleteInst(TR::Instruction* old);
TR::Instruction *generateNop(TR::Node *n, TR::Instruction *preced = 0, TR_NOPKind nopKind=TR_NOPStandard);
TR::Instruction *generateGroupEndingNop(TR::Node *node , TR::Instruction *preced = 0);
TR::Instruction *generateProbeNop(TR::Node *node , TR::Instruction *preced = 0);

bool isSnippetMatched(TR::Snippet *, int32_t, TR::SymbolReference *);

bool mulDecompositionCostIsJustified(int numOfOperations, char bitPosition[], char operationType[], int64_t value);

void emitDataSnippets();
bool hasDataSnippets();

TR::Instruction *generateSwitchToInterpreterPrePrologue(TR::Instruction *cursor, TR::Node *node);

int32_t setEstimatedLocationsForDataSnippetLabels(int32_t estimatedSnippetStart);

#ifdef DEBUG
void dumpDataSnippets(TR::FILE *outFile);
#endif

int32_t findOrCreateFloatConstant(void *v, TR::DataType t,
TR::Instruction *n0, TR::Instruction *n1,
TR::Instruction *n2, TR::Instruction *n3);
int32_t findOrCreateAddressConstant(void *v, TR::DataType t,
TR::Instruction *n0, TR::Instruction *n1,
TR::Instruction *n2, TR::Instruction *n3,
TR::Node *node, bool isUnloadablePicSite);

// different from evaluateNode in that it returns a clobberable register
TR::Register *gprClobberEvaluate(TR::Node *node);

const TR::PPCLinkageProperties &getProperties() { return *_linkageProperties; }

using OMR::CodeGenerator::apply16BitLabelRelativeRelocation;
void apply16BitLabelRelativeRelocation(int32_t * cursor, TR::LabelSymbol *);
void apply24BitLabelRelativeRelocation(int32_t * cursor, TR::LabelSymbol *);
void apply16BitLoadLabelRelativeRelocation(TR::Instruction *liInstruction, TR::LabelSymbol *startLabel, TR::LabelSymbol *endLabel, int32_t deltaToStartLabel);
void apply64BitLoadLabelRelativeRelocation(TR::Instruction *lastInstruction, TR::LabelSymbol *label);

TR::RealRegister *getStackPointerRegister()
{
return _stackPtrRegister;
}
TR::RealRegister *setStackPointerRegister(TR::RealRegister *r) {return (_stackPtrRegister = r);}

TR::RealRegister *getMethodMetaDataRegister()                       {return _methodMetaDataRegister;}
TR::RealRegister *setMethodMetaDataRegister(TR::RealRegister *r) {return (_methodMetaDataRegister = r);}

TR::RealRegister *getTOCBaseRegister()                       {return _tocBaseRegister;}
TR::RealRegister *setTOCBaseRegister(TR::RealRegister *r)  {return (_tocBaseRegister = r);}

uintptrj_t *getTOCBase();

TR_PPCScratchRegisterManager* generateScratchRegisterManager(int32_t capacity = 32);

void buildRegisterMapForInstruction(TR_GCStackMap *map);

int32_t getPreferredLoopUnrollFactor();

bool canTransformUnsafeCopyToArrayCopy() { return true; }
bool canTransformUnsafeSetMemory();

bool processInstruction(TR::Instruction *instr, TR_BitVector ** registerUsageInfo, int32_t &blockNum, int32_t &isFence, bool traceIt);
uint32_t isPreservedRegister(int32_t regIndex);
bool isReturnInstruction(TR::Instruction *instr);
bool isBranchInstruction(TR::Instruction *instr);
bool isLabelInstruction(TR::Instruction *instr);
int32_t isFenceInstruction(TR::Instruction *instr);
bool isAlignmentInstruction(TR::Instruction *instr);
TR::Instruction *splitEdge(TR::Instruction *cursor, bool isFallThrough, bool needsJump, TR::Instruction *newSplitLabel, TR::list<TR::Instruction*> *jmpInstrs, bool firstJump = false);
TR::Instruction *splitBlockEntry(TR::Instruction *instr);
int32_t computeRegisterSaveDescription(TR_BitVector *regs, bool populateInfo = false);
void processIncomingParameterUsage(TR_BitVector **registerUsageInfo, int32_t blockNum);
void updateSnippetMapWithRSD(TR::Instruction *cur, int32_t rsd);
bool isTargetSnippetOrOutOfLine(TR::Instruction *instr, TR::Instruction **start, TR::Instruction **end);
virtual bool supportsAESInstructions();

virtual bool getSupportsTLE()
{
return false;
}

bool getSupportsOpCodeForAutoSIMD(TR::ILOpCode, TR::DataType);

bool getSupportsEncodeUtf16LittleWithSurrogateTest();

bool getSupportsEncodeUtf16BigWithSurrogateTest();

// Active counter used to track control flow basic blocks generated at instruction selection.
int32_t _nextAvailableBlockIndex;
// The index of the current basic block (used and updated during instruction selection).
int32_t _currentBlockIndex;

void setNextAvailableBlockIndex(int32_t blockIndex) { _nextAvailableBlockIndex = blockIndex; }
int32_t getNextAvailableBlockIndex() { return _currentBlockIndex = _nextAvailableBlockIndex; }
void incNextAvailableBlockIndex() { _nextAvailableBlockIndex++; }

void setCurrentBlockIndex(int32_t blockIndex) { _currentBlockIndex = blockIndex; }
int32_t getCurrentBlockIndex() { return _currentBlockIndex; }
int32_t arrayInitMinimumNumberOfBytes() {return 32;}

TR::SymbolReference &getDouble2LongSymbolReference()  { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCdouble2Long, false, false, false); }
TR::SymbolReference &getDoubleRemainderSymbolReference()  { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCdoubleRemainder, false, false, false); }
TR::SymbolReference &getInteger2DoubleSymbolReference()  { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinteger2Double, false, false, false); }
TR::SymbolReference &getLong2DoubleSymbolReference()  { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPClong2Double, false, false, false); }
TR::SymbolReference &getLong2FloatSymbolReference()  { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPClong2Float, false, false, false); }
TR::SymbolReference &getLong2Float_mvSymbolReference()  { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPClong2Float_mv, false, false, false); }
TR::SymbolReference &getLongMultiplySymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPClongMultiply, false, false, false); }
TR::SymbolReference &getLongDivideSymbolReference()   { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPClongDivide, false, false, false); }
TR::SymbolReference &getUnresolvedStaticGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterUnresolvedStaticGlue, false, false, false); }
TR::SymbolReference &getUnresolvedSpecialGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterUnresolvedSpecialGlue, false, false, false); }
TR::SymbolReference &getUnresolvedDirectVirtualGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterUnresolvedDirectVirtualGlue, false, false, false); }
TR::SymbolReference &getUnresolvedClassGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterUnresolvedClassGlue, false, false, false); }
TR::SymbolReference &getUnresolvedClassGlue2SymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterUnresolvedClassGlue2, false, false, false); }
TR::SymbolReference &getUnresolvedStringGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterUnresolvedStringGlue, false, false, false); }
TR::SymbolReference &getUnresolvedStaticDataGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterUnresolvedStaticDataGlue, false, false, false); }
TR::SymbolReference &getUnresolvedStaticDataStoreGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterUnresolvedStaticDataStoreGlue, false, false, false); }
TR::SymbolReference &getUnresolvedInstanceDataGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterUnresolvedInstanceDataGlue, false, false, false); }
TR::SymbolReference &getUnresolvedInstanceDataStoreGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterUnresolvedInstanceDataStoreGlue, false, false, false); }
TR::SymbolReference &getVirtualUnresolvedHelperSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCvirtualUnresolvedHelper, false, false, false); }
TR::SymbolReference &getInterfaceCallHelperSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterfaceCallHelper, false, false, false); }
TR::SymbolReference &getItrgSendVirtual0SymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCicallVMprJavaSendVirtual0, false, false, false); }
TR::SymbolReference &getItrgSendVirtual1SymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCicallVMprJavaSendVirtual1, false, false, false); }
TR::SymbolReference &getItrgSendVirtualJSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCicallVMprJavaSendVirtualJ, false, false, false); }
TR::SymbolReference &getItrgSendVirtualFSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCicallVMprJavaSendVirtualF, false, false, false); }
TR::SymbolReference &getItrgSendVirtualDSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCicallVMprJavaSendVirtualD, false, false, false); }
TR::SymbolReference &getVoidStaticGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterVoidStaticGlue, false, false, false); }
TR::SymbolReference &getSyncVoidStaticGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterSyncVoidStaticGlue, false, false, false); }
TR::SymbolReference &getGPR3StaticGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterGPR3StaticGlue, false, false, false); }
TR::SymbolReference &getSyncGPR3StaticGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterSyncGPR3StaticGlue, false, false, false); }
TR::SymbolReference &getGPR3GPR4StaticGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterGPR3GPR4StaticGlue, false, false, false); }
TR::SymbolReference &getSyncGPR3GPR4StaticGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterSyncGPR3GPR4StaticGlue, false, false, false); }
TR::SymbolReference &getFPR0FStaticGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterFPR0FStaticGlue, false, false, false); }
TR::SymbolReference &getSyncFPR0FStaticGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterSyncFPR0FStaticGlue, false, false, false); }
TR::SymbolReference &getFPR0DStaticGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterFPR0DStaticGlue, false, false, false); }
TR::SymbolReference &getSyncFPR0DStaticGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinterpreterSyncFPR0DStaticGlue, false, false, false); }
TR::SymbolReference &getNativeStaticHelperForUnresolvedGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCnativeStaticHelperForUnresolvedGlue, false, false, false); }
TR::SymbolReference &getNativeStaticHelperSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCnativeStaticHelper, false, false, false); }
TR::SymbolReference &getCollapseJNIReferenceFrameSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCcollapseJNIReferenceFrame, false, false, false); }
TR::SymbolReference &getArrayCopySymbolReference();
TR::SymbolReference &getWordArrayCopySymbolReference();
TR::SymbolReference &getHalfWordArrayCopySymbolReference();
TR::SymbolReference &getForwardArrayCopySymbolReference();
TR::SymbolReference &getForwardWordArrayCopySymbolReference();
TR::SymbolReference &getForwardHalfWordArrayCopySymbolReference();
TR::SymbolReference &getReferenceArrayCopySymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCreferenceArrayCopy, false, false, false); }
TR::SymbolReference &getGeneralArrayCopySymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCgeneralArrayCopy, false, false, false); }
TR::SymbolReference &getArrayTranslateTRTOSimpleVMXSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCarrayTranslateTRTOSimpleVMX, false, false, false); }
TR::SymbolReference &getArrayCmpVMXSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCarrayCmpVMX, false, false, false); }
TR::SymbolReference &getArrayCmpLenVMXSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCarrayCmpLenVMX, false, false, false); }
TR::SymbolReference &getArrayCmpScalarSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCarrayCmpScalar, false, false, false); }
TR::SymbolReference &getArrayCmpLenScalarSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCarrayCmpLenScalar, false, false, false); }
TR::SymbolReference &getSamplingPatchCallSiteSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCsamplingPatchCallSite, false, false, false); }
TR::SymbolReference &getSamplingRecompileMethodSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCsamplingRecompileMethod, false, false, false); }
TR::SymbolReference &getCountingPatchCallSiteSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCcountingPatchCallSite, false, false, false); }
TR::SymbolReference &getCountingRecompileMethodSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCcountingRecompileMethod, false, false, false); }
TR::SymbolReference &getInduceRecompilationSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCinduceRecompilation, false, false, false); }
TR::SymbolReference &getRevertToInterpreterGlueSymbolReference() { return *_symRefTab->findOrCreateRuntimeHelper(TR_PPCrevertToInterpreterGlue, false, false, false); }

bool hasCall()                 {return _flags.testAny(HasCall);}
bool noStackFrame()            {return _flags.testAny(NoStackFrame);}
bool canExceptByTrap()         {return _flags.testAny(CanExceptByTrap);}
bool enableTLHPrefetching()    {return _flags.testAny(EnableTLHPrefetching);}
virtual bool isOutOfLineHotPath()      {return _flags.testAny(IsOutOfLineHotPath);}

void setHasCall()              { _flags.set(HasCall);}
void setNoStackFrame()         { _flags.set(NoStackFrame);}
void setCanExceptByTrap()      { _flags.set(CanExceptByTrap);}
void setEnableTLHPrefetching() { _flags.set(EnableTLHPrefetching);}
void setIsOutOfLineHotPath(bool v)   { _flags.set(IsOutOfLineHotPath, v);}

void setIsDualTLH()           { _flags.set(IsDualTLH);}
bool isDualTLH()              {return _flags.testAny(IsDualTLH);}

TR_BitVector  *getBlockCallInfo() {return _blockCallInfo;}
TR_BitVector  *setBlockCallInfo(TR_BitVector *v) {return (_blockCallInfo = v);}

TR_Array<TR::Register *> &getTransientLongRegisters() {return _transientLongRegisters;}
void freeAndResetTransientLongs();

TR_Array<TR::SymbolReference *> *getTrackStatics() {return _trackStatics;}
void setTrackStatics(TR_Array<TR::SymbolReference *> *p) {_trackStatics = p;}
void staticTracking(TR::SymbolReference *symRef);

TR_Array<TR_PPCLoadLabelItem *> *getTrackItems() {return _trackItems;}
void setTrackItems(TR_Array<TR_PPCLoadLabelItem *> *p) {_trackItems = p;}
void itemTracking(int32_t offset, TR::LabelSymbol *sym);

void setRealRegisterAssociation(TR::Register *vr, TR::RealRegister::RegNum rn);
void addRealRegisterInterference(TR::Register *vr, TR::RealRegister::RegNum rn);

TR_GlobalRegisterNumber pickRegister(TR_RegisterCandidate *, TR::Block * *, TR_BitVector & availableRegisters, TR_GlobalRegisterNumber &, TR_LinkHead<TR_RegisterCandidate> *candidates);
bool allowGlobalRegisterAcrossBranch(TR_RegisterCandidate *, TR::Node * branchNode);
using OMR::CodeGenerator::getMaximumNumberOfGPRsAllowedAcrossEdge;
int32_t getMaximumNumberOfGPRsAllowedAcrossEdge(TR::Node *);
int32_t getMaximumNumberOfFPRsAllowedAcrossEdge(TR::Node *);
bool isGlobalRegisterAvailable(TR_GlobalRegisterNumber i, TR::DataType dt);

TR_BitVector _globalRegisterBitVectors[TR_numSpillKinds];
virtual TR_BitVector *getGlobalRegisters(TR_SpillKinds kind, TR_LinkageConventions lc){ return &_globalRegisterBitVectors[kind]; }

TR_GlobalRegisterNumber _gprLinkageGlobalRegisterNumbers[TR::RealRegister::NumRegisters], _fprLinkageGlobalRegisterNumbers[TR::RealRegister::NumRegisters]; // these could be smaller
TR_GlobalRegisterNumber getLinkageGlobalRegisterNumber(int8_t linkageRegisterIndex, TR::DataType type);

virtual void simulateNodeEvaluation(TR::Node *node, TR_RegisterPressureState *state, TR_RegisterPressureSummary *summary);

bool internalPointerSupportImplemented() {return true;}

bool materializesLargeConstants() { return true;}

bool needsNormalizationBeforeShifts();

bool getSupportsBitOpCodes() {return true;}

bool supportsSinglePrecisionSQRT();
bool supportsFusedMultiplyAdd() {return true;}
bool supportsNegativeFusedMultiplyAdd() {return true;}

bool getSupportsTenuredObjectAlignment() { return true; }
bool isObjectOfSizeWorthAligning(uint32_t size)
{
uint32_t lineSize = 64;
return  ((size < (lineSize<<1)) && (size > (lineSize >> 2)));
}

using OMR::CodeGenerator::getSupportsConstantOffsetInAddressing;
bool getSupportsConstantOffsetInAddressing(int64_t value) { return (value>=LOWER_IMMED) && (value<=UPPER_IMMED);}

bool supportsPassThroughCopyToNewVirtualRegister() { return true; }

bool branchesAreExpensive()
{
// Currently returning true
// Should be changed on PPC to account for processors with good
// branch prediction (return false in that case)
//
return true;
}

TR::RealRegister *regMaskToRealRegister(TR_RegisterMask mask, TR_RegisterKinds rk);

static uint32_t registerBitMask(int32_t reg);

int32_t getInternalPtrMapBit() { return 18;}

int32_t getMaximumNumbersOfAssignableGPRs();
int32_t getMaximumNumbersOfAssignableFPRs();
int32_t getMaximumNumbersOfAssignableVRs();
bool doRematerialization();

bool isRotateAndMask(TR::Node * node);

// PPC specific thresholds for constant re-materialization
int64_t getLargestNegConstThatMustBeMaterialized() {return -32769;}  // minimum 16-bit signed int minux 1
int64_t getSmallestPosConstThatMustBeMaterialized() {return 32768;}  // maximum 16-bit signed int plus 1
bool shouldValueBeInACommonedNode(int64_t); // no virt, cast

bool ilOpCodeIsSupported(TR::ILOpCodes);
// Constant Data update
bool checkAndFetchRequestor(TR::Instruction *instr, TR::Instruction **q);

// OutOfLineCodeSection List functions
TR::list<TR_PPCOutOfLineCodeSection*> &getPPCOutOfLineCodeSectionList() {return _outOfLineCodeSectionList;}
TR_PPCOutOfLineCodeSection *findOutLinedInstructionsFromLabel(TR::LabelSymbol *label);
TR::Snippet *findSnippetInstructionsFromLabel(TR::LabelSymbol *label);

TR::Instruction *generateDebugCounterBump(TR::Instruction *cursor, TR::DebugCounterBase *counter, int32_t delta, TR::RegisterDependencyConditions *cond);
TR::Instruction *generateDebugCounterBump(TR::Instruction *cursor, TR::DebugCounterBase *counter, TR::Register *deltaReg, TR::RegisterDependencyConditions *cond);
TR::Instruction *generateDebugCounterBump(TR::Instruction *cursor, TR::DebugCounterBase *counter, int32_t delta, TR_ScratchRegisterManager &srm);
TR::Instruction *generateDebugCounterBump(TR::Instruction *cursor, TR::DebugCounterBase *counter, TR::Register *deltaReg, TR_ScratchRegisterManager &srm);

bool supportsDebugCounters(TR::DebugCounterInjectionPoint injectionPoint){ return injectionPoint != TR::TR_AfterRegAlloc; }

int32_t arrayTranslateMinimumNumberOfElements(bool isByteSource, bool isByteTarget) { return 8; } //FIXME
int32_t arrayTranslateAndTestMinimumNumberOfIterations() { return 8; } //FIXME

// Provide codeGen-specific hooks for class unloading events
static void ppcCGOnClassUnloading(void * loaderPtr);

TR::Instruction *loadAddressConstantFixed(
TR::Node        *node,
intptrj_t         value,
TR::Register    *targetRegister,
TR::Instruction *cursor=NULL,
TR::Register    *tempReg=NULL,
int16_t         typeAddress = -1,
bool            doAOTRelocation = true);

TR::Instruction *loadIntConstantFixed(
TR::Node        *node,
int32_t         value,
TR::Register    *targetRegister,
TR::Instruction *cursor=NULL,
int16_t         typeAddress = -1);

TR::Instruction *fixedLoadLabelAddressIntoReg(
TR::Node          *node,
TR::Register      *trgReg,
TR::LabelSymbol    *label,
TR::Instruction   *cursor=NULL,
TR::Register      *tempReg=NULL,
bool              useADDISFor32Bit = false);

void addMetaDataForLoadIntConstantFixed(
TR::Node *node,
TR::Instruction *firstInstruction,
TR::Instruction *secondInstruction,
int16_t typeAddress,
int32_t value);

void addMetaDataForLoadAddressConstantFixed(
TR::Node *node,
TR::Instruction *firstInstruction,
TR::Register *tempReg,
int16_t typeAddress,
intptrj_t value);

void addMetaDataFor32BitFixedLoadLabelAddressIntoReg(
TR::Node *node,
TR::LabelSymbol *label,
TR::Instruction *firstInstruction,
TR::Instruction *secondInstruction);

void addMetaDataFor64BitFixedLoadLabelAddressIntoReg(
TR::Node *node,
TR::LabelSymbol *label,
TR::Register *tempReg,
TR::Instruction **q);

private:

enum // flags
{
HasCall               = 0x00000200,
NoStackFrame          = 0x00000400,
CanExceptByTrap       = 0x00000800,
EnableTLHPrefetching  = 0x00001000,
IsOutOfLineColdPath   = 0x00002000, // AVAILABLE
IsOutOfLineHotPath    = 0x00004000,
IsDualTLH             = 0x00008000,
DummyLastFlag
};

void initialize();

TR_BitVector *computeCallInfoBitVector();





TR::RealRegister              *_stackPtrRegister;
TR::RealRegister              *_methodMetaDataRegister;
TR::RealRegister              *_tocBaseRegister;
TR::PPCImmInstruction            *_returnTypeInfoInstruction;
TR::ConstantDataSnippet       *_constantData;
const TR::PPCLinkageProperties   *_linkageProperties;
TR_BitVector                    *_blockCallInfo;
TR_Array<TR::SymbolReference *>  *_trackStatics;
TR_Array<TR_PPCLoadLabelItem *> *_trackItems;
TR_Array<TR::Register *>          _transientLongRegisters;
TR::list<TR_PPCOutOfLineCodeSection*> _outOfLineCodeSectionList;
flags32_t                        _flags;

uint32_t                         _numGPR;
uint32_t                         _firstGPR;
uint32_t                         _lastGPR;
uint32_t                         _firstParmGPR;
uint32_t                         _lastVolatileGPR;
uint32_t                         _numFPR;
uint32_t                         _firstFPR;
uint32_t                         _lastFPR;
uint32_t                         _firstParmFPR;
uint32_t                         _lastVolatileFPR;
uint32_t *                       _tbTableStart;
uint32_t *                       _tbTabletbOff;
uint32_t *                       _tbTableEnd;
uint32_t                         _numVRF;

};

} // PPC

} // TR


class TR_PPCScratchRegisterManager : public TR_ScratchRegisterManager
{
public:
TR_PPCScratchRegisterManager(int32_t capacity, TR::CodeGenerator *cg) : TR_ScratchRegisterManager(capacity, cg) {}
using TR_ScratchRegisterManager::addScratchRegistersToDependencyList;
void addScratchRegistersToDependencyList(TR::RegisterDependencyConditions *deps, bool excludeGPR0);
};

#endif
/*******************************************************************************
* Copyright (c) 2000, 2018 IBM Corp. and others
*
* This program and the accompanying materials are made available under
* the terms of the Eclipse Public License 2.0 which accompanies this
* distribution and is available at http://eclipse.org/legal/epl-2.0
* or the Apache License, Version 2.0 which accompanies this distribution
* and is available at https://www.apache.org/licenses/LICENSE-2.0.
*
* This Source Code may also be made available under the following Secondary
* Licenses when the conditions for such availability set forth in the
* Eclipse Public License, v. 2.0 are satisfied: GNU General Public License,
* version 2 with the GNU Classpath Exception [1] and GNU General Public
* License, version 2 with the OpenJDK Assembly Exception [2].
*
* [1] https://www.gnu.org/software/classpath/license.html
* [2] http://openjdk.java.net/legal/assembly-exception.html
*
* SPDX-License-Identifier: EPL-2.0 OR Apache-2.0 OR GPL-2.0 WITH Classpath-exception-2.0 OR LicenseRef-GPL-2.0 WITH Assembly-exception
*******************************************************************************/

#ifndef OMR_X86_CODEGENERATOR_INCL
#define OMR_X86_CODEGENERATOR_INCL

/*
* The following #define and typedef must appear before any #includes in this file
*/
#ifndef OMR_CODEGENERATOR_CONNECTOR
#define OMR_CODEGENERATOR_CONNECTOR
namespace OMR { namespace X86 { class CodeGenerator; } }
namespace OMR { typedef OMR::X86::CodeGenerator CodeGeneratorConnector; }
#endif

#include "compiler/codegen/OMRCodeGenerator.hpp"

#include "codegen/Machine.hpp"                 // for Machine, etc
#include "codegen/RealRegister.hpp"
#include "codegen/Register.hpp"                // for Register
#include "codegen/RegisterIterator.hpp"        // for RegisterIterator
#include "codegen/ScratchRegisterManager.hpp"
#include "compile/Compilation.hpp"             // for Compilation
#include "env/jittypes.h"                      // for intptrj_t
#include "il/SymbolReference.hpp"              // for SymbolReference
#include "il/symbol/AutomaticSymbol.hpp"       // for AutomaticSymbol
#include "il/symbol/ResolvedMethodSymbol.hpp"  // for ResolvedMethodSymbol
#include "infra/BitVector.hpp"                 // for TR_BitVector
#include "infra/TRlist.hpp"
#include "x/codegen/X86Ops.hpp"                // for TR_X86OpCodes
#include "x/codegen/X86Register.hpp"           // for TR_X86FPStackRegister, etc
#include "env/CompilerEnv.hpp"

#if defined(LINUX) || defined(OSX)
#include <sys/time.h>                          // for timeval
#endif

#include "codegen/Instruction.hpp"
#include "il/symbol/LabelSymbol.hpp"
#include "il/symbol/StaticSymbol.hpp"
#include "x/codegen/OutlinedInstructions.hpp"
#include "codegen/GCStackMap.hpp"
#include "codegen/GCStackAtlas.hpp"

class TR_GCStackMap;
namespace TR { class IA32ConstantDataSnippet; }
namespace TR { class IA32DataSnippet; }
class TR_OutlinedInstructions;
namespace OMR { namespace X86 { class CodeGenerator; } }
namespace TR { class CodeGenerator; }
namespace TR { class MemoryReference; }
namespace TR { class X86ImmInstruction;         }
namespace TR { class X86LabelInstruction;       }
namespace TR { class X86MemTableInstruction;    }
namespace TR { class X86ScratchRegisterManager; }
namespace TR { class X86VFPSaveInstruction;     }
namespace TR { struct X86LinkageProperties; }

namespace TR {

class ClobberingInstruction
{
TR::Instruction *_instruction;
TR::list<TR::Register*> _clobberedRegisters;

public:
TR_ALLOC(TR_Memory::ClobberingInstruction)

ClobberingInstruction(TR_Memory * m) : _instruction(0), _clobberedRegisters(getTypedAllocator<TR::Register*>(TR::comp()->allocator())) {};
ClobberingInstruction(TR::Instruction *instr, TR_Memory * m) : _instruction(instr), _clobberedRegisters(getTypedAllocator<TR::Register*>(TR::comp()->allocator())) {}

TR::Instruction *getInstruction() {return _instruction;}
TR::Instruction *setInstruction(TR::Instruction *i) {return (_instruction = i);}
TR::list<TR::Register*> &getClobberedRegisters() {return _clobberedRegisters;}
void addClobberedRegister(TR::Register *reg) {_clobberedRegisters.push_front(reg);}
};

}

typedef TR::ClobberingInstruction TR_ClobberingInstruction;

struct TR_BetterSpillPlacement
{
TR_ALLOC(TR_Memory::BetterSpillPlacement)

TR_BetterSpillPlacement *_next;
TR_BetterSpillPlacement *_prev;
TR::Register *_virtReg;
TR::Instruction *_branchInstruction;
uint32_t _freeRealRegs;
};

#define CPUID_SIGNATURE_STEPPING_MASK       0x0000000f
#define CPUID_SIGNATURE_MODEL_MASK          0x000000f0
#define CPUID_SIGNATURE_FAMILY_MASK         0x00000f00
#define CPUID_SIGNATURE_PROCESSOR_MASK      0x00003000
#define CPUID_SIGNATURE_EXTENDEDMODEL_MASK  0x000f0000
#define CPUID_SIGNATURE_EXTENDEDFAMILY_MASK 0x0ff00000

struct TR_X86ProcessorInfo
{
public:

TR_ALLOC(TR_Memory::IA32ProcessorInfo)

enum TR_X86ProcessorVendors
{
TR_AuthenticAMD                  = 0x01,
TR_GenuineIntel                  = 0x02,
TR_UnknownVendor                 = 0x04
};

bool enabledXSAVE()                     {return _featureFlags2.testAny(TR_OSXSAVE);}
bool hasBuiltInFPU()                    {return _featureFlags.testAny(TR_BuiltInFPU);}
bool supportsVirtualModeExtension()     {return _featureFlags.testAny(TR_VirtualModeExtension);}
bool supportsDebuggingExtension()       {return _featureFlags.testAny(TR_DebuggingExtension);}
bool supportsPageSizeExtension()        {return _featureFlags.testAny(TR_PageSizeExtension);}
bool supportsRDTSCInstruction()         {return _featureFlags.testAny(TR_RDTSCInstruction);}
bool hasModelSpecificRegisters()        {return _featureFlags.testAny(TR_ModelSpecificRegisters);}
bool supportsPhysicalAddressExtension() {return _featureFlags.testAny(TR_PhysicalAddressExtension);}
bool supportsMachineCheckException()    {return _featureFlags.testAny(TR_MachineCheckException);}
bool supportsCMPXCHG8BInstruction()     {return _featureFlags.testAny(TR_CMPXCHG8BInstruction);}
bool supportsCMPXCHG16BInstruction()    {return _featureFlags2.testAny(TR_CMPXCHG16BInstruction);}
bool hasAPICHardware()                  {return _featureFlags.testAny(TR_APICHardware);}
bool hasMemoryTypeRangeRegisters()      {return _featureFlags.testAny(TR_MemoryTypeRangeRegisters);}
bool supportsPageGlobalFlag()           {return _featureFlags.testAny(TR_PageGlobalFlag);}
bool hasMachineCheckArchitecture()      {return _featureFlags.testAny(TR_MachineCheckArchitecture);}
bool supportsCMOVInstructions()         {return _featureFlags.testAny(TR_CMOVInstructions);}
bool supportsFCOMIInstructions()        {return _featureFlags.testAll(TR_BuiltInFPU | TR_CMOVInstructions);}
bool hasPageAttributeTable()            {return _featureFlags.testAny(TR_PageAttributeTable);}
bool has36BitPageSizeExtension()        {return _featureFlags.testAny(TR_36BitPageSizeExtension);}
bool hasProcessorSerialNumber()         {return _featureFlags.testAny(TR_ProcessorSerialNumber);}
bool supportsCLFLUSHInstruction()       {return _featureFlags.testAny(TR_CLFLUSHInstruction);}
bool supportsDebugTraceStore()          {return _featureFlags.testAny(TR_DebugTraceStore);}
bool hasACPIRegisters()                 {return _featureFlags.testAny(TR_ACPIRegisters);}
bool supportsMMXInstructions()          {return _featureFlags.testAny(TR_MMXInstructions);}
bool supportsFastFPSavesRestores()      {return _featureFlags.testAny(TR_FastFPSavesRestores);}
bool supportsSSE()                      {return _featureFlags.testAny(TR_SSE);}
bool supportsSSE2()                     {return _featureFlags.testAny(TR_SSE2);}
bool supportsSSE3()                     {return _featureFlags2.testAny(TR_SSE3);}
bool supportsSSSE3()                    {return _featureFlags2.testAny(TR_SSSE3);}
bool supportsSSE4_1()                   {return _featureFlags2.testAny(TR_SSE4_1);}
bool supportsSSE4_2()                   {return _featureFlags2.testAny(TR_SSE4_2);}
bool supportsAVX()                      {return _featureFlags2.testAny(TR_AVX) && enabledXSAVE();}
bool supportsAVX2()                     {return _featureFlags8.testAny(TR_AVX2) && enabledXSAVE();}
bool supportsBMI1()                     {return _featureFlags8.testAny(TR_BMI1) && enabledXSAVE();}
bool supportsBMI2()                     {return _featureFlags8.testAny(TR_BMI2) && enabledXSAVE();}
bool supportsFMA()                      {return _featureFlags2.testAny(TR_FMA) && enabledXSAVE();}
bool supportsCLMUL()                    {return _featureFlags2.testAny(TR_CLMUL);}
bool supportsAESNI()                    {return _featureFlags2.testAny(TR_AESNI);}
bool supportsPOPCNT()                   {return _featureFlags2.testAny(TR_POPCNT);}
bool supportsSelfSnoop()                {return _featureFlags.testAny(TR_SelfSnoop);}
bool supportsTM()                       {return _featureFlags8.testAny(TR_RTM);}
bool supportsHyperThreading()           {return _featureFlags.testAny(TR_HyperThreading);}
bool hasThermalMonitor()                {return _featureFlags.testAny(TR_ThermalMonitor);}

bool supportsMFence()                   {return _featureFlags.testAny(TR_SSE2);}
bool supportsLFence()                   {return _featureFlags.testAny(TR_SSE2);}
bool supportsSFence()                   {return _featureFlags.testAny(TR_SSE | TR_MMXInstructions);}
bool prefersMultiByteNOP()              {return getX86Architecture() && isGenuineIntel() && !isIntelPentium();}

uint32_t getCPUStepping(uint32_t signature)       {return (signature & CPUID_SIGNATURE_STEPPING_MASK);}
uint32_t getCPUModel(uint32_t signature)          {return (signature & CPUID_SIGNATURE_MODEL_MASK) >> 4;}
uint32_t getCPUFamily(uint32_t signature)         {return (signature & CPUID_SIGNATURE_FAMILY_MASK) >> 8;}
uint32_t getCPUProcessor(uint32_t signature)      {return (signature & CPUID_SIGNATURE_PROCESSOR_MASK) >> 12;}
uint32_t getCPUExtendedModel(uint32_t signature)  {return (signature & CPUID_SIGNATURE_EXTENDEDMODEL_MASK) >> 16;}
uint32_t getCPUExtendedFamily(uint32_t signature) {return (signature & CPUID_SIGNATURE_EXTENDEDFAMILY_MASK) >> 20;}

bool isIntelPentium()      { return (_processorDescription & 0x000000ff) == TR_ProcessorIntelPentium; }
bool isIntelP6()           { return (_processorDescription & 0x000000ff) == TR_ProcessorIntelP6; }
bool isIntelPentium4()     { return (_processorDescription & 0x000000ff) == TR_ProcessorIntelPentium4; }
bool isIntelCore2()        { return (_processorDescription & 0x000000ff) == TR_ProcessorIntelCore2; }
bool isIntelTulsa()        { return (_processorDescription & 0x000000ff) == TR_ProcessorIntelTulsa; }
bool isIntelNehalem()      { return (_processorDescription & 0x000000ff) == TR_ProcessorIntelNehalem; }
bool isIntelWestmere()     { return (_processorDescription & 0x000000ff) == TR_ProcessorIntelWestmere; }
bool isIntelSandyBridge()  { return (_processorDescription & 0x000000ff) == TR_ProcessorIntelSandyBridge; }
bool isIntelIvyBridge()    { return (_processorDescription & 0x000000ff) == TR_ProcessorIntelIvyBridge; }
bool isIntelHaswell()      { return (_processorDescription & 0x000000ff) == TR_ProcessorIntelHaswell; }
bool isIntelBroadwell()    { return (_processorDescription & 0x000000ff) == TR_ProcessorIntelBroadwell; }
bool isIntelSkylake()      { return (_processorDescription & 0x000000ff) == TR_ProcessorIntelSkylake; }

bool isIntelOldMachine()   { return (isIntelPentium() || isIntelP6() || isIntelPentium4() || isIntelCore2() || isIntelTulsa() || isIntelNehalem()); }

bool isAMDK6()             { return (_processorDescription & 0x000000fe) == TR_ProcessorAMDK5; } // accept either K5 or K6
bool isAMDAthlonDuron()    { return (_processorDescription & 0x000000ff) == TR_ProcessorAMDAthlonDuron; }
bool isAMDOpteron()        { return (_processorDescription & 0x000000ff) == TR_ProcessorAMDOpteron; }
bool isAMD15h()            { return (_processorDescription & 0x000000ff) == TR_ProcessorAMDFamily15h; }

bool isGenuineIntel() {return _vendorFlags.testAny(TR_GenuineIntel);}
bool isAuthenticAMD() {return _vendorFlags.testAny(TR_AuthenticAMD);}

bool requiresLFENCE() { return false; /* Dummy for now, we may need LFENCE in future processors*/}

int32_t getX86Architecture() { return (_processorDescription & 0x000000ff);}

private:

flags8_t   _vendorFlags;
flags32_t  _featureFlags;   // cache feature flags for re-use
flags32_t  _featureFlags2;  // cache feature flags 2 for re-use
flags32_t  _featureFlags8;  // cache feature flags 8 for re-use

uint32_t _processorDescription;

friend class OMR::X86::CodeGenerator;

void initialize(TR::Compilation *);
};

enum TR_PaddingProperties
{
TR_NoOpPadding       = 0,
TR_AtomicNoOpPadding = 1,
// Note--other possible property flags:
// - Don't care about condition codes (eg. add eax,0 would be ok)
// - Not executed (eg. int3 or eyecatcher would be ok)
};

#define PADDING_TABLE_MAX_ENCODING_LENGTH 9

class TR_X86PaddingTable
{
public:

TR_X86PaddingTable(uint8_t biggestEncoding, uint8_t*** encodings, flags8_t flags, uint16_t sibMask, uint16_t prefixMask):
_biggestEncoding(biggestEncoding), _encodings(encodings), _flags(flags), _sibMask(sibMask), _prefixMask(prefixMask) {}

enum { registerMatters=1 }; // for _flags

uint8_t     _biggestEncoding;
flags8_t    _flags;
uint8_t***  _encodings;
uint16_t    _sibMask;    // 2^n is set if the NOP with size n uses a SIB byte
uint16_t    _prefixMask; // 2^n is set if the NOP with size n has a prefix byte
};

struct TR_VFPState
{
TR::RealRegister::RegNum _register;
int32_t _displacement;

TR_VFPState():_register(TR::RealRegister::NoReg),_displacement(0){} // Assumes noReg is zero
void operator =  (const TR_VFPState &other){ _register = other._register; _displacement = other._displacement; }
bool operator == (const TR_VFPState &other) const { return _register == other._register && _displacement == other._displacement; }
bool operator != (const TR_VFPState &other) const { return !operator==(other); }
};

namespace OMR
{

namespace X86
{

class OMR_EXTENSIBLE CodeGenerator : public OMR::CodeGenerator
{

public:

TR::Linkage *createLinkage(TR_LinkageConventions lc);
void beginInstructionSelection();
void endInstructionSelection();


static TR_X86ProcessorInfo &getX86ProcessorInfo() {return _targetProcessorInfo;}

typedef enum
{
Backward = 0,
Forward  = 1
} RegisterAssignmentDirection;

void doRegisterAssignment(TR_RegisterKinds kindsToAssign);
void doBinaryEncoding();

void doBackwardsRegisterAssignment(TR_RegisterKinds kindsToAssign, TR::Instruction *startInstruction, TR::Instruction *appendInstruction = NULL);

bool hasComplexAddressingMode() { return true; }
bool getSupportsBitOpCodes() { return true; }

bool getSupportsOpCodeForAutoSIMD(TR::ILOpCode, TR::DataType);
bool getSupportsEncodeUtf16LittleWithSurrogateTest();
bool getSupportsEncodeUtf16BigWithSurrogateTest();

virtual bool getSupportsIbyteswap();

virtual bool getSupportsBitPermute();

bool supportsMergingGuards();

bool supportsAtomicAdd()                {return true;}
bool hasTMEvaluator()                       {return true;}

int64_t getLargestNegConstThatMustBeMaterialized() { TR_ASSERT(0, "Not Implemented on x86"); return 0; }
int64_t getSmallestPosConstThatMustBeMaterialized() { TR_ASSERT(0, "Not Implemented on x86"); return 0; }

void performNonLinearRegisterAssignmentAtBranch(TR::X86LabelInstruction *branchInstruction, TR_RegisterKinds kindsToBeAssigned);
void prepareForNonLinearRegisterAssignmentAtMerge(TR::X86LabelInstruction *mergeInstruction);

void processClobberingInstructions(TR::ClobberingInstruction * clobInstructionCursor, TR::Instruction *instructionCursor);

// different from evaluateNode in that it returns a clobberable register
TR::Register *shortClobberEvaluate(TR::Node *node);
TR::Register *intClobberEvaluate(TR::Node *node);
virtual TR::Register *longClobberEvaluate(TR::Node *node)=0;

TR::Register *floatClobberEvaluate(TR::Node *node);
TR::Register *doubleClobberEvaluate(TR::Node *node);

const TR::X86LinkageProperties &getProperties() { return *_linkageProperties; }

RegisterAssignmentDirection getAssignmentDirection() {return _assignmentDirection;}
RegisterAssignmentDirection setAssignmentDirection(RegisterAssignmentDirection d)
{
return (_assignmentDirection = d);
}

TR::RegisterIterator *getX87RegisterIterator()                          {return _x87RegisterIterator;}
TR::RegisterIterator *setX87RegisterIterator(TR::RegisterIterator *iter) {return (_x87RegisterIterator = iter);}

TR::RealRegister *getFrameRegister()                       {return _frameRegister;}
TR::RealRegister *getMethodMetaDataRegister();

bool allowGlobalRegisterAcrossBranch(TR_RegisterCandidate *, TR::Node *);

void buildRegisterMapForInstruction(TR_GCStackMap *map);

bool processInstruction(TR::Instruction *instr, TR_BitVector ** registerUsageInfo, int32_t &blockNum, int32_t &isFence, bool traceIt);
uint32_t isPreservedRegister(int32_t regIndex);
bool isReturnInstruction(TR::Instruction *instr);
bool isBranchInstruction(TR::Instruction *instr);
bool isLabelInstruction(TR::Instruction *instr);
int32_t isFenceInstruction(TR::Instruction *instr);
bool isAlignmentInstruction(TR::Instruction *instr);
TR::Instruction *splitEdge(TR::Instruction *cursor, bool isFallThrough, bool needsJump, TR::Instruction *newSplitLabel, TR::list<TR::Instruction*> *jmpInstrs, bool firstJump = false);
TR::Instruction *splitBlockEntry(TR::Instruction *instr);
int32_t computeRegisterSaveDescription(TR_BitVector *regs, bool populateInfo = false);
void processIncomingParameterUsage(TR_BitVector **registerUsageInfo, int32_t blockNum);
void updateSnippetMapWithRSD(TR::Instruction *cur, int32_t rsd);
bool isTargetSnippetOrOutOfLine(TR::Instruction *instr, TR::Instruction **start, TR::Instruction **end);

TR::SymbolReference *getWordConversionTemp();
TR::SymbolReference *getDoubleWordConversionTemp();

TR::SymbolReference *findOrCreateCurrentTimeMillisTempSymRef();
TR::SymbolReference *getNanoTimeTemp();

int32_t branchDisplacementToHelperOrTrampoline(uint8_t *nextInstructionAddress, TR::SymbolReference *helper);

/*
* \brief Reserve space in the code cache for a specified number of trampolines.
*
* \param[in] numTrampolines : number of trampolines to reserve
*
* \return : none
*/
void reserveNTrampolines(int32_t numTrampolines) { return; }

// Note: This leaves the code aligned in the specified manner.
TR::Instruction *generateSwitchToInterpreterPrePrologue(TR::Instruction *prev, uint8_t alignment, uint8_t alignmentMargin);

int32_t setEstimatedLocationsForDataSnippetLabels(int32_t estimatedSnippetStart);
void emitDataSnippets();
bool hasDataSnippets() { return _dataSnippetList.empty() ? false : true; }

TR::list<TR::Register*> &getSpilledIntRegisters() {return _spilledIntRegisters;}

TR::list<TR::Register*> &getLiveDiscardableRegisters() {return _liveDiscardableRegisters;}
void addLiveDiscardableRegister(TR::Register *reg);
void removeLiveDiscardableRegister(TR::Register *reg);
void clobberLiveDiscardableRegisters(TR::Instruction  *instr, TR::MemoryReference  *mr);

bool canTransformUnsafeCopyToArrayCopy();

using OMR::CodeGenerator::canNullChkBeImplicit;
bool canNullChkBeImplicit(TR::Node *node);

TR::list<TR::Register*> &getDependentDiscardableRegisters() {return _dependentDiscardableRegisters;}
void addDependentDiscardableRegister(TR::Register *reg) {_dependentDiscardableRegisters.push_front(reg);}
void clobberLiveDependentDiscardableRegisters(TR::ClobberingInstruction *instr, TR::Register *baseReg);
void reactivateDependentDiscardableRegisters(TR::Register *baseReg);
void deactivateDependentDiscardableRegisters(TR::Register *baseReg);

TR::list<TR::ClobberingInstruction*> &getClobberingInstructions() {return _clobberingInstructions;}
void addClobberingInstruction(TR::ClobberingInstruction *i)  {_clobberingInstructions.push_front(i);}

TR::list<TR_OutlinedInstructions*> &getOutlinedInstructionsList() {return _outlinedInstructionsList;}

TR_X86ScratchRegisterManager *generateScratchRegisterManager(int32_t capacity=7);

bool supportsConstantRematerialization();
bool supportsLocalMemoryRematerialization();
bool supportsStaticMemoryRematerialization();
bool supportsIndirectMemoryRematerialization();
bool supportsAddressRematerialization();
bool supportsXMMRRematerialization();

TR::Instruction *setLastCatchAppendInstruction(TR::Instruction *i) {return (_lastCatchAppendInstruction=i);}
TR::Instruction *getLastCatchAppendInstruction()                  {return _lastCatchAppendInstruction;}

void saveBetterSpillPlacements(TR::Instruction *branchInstruction);
void removeBetterSpillPlacementCandidate(TR::RealRegister *realReg);
TR::Instruction *findBetterSpillPlacement(TR::Register *virtReg, int32_t realRegNum);

int32_t getInstructionPatchAlignmentBoundary()       {return _instructionPatchAlignmentBoundary;}
void setInstructionPatchAlignmentBoundary(int32_t p) {_instructionPatchAlignmentBoundary = p;}

int32_t getPicSlotCount()       {return _PicSlotCount;}
void setPicSlotCount(int32_t c) {_PicSlotCount = c;}
int32_t incPicSlotCountBy(int32_t i) {return (_PicSlotCount += i);}

int32_t getLowestCommonCodePatchingAlignmentBoundary() {return _lowestCommonCodePatchingAlignmentBoundary;}
void setLowestCommonCodePatchingAlignmentBoundary(int32_t b) {_lowestCommonCodePatchingAlignmentBoundary = b;}

// NOT NEEDED, overridden in amd64/i386
bool internalPointerSupportImplemented() {return false;} // no virt

bool supportsSinglePrecisionSQRT() {return true;}

bool supportsComplexAddressing();

bool getSupportsNewObjectAlignment() { return true; }
bool getSupportsTenuredObjectAlignment() { return true; }
bool isObjectOfSizeWorthAligning(uint32_t size)
{
uint32_t lineSize = 64;//getX86ProcessorInfo().getL1DataCacheLineSize();
//the query doesn't seem to work on AMD processors;
return  ((size < (lineSize<<1)) && (size > (lineSize >> 2)));

}

int32_t getMaximumNumbersOfAssignableGPRs();
int32_t getMaximumNumbersOfAssignableFPRs();
int32_t getMaximumNumbersOfAssignableVRs();
bool willBeEvaluatedAsCallByCodeGen(TR::Node *node, TR::Compilation *comp);

uint8_t getSizeOfCombinedBuffer();

bool supportsInliningOfIsInstance();

bool supportsPassThroughCopyToNewVirtualRegister() { return true; }

bool doRematerialization() {return true;}

bool materializesHeapBase() { return false; }
bool canFoldLargeOffsetInAddressing() { return true; }

int32_t arrayInitMinimumNumberOfBytes()
{
if (TR::Compiler->target.is64Bit()) return 12;
return 8;
}

// codegen methods referenced from ras/
//
uint32_t estimateBinaryLength(TR::MemoryReference *);

void apply32BitLoadLabelRelativeRelocation(TR::Instruction *movInstruction, TR::LabelSymbol *startLabel, TR::LabelSymbol *endLabel, int32_t deltaToStartLabel);

void apply32BitLabelRelativeRelocation(int32_t * cursor, TR::LabelSymbol *);

bool isAddressScaleIndexSupported(int32_t scale) { if (scale <= 8) return true; return false; }

void addMetaDataForBranchTableAddress(uint8_t *target, TR::Node *caseNode, TR::X86MemTableInstruction *jmpTableInstruction);


///////////////////////////////////
//
// No-op generation
//

// Emits length bytes of padding starting at cursor with the given properties.
// Returns the first byte beyond the padding.
// neighborhood, if provided, provides a context that allows for a good
// no-op instruction to be chosen based on the surrounding instructions.
//
virtual uint8_t *generatePadding(uint8_t              *cursor,
intptrj_t             length,
TR::Instruction    *neighborhood=NULL,
TR_PaddingProperties  properties=TR_NoOpPadding,
bool                  recursive=false);

// Returns the number of bytes of padding required to cause cursor to be
// aligned to the given boundary, minus the margin.
// Specifically, cursor + margin + alignment(cursor, boundary, margin) will
// be a multiple of boundary.
//
intptrj_t alignment(intptrj_t cursor, intptrj_t boundary, intptrj_t margin=0)
{
TR_ASSERT((boundary & (boundary-1)) == 0, "Can only align to powers of 2");
return (-cursor - margin) & (boundary-1);
}
intptrj_t alignment(void *cursor, intptrj_t boundary, intptrj_t margin=0);

bool patchableRangeNeedsAlignment(void *cursor, intptrj_t length, intptrj_t boundary, intptrj_t margin=0);

bool nopsAlsoProcessedByRelocations() { return false; }

#if defined(DEBUG)
void dumpPreFPRegisterAssignment(TR::Instruction *);
void dumpPostFPRegisterAssignment(TR::Instruction *, TR::Instruction *);
void dumpPreGPRegisterAssignment(TR::Instruction *);
void dumpPostGPRegisterAssignment(TR::Instruction *, TR::Instruction *);
#endif

void dumpDataSnippets(TR::FILE *pOutFile);

TR::IA32ConstantDataSnippet *findOrCreate2ByteConstant(TR::Node *, int16_t c);
TR::IA32ConstantDataSnippet *findOrCreate4ByteConstant(TR::Node *, int32_t c);
TR::IA32ConstantDataSnippet *findOrCreate8ByteConstant(TR::Node *, int64_t c);
TR::IA32ConstantDataSnippet *findOrCreate16ByteConstant(TR::Node *, void *c);
TR::IA32DataSnippet *create2ByteData(TR::Node *, int16_t c);
TR::IA32DataSnippet *create4ByteData(TR::Node *, int32_t c);
TR::IA32DataSnippet *create8ByteData(TR::Node *, int64_t c);
TR::IA32DataSnippet *create16ByteData(TR::Node *, void *c);

bool supportsCMOV() {return (_targetProcessorInfo.supportsCMOVInstructions());}

static TR_X86ProcessorInfo _targetProcessorInfo;

// The core "clobberEvaluate" logic for single registers (not register
// pairs), parameterized by the opcode used to move the desired value into a
// clobberable register if necessary
TR::Register *gprClobberEvaluate(TR::Node *node, TR_X86OpCodes movRegRegOpCode);

TR_OutlinedInstructions *findOutlinedInstructionsFromLabel(TR::LabelSymbol *label);
TR_OutlinedInstructions *findOutlinedInstructionsFromMergeLabel(TR::LabelSymbol *label);
TR_OutlinedInstructions *findOutlinedInstructionsFromLabelForShrinkWrapping(TR::LabelSymbol *label);

const TR_VFPState         &vfpState()           { return _vfpState; }
TR::X86VFPSaveInstruction  *vfpResetInstruction(){ return _vfpResetInstruction; }
void setVFPState(const TR_VFPState &newState){ _vfpState = newState; }
void initializeVFPState(TR::RealRegister::RegNum reg, int32_t displacement){ _vfpState._register = reg; _vfpState._displacement = displacement; }


void removeUnavailableRegisters(TR_RegisterCandidate * rc, TR::Block * * blocks, TR_BitVector & availableRegisters);

TR::Instruction *generateDebugCounterBump(TR::Instruction *cursor, TR::DebugCounterBase *counter, int32_t delta, TR::RegisterDependencyConditions *cond);
TR::Instruction *generateDebugCounterBump(TR::Instruction *cursor, TR::DebugCounterBase *counter, TR::Register *deltaReg, TR::RegisterDependencyConditions *cond);
TR::Instruction *generateDebugCounterBump(TR::Instruction *cursor, TR::DebugCounterBase *counter, int32_t delta, TR_ScratchRegisterManager &srm);
TR::Instruction *generateDebugCounterBump(TR::Instruction *cursor, TR::DebugCounterBase *counter, TR::Register *deltaReg, TR_ScratchRegisterManager &srm);

bool supportsDebugCounters(TR::DebugCounterInjectionPoint injectionPoint){ return true; }

virtual uint8_t nodeResultGPRCount  (TR::Node *node, TR_RegisterPressureState *state);

virtual TR_BitVector *getGlobalRegisters(TR_SpillKinds kind, TR_LinkageConventions lc){ return &_globalRegisterBitVectors[kind]; }

virtual void simulateNodeEvaluation (TR::Node *node, TR_RegisterPressureState *state, TR_RegisterPressureSummary *summary);

protected:

CodeGenerator();

// Note: the following should be called by subclasses near the end of their
// constructors.  This breaks a rather nasty cyclic initialization dependency,
// but it is pretty ugly.
// TODO: Figure out a cleaner solution for this.
void initialize( TR::Compilation *comp);

TR::RealRegister::RegNum pickNOPRegister(TR::Instruction  *successor);

TR_BitVector _globalRegisterBitVectors[TR_numSpillKinds];

static uint8_t k8PaddingEncoding[PADDING_TABLE_MAX_ENCODING_LENGTH][PADDING_TABLE_MAX_ENCODING_LENGTH];
static uint8_t intelMultiBytePaddingEncoding[PADDING_TABLE_MAX_ENCODING_LENGTH][PADDING_TABLE_MAX_ENCODING_LENGTH];
static uint8_t old32BitPaddingEncoding[PADDING_TABLE_MAX_ENCODING_LENGTH][PADDING_TABLE_MAX_ENCODING_LENGTH];

static TR_X86PaddingTable _intelMultiBytePaddingTable;
static TR_X86PaddingTable _K8PaddingTable;
static TR_X86PaddingTable _old32BitPaddingTable;

TR::X86ImmInstruction *_returnTypeInfoInstruction;
const TR::X86LinkageProperties  *_linkageProperties;

private:

bool nodeIsFoldableMemOperand(TR::Node *node, TR::Node *parent, TR_RegisterPressureState *state);

TR::IA32ConstantDataSnippet     *findOrCreateConstant(TR::Node *, void *c, uint8_t size);

TR::RealRegister             *_frameRegister;

TR::SymbolReference             *_wordConversionTemp;
TR::SymbolReference             *_doubleWordConversionTemp;
TR::SymbolReference             *_currentTimeMillisTemp;
TR::SymbolReference             *_nanoTimeTemp;

TR::Instruction                 *_lastCatchAppendInstruction;
TR_BetterSpillPlacement        *_betterSpillPlacements;

TR::list<TR::IA32DataSnippet*>          _dataSnippetList;
TR::list<TR::Register*>               _spilledIntRegisters;
TR::list<TR::Register*>               _liveDiscardableRegisters;
TR::list<TR::Register*>               _dependentDiscardableRegisters;
TR::list<TR::ClobberingInstruction*>  _clobberingInstructions;
std::list<TR::ClobberingInstruction*, TR::typed_allocator<TR::ClobberingInstruction*, TR::Allocator> >::iterator _clobIterator;
TR::list<TR_OutlinedInstructions*>   _outlinedInstructionsList;

RegisterAssignmentDirection     _assignmentDirection;
TR::RegisterIterator            *_x87RegisterIterator;

int32_t                         _instructionPatchAlignmentBoundary;
int32_t                         _PicSlotCount;
int32_t                         _lowestCommonCodePatchingAlignmentBoundary;

TR_VFPState                     _vfpState;
TR::X86VFPSaveInstruction       *_vfpResetInstruction;

TR_X86PaddingTable             *_paddingTable;

TR::LabelSymbol                  *_switchToInterpreterLabel;

TR_X86OpCodes                   _xmmDoubleLoadOpCode;

enum TR_X86CodeGeneratorFlags
{
EnableBetterSpillPlacements              = 0x00000001, ///< use better spill placements
EnableRematerialisation                  = 0x00000002, ///< use register rematerialisation
EnableRegisterAssociations               = 0x00000004, ///< use register associations for register assignment
EnableSinglePrecisionMethods             = 0x00000008, ///< support changing FPCW to single precision for individual methods
EnableRegisterInterferences              = 0x00000010, ///< consider register interferences during register assignment
EnableRegisterWeights                    = 0x00000020, ///< use register weights in choosing a best register candidate
UseSSEForSinglePrecision                 = 0x00000040, ///< use SSE extensions for single precision
UseSSEForDoublePrecision                 = 0x00000080, ///< use SSE extensions for double precision
EnableImplicitDivideCheck                = 0x00000100, ///< platform can catch hardware exceptions for divide overflow and divide by zero
GenerateMasmListingSyntax                = 0x00000200, ///< generate Masm-style syntax in the debug listings
MapAutosTo8ByteSlots                     = 0x00000400, ///< don't round up sizes of autos to an 8-byte slot size when the stack is mapped
EnableTLHPrefetching                     = 0x00000800, ///< enable software prefetches on TLH allocates
UseGPRsForWin32CTMConversion             = 0x00001000, ///< use magic number approach for long division for CTM conversion to milliseconds
UseLongDivideHelperForWin32CTMConversion = 0x00002000, ///< use long division helper for CTM conversion to milliseconds
TargetSupportsSoftwarePrefetches         = 0x00004000, ///< target processor and OS both support software prefetch instructions
MethodEnterExitTracingEnabled            = 0x00008000, ///< trace method enter/exits
// Available                             = 0x00010000,
PushPreservedRegisters                   = 0x00020000  ///< we've chosen to save/restore preserved regs using push/pop instructions instead of movs
};

flags32_t _flags;

public:

bool allowGuardMerging() { return false; }

bool enableBetterSpillPlacements()
{
return _flags.testAny(EnableBetterSpillPlacements);
}
void setEnableBetterSpillPlacements() {_flags.set(EnableBetterSpillPlacements);}
void resetEnableBetterSpillPlacements() {_flags.reset(EnableBetterSpillPlacements);}

bool enableRematerialisation()
{
return _flags.testAny(EnableRematerialisation);
}
void setEnableRematerialisation() {_flags.set(EnableRematerialisation);}
void resetEnableRematerialisation() {_flags.reset(EnableRematerialisation);}

bool enableSinglePrecisionMethods()
{
return _flags.testAny(EnableSinglePrecisionMethods);
}
void setEnableSinglePrecisionMethods() {_flags.set(EnableSinglePrecisionMethods);}

bool enableRegisterWeights()
{
return _flags.testAny(EnableRegisterWeights);
}
void setEnableRegisterWeights() {_flags.set(EnableRegisterWeights);}

bool enableRegisterInterferences()
{
return _flags.testAny(EnableRegisterInterferences);
}
void setEnableRegisterInterferences() {_flags.set(EnableRegisterInterferences);}

bool enableRegisterAssociations()
{
return _flags.testAny(EnableRegisterAssociations);
}
void setEnableRegisterAssociations() {_flags.set(EnableRegisterAssociations);}

bool useSSEForSinglePrecision()
{
return _flags.testAny(UseSSEForSinglePrecision);
}
void setUseSSEForSinglePrecision() {_flags.set(UseSSEForSinglePrecision);}

bool useSSEForDoublePrecision()
{
return _flags.testAny(UseSSEForDoublePrecision);
}
void setUseSSEForDoublePrecision() {_flags.set(UseSSEForDoublePrecision);}

bool useSSEForSingleAndDoublePrecision()
{
return _flags.testAll(UseSSEForSinglePrecision | UseSSEForDoublePrecision);
}

bool useSSEFor(TR::DataType type);

bool useGPRsForWin32CTMConversion()
{
return _flags.testAny(UseGPRsForWin32CTMConversion);
}
void setUseGPRsForWin32CTMConversion() {_flags.set(UseGPRsForWin32CTMConversion);}

bool useLongDivideHelperForWin32CTMConversion()
{
return _flags.testAny(UseLongDivideHelperForWin32CTMConversion);
}
void setUseLongDivideHelperForWin32CTMConversion() {_flags.set(UseLongDivideHelperForWin32CTMConversion);}

bool targetSupportsSoftwarePrefetches()
{
return _flags.testAny(TargetSupportsSoftwarePrefetches);
}
void setTargetSupportsSoftwarePrefetches() {_flags.set(TargetSupportsSoftwarePrefetches);}

bool enableTLHPrefetching()
{
return _flags.testAny(EnableTLHPrefetching);
}
void setEnableTLHPrefetching() {_flags.set(EnableTLHPrefetching);}

bool needToAvoidCommoningInGRA();

bool generateMasmListingSyntax()    {return _flags.testAny(GenerateMasmListingSyntax);}
void setGenerateMasmListingSyntax() {_flags.set(GenerateMasmListingSyntax);}

bool codegenSupportsUnsignedIntegerDivide() {return true;}

virtual bool supportsDirectJNICallsForAOT() { return true; }

bool enableImplicitDivideCheck()    { return _flags.testAny(EnableImplicitDivideCheck); }
void setEnableImplicitDivideCheck() { _flags.set(EnableImplicitDivideCheck); }

bool mapAutosTo8ByteSlots()     { return _flags.testAny(MapAutosTo8ByteSlots); }
void setMapAutosTo8ByteSlots()  { _flags.set(MapAutosTo8ByteSlots); }

TR::X86ImmInstruction* getReturnTypeInfoInstruction() { return _returnTypeInfoInstruction; }

TR_X86OpCodes getXMMDoubleLoadOpCode() {return _xmmDoubleLoadOpCode;}
void setXMMDoubleLoadOpCode(TR_X86OpCodes o) {_xmmDoubleLoadOpCode = o;}

TR::LabelSymbol *getSwitchToInterpreterLabel() {return _switchToInterpreterLabel;}
void setSwitchToInterpreterLabel(TR::LabelSymbol *s) {_switchToInterpreterLabel = s;}

bool methodEnterExitTracingEnabled()
{
return _flags.testAny(MethodEnterExitTracingEnabled);
}
void setMethodEnterExitTracingEnabled() {_flags.set(MethodEnterExitTracingEnabled);}

bool pushPreservedRegisters()
{ return _flags.testAny(PushPreservedRegisters); }
void setPushPreservedRegisters() {_flags.set(PushPreservedRegisters);}

// arrayTranslateTableRequiresAlignment is N/A, since we don't have
// arraytranslate tables.
int32_t arrayTranslateMinimumNumberOfElements(bool isByteSource, bool isByteTarget);
int32_t arrayTranslateAndTestMinimumNumberOfIterations();
};

}

}


// Trampolines
//

#if defined(TR_TARGET_64BIT)
#define NEEDS_TRAMPOLINE(target, rip, cg) (cg->alwaysUseTrampolines() || !IS_32BIT_RIP((target), (rip)))
#else
// Give the C++ compiler a hand
#define NEEDS_TRAMPOLINE(target, rip, cg) (0)
#endif

#define IS_32BIT_RIP(x,rip)  ((intptrj_t)(x) == (intptrj_t)(rip) + (int32_t)((intptrj_t)(x) - (intptrj_t)(rip)))


class TR_X86FPStackIterator : public TR::RegisterIterator
{
public:

TR_X86FPStackIterator(TR::Machine *machine, TR_RegisterKinds kind = TR_NoRegister):
TR::RegisterIterator(machine, kind)
{
_machine = machine;
_cursor = TR_X86FPStackRegister::fpFirstStackReg;
}

TR::Register *getFirst() {return _machine->getFPStackLocationPtr(_cursor = TR_X86FPStackRegister::fpFirstStackReg);}
TR::Register *getCurrent() {return _machine->getFPStackLocationPtr(_cursor);}
TR::Register *getNext() {return _cursor > TR_X86FPStackRegister::fpLastStackReg ? NULL : _machine->getFPStackLocationPtr(++_cursor);}

private:

TR::Machine *_machine;
int32_t _cursor;
};

class TR_X86ScratchRegisterManager: public TR_ScratchRegisterManager
{
public:

TR_X86ScratchRegisterManager(int32_t capacity, TR::CodeGenerator *cg) : TR_ScratchRegisterManager(capacity, cg){}
bool reclaimAddressRegister(TR::MemoryReference *mr);
};

#endif
/*******************************************************************************
* Copyright (c) 2000, 2018 IBM Corp. and others
*
* This program and the accompanying materials are made available under
* the terms of the Eclipse Public License 2.0 which accompanies this
* distribution and is available at http://eclipse.org/legal/epl-2.0
* or the Apache License, Version 2.0 which accompanies this distribution
* and is available at https://www.apache.org/licenses/LICENSE-2.0.
*
* This Source Code may also be made available under the following Secondary
* Licenses when the conditions for such availability set forth in the
* Eclipse Public License, v. 2.0 are satisfied: GNU General Public License,
* version 2 with the GNU Classpath Exception [1] and GNU General Public
* License, version 2 with the OpenJDK Assembly Exception [2].
*
* [1] https://www.gnu.org/software/classpath/license.html
* [2] http://openjdk.java.net/legal/assembly-exception.html
*
* SPDX-License-Identifier: EPL-2.0 OR Apache-2.0 OR GPL-2.0 WITH Classpath-exception-2.0 OR LicenseRef-GPL-2.0 WITH Assembly-exception
*******************************************************************************/

#ifndef OMR_I386_CODEGENERATOR_INCL
#define OMR_I386_CODEGENERATOR_INCL

/*
* The following #define and typedef must appear before any #includes in this file
*/
#ifndef OMR_CODEGENERATOR_CONNECTOR
#define OMR_CODEGENERATOR_CONNECTOR
namespace OMR { namespace X86 { namespace I386 { class CodeGenerator; } } }
namespace OMR { typedef OMR::X86::I386::CodeGenerator CodeGeneratorConnector; }
#else
#error OMR::X86::I386::CodeGenerator expected to be a primary connector, but a OMR connector is already defined
#endif

#include "compiler/x/codegen/OMRCodeGenerator.hpp"

#include "codegen/RegisterConstants.hpp"  // for TR_GlobalRegisterNumber

class TR_RegisterCandidate;
namespace TR { class Block; }
namespace TR { class Node; }
template <class T> class TR_LinkHead;

namespace OMR
{

namespace X86
{

namespace I386
{

class OMR_EXTENSIBLE CodeGenerator : public OMR::X86::CodeGenerator
{

public:

CodeGenerator();

virtual TR::Register *longClobberEvaluate(TR::Node *node);

TR_GlobalRegisterNumber pickRegister(TR_RegisterCandidate *, TR::Block * *, TR_BitVector &, TR_GlobalRegisterNumber &, TR_LinkHead<TR_RegisterCandidate> *candidates);

using OMR::X86::CodeGenerator::getMaximumNumberOfGPRsAllowedAcrossEdge;
int32_t getMaximumNumberOfGPRsAllowedAcrossEdge(TR::Node *);

bool internalPointerSupportImplemented() {return true;}
uint32_t getRegisterMapInfoBitsMask() {return 0x00ff0000;}

bool codegenMulDecomposition(int64_t multiplier);

};

} // namespace I386

} // namespace X86

} // namespace OMR

#endif
/*******************************************************************************
* Copyright (c) 2000, 2018 IBM Corp. and others
*
* This program and the accompanying materials are made available under
* the terms of the Eclipse Public License 2.0 which accompanies this
* distribution and is available at http://eclipse.org/legal/epl-2.0
* or the Apache License, Version 2.0 which accompanies this distribution
* and is available at https://www.apache.org/licenses/LICENSE-2.0.
*
* This Source Code may also be made available under the following Secondary
* Licenses when the conditions for such availability set forth in the
* Eclipse Public License, v. 2.0 are satisfied: GNU General Public License,
* version 2 with the GNU Classpath Exception [1] and GNU General Public
* License, version 2 with the OpenJDK Assembly Exception [2].
*
* [1] https://www.gnu.org/software/classpath/license.html
* [2] http://openjdk.java.net/legal/assembly-exception.html
*
* SPDX-License-Identifier: EPL-2.0 OR Apache-2.0 OR GPL-2.0 WITH Classpath-exception-2.0 OR LicenseRef-GPL-2.0 WITH Assembly-exception
*******************************************************************************/

#ifndef OMR_AMD64_CODEGENERATOR_INCL
#define OMR_AMD64_CODEGENERATOR_INCL

/*
* The following #define and typedef must appear before any #includes in this file
*/
#ifndef OMR_CODEGENERATOR_CONNECTOR
#define OMR_CODEGENERATOR_CONNECTOR
namespace OMR { namespace X86 { namespace AMD64 { class CodeGenerator; } } }
namespace OMR { typedef OMR::X86::AMD64::CodeGenerator CodeGeneratorConnector; }
#else
#error OMR::X86::AMD64::CodeGenerator expected to be a primary connector, but a OMR connector is already defined
#endif

#include "compiler/x/codegen/OMRCodeGenerator.hpp"

#include "codegen/RealRegister.hpp"       // for TR::RealRegister::NumRegisters
#include "codegen/RegisterConstants.hpp"  // for TR_GlobalRegisterNumber

namespace TR { class ILOpCode; }
namespace TR { class Node; }

namespace OMR
{

namespace X86
{

namespace AMD64
{

class OMR_EXTENSIBLE CodeGenerator : public OMR::X86::CodeGenerator
{
public:

CodeGenerator();

virtual TR::Register *longClobberEvaluate(TR::Node *node);

TR_GlobalRegisterNumber getLinkageGlobalRegisterNumber(int8_t linkageRegisterIndex, TR::DataType type);
TR_BitVector *getGlobalGPRsPreservedAcrossCalls(){ return &_globalGPRsPreservedAcrossCalls; }
TR_BitVector *getGlobalFPRsPreservedAcrossCalls(){ return &_globalFPRsPreservedAcrossCalls; }

using OMR::X86::CodeGenerator::getMaximumNumberOfGPRsAllowedAcrossEdge;
int32_t getMaximumNumberOfGPRsAllowedAcrossEdge(TR::Node *);

bool opCodeIsNoOpOnThisPlatform(TR::ILOpCode &opCode);

bool internalPointerSupportImplemented() { return true; }

protected:

TR_BitVector _globalGPRsPreservedAcrossCalls;
TR_BitVector _globalFPRsPreservedAcrossCalls;

private:

TR_GlobalRegisterNumber _gprLinkageGlobalRegisterNumbers[TR::RealRegister::NumRegisters], _fprLinkageGlobalRegisterNumbers[TR::RealRegister::NumRegisters];
void initLinkageToGlobalRegisterMap();
};

} // namespace AMD64

} // namespace X86

} // namespace OMR

#endif
/*******************************************************************************
* Copyright (c) 2000, 2018 IBM Corp. and others
*
* This program and the accompanying materials are made available under
* the terms of the Eclipse Public License 2.0 which accompanies this
* distribution and is available at http://eclipse.org/legal/epl-2.0
* or the Apache License, Version 2.0 which accompanies this distribution
* and is available at https://www.apache.org/licenses/LICENSE-2.0.
*
* This Source Code may also be made available under the following Secondary
* Licenses when the conditions for such availability set forth in the
* Eclipse Public License, v. 2.0 are satisfied: GNU General Public License,
* version 2 with the GNU Classpath Exception [1] and GNU General Public
* License, version 2 with the OpenJDK Assembly Exception [2].
*
* [1] https://www.gnu.org/software/classpath/license.html
* [2] http://openjdk.java.net/legal/assembly-exception.html
*
* SPDX-License-Identifier: EPL-2.0 OR Apache-2.0 OR GPL-2.0 WITH Classpath-exception-2.0 OR LicenseRef-GPL-2.0 WITH Assembly-exception
*******************************************************************************/

#ifndef OMR_CODEGENERATOR_INCL
#define OMR_CODEGENERATOR_INCL

/*
* The following #define and typedef must appear before any #includes in this file
*/
#ifndef OMR_CODEGENERATOR_CONNECTOR
#define OMR_CODEGENERATOR_CONNECTOR
namespace OMR { class CodeGenerator; }
namespace OMR { typedef OMR::CodeGenerator CodeGeneratorConnector; }
#endif

#include <limits.h>                             // for INT_MAX, etc
#include <stddef.h>                             // for NULL, size_t
#include <stdint.h>                             // for uint8_t, etc
#include <map>
#include "codegen/CodeGenPhase.hpp"             // for CodeGenPhase
#include "codegen/FrontEnd.hpp"                 // for feGetEnv
#include "codegen/LinkageConventionsEnum.hpp"
#include "codegen/RecognizedMethods.hpp"
#include "codegen/RegisterConstants.hpp"
#include "codegen/StorageInfo.hpp"
#include "codegen/TreeEvaluator.hpp"
#include "compile/Compilation.hpp"              // for Compilation
#include "control/Options.hpp"
#include "control/Options_inlines.hpp"
#include "cs2/hashtab.h"                        // for HashTable, etc
#include "env/CompilerEnv.hpp"                  // for TR::Host
#include "env/ObjectModel.hpp"                  // for ObjectModel
#include "env/TRMemory.hpp"                     // for Allocator, etc
#include "env/jittypes.h"
#include "il/DataTypes.hpp"                     // for DataTypes, etc
#include "il/ILOpCodes.hpp"                     // for ILOpCodes
#include "il/ILOps.hpp"                         // for TR::ILOpCode
#include "il/Node.hpp"                          // for vcount_t, etc
#include "infra/Array.hpp"                      // for TR_Array
#include "infra/Assert.hpp"                     // for TR_ASSERT
#include "infra/Flags.hpp"                      // for flags32_t, etc
#include "infra/HashTab.hpp"                    // for TR_HashTab, etc
#include "infra/Link.hpp"
#include "infra/List.hpp"                       // for List, etc
#include "infra/TRlist.hpp"
#include "infra/Random.hpp"
#include "infra/Stack.hpp"                      // for TR_Stack
#include "optimizer/Dominators.hpp"             // for TR_Dominators
#include "ras/DebugCounter.hpp"
#include "runtime/Runtime.hpp"
#include "codegen/StaticRelocation.hpp"

#define OPT_DETAILS_CA "O^O COMPLETE ALIASING: "

#define NEED_CC(n) (n->nodeRequiresConditionCodes())

class TR_BackingStore;
class TR_BitVector;
class TR_GCStackMap;
class TR_InterferenceGraph;
class TR_LiveReference;
class TR_LiveRegisters;
class TR_OSRMethodData;
class TR_PseudoRegister;
class TR_RegisterCandidate;
class TR_RegisterCandidates;
namespace TR { class Relocation; }
namespace TR { class RelocationDebugInfo; }
class TR_ResolvedMethod;
class TR_ScratchRegisterManager;
namespace TR { class GCStackAtlas; }
namespace OMR { class RegisterUsage; }
namespace TR { class AheadOfTimeCompile; }
namespace TR { class AutomaticSymbol; }
namespace TR { class Block; }
namespace TR { class CodeCache; }
namespace TR { class CodeGenerator; }
namespace TR { class Instruction; }
namespace TR { class LabelSymbol; }
namespace TR { class Linkage; }
namespace TR { class MemoryReference; }
namespace TR { class RealRegister; }
namespace TR { class Recompilation; }
namespace TR { class Register; }
namespace TR { class RegisterDependencyConditions; }
namespace TR { class RegisterIterator; }
namespace TR { class RegisterPair; }
namespace TR { class Snippet; }
namespace TR { class Symbol; }
namespace TR { class SymbolReference; }
namespace TR { class SymbolReferenceTable; }
namespace TR { class TreeTop; }

typedef TR::SparseBitVector SharedSparseBitVector;

enum TR_SpillKinds // For register pressure simulation
{
// Mandatory spill kinds are certain to cause a spill to memory
//
TR_hprSpill,       // zGryphon 390 All integer highword regs
TR_gprSpill,       // All integer regs
TR_fprSpill,       // All floating-point regs
TR_vrfSpill,       // All vector regs
TR_vmThreadSpill,  // The VMThread reg
TR_numMandatorySpillKinds,

// Probable spill kinds are likely to cause a spill to memory, but may spill
// to another free register instead
//
TR_volatileSpill   // Regs not preserved across calls
= TR_numMandatorySpillKinds,
#if defined(TR_TARGET_X86)
TR_edxSpill,       // Used by multiply and divide
#endif
#if defined(TR_TARGET_S390)
TR_litPoolSpill,
TR_staticBaseSpill,
TR_gpr0Spill,
TR_gpr1Spill,
TR_gpr2Spill,
TR_gpr3Spill,
TR_gpr4Spill,
TR_gpr5Spill,
TR_gpr6Spill,
TR_gpr7Spill,
TR_gpr8Spill,
TR_gpr9Spill,
TR_gpr10Spill,
TR_gpr11Spill,
TR_gpr12Spill,
TR_gpr13Spill,
TR_gpr14Spill,
TR_gpr15Spill,
#endif
TR_linkageSpill,    // Regs used to pass arguments
TR_numProbableSpillKinds, // Includes the mandatory spills

// The other spill kinds are more likely to cause register shuffling
//
#if defined(TR_TARGET_X86)
TR_eaxSpill       // Used by multiply and divide
= TR_numProbableSpillKinds,
TR_ecxSpill,       // Used by shifts
TR_numSpillKinds
#else
TR_numSpillKinds
= TR_numProbableSpillKinds
#endif
};

enum TR_NOPKind
{
TR_NOPStandard,     // Used to generate a normal NOP instruction
TR_NOPEndGroup,     // Used to generate a NOP that ends an instruction group (P6 uses this)
TR_ProbeNOP         // Used in P8 and higher for RI
};

class TR_ClobberEvalData
{

// NOTE:
// No TR_Memory type defined for this class
// since original use was to use as a stack-alloc'd object
// If you're thinking of using it more widely, please
// update the class definition to include something like:
//    TR_ALLOC(TR_Memory::ClobberEvalData)

private:
flags8_t  _flags;

public:
TR_ClobberEvalData()
{ }

bool isPair()
{
return _flags.testAny(Pair);
}

bool canClobberLowWord()
{
return _flags.testAny(ClobberLowWord);
};

bool canClobberHighWord()
{
return _flags.testAny(ClobberHighWord);
}

void setPair()
{
_flags.set(Pair);
}

void setClobberLowWord()
{
_flags.set(ClobberLowWord);
}

void setClobberHighWord()
{
_flags.set(ClobberHighWord);
}

private:

enum
{
Pair = 0x01,
ClobberLowWord = 0x02,
ClobberHighWord = 0x04
};

};

namespace TR
{
enum AOTRelocationPositionRequest
{
AOTRelocationAtFront,
AOTRelocationAtBack,
};
}

TR::Node* generatePoisonNode(TR::Compilation *comp, TR::Block *currentBlock, TR::SymbolReference *liveAutoSymRef);



namespace OMR
{

class /*OMR_EXTENSIBLE*/ CodeGenerator
{
private:

TR::Compilation *_compilation;
TR_Memory *_trMemory;

TR::Machine *_machine;

TR_BitVector *_liveLocals;
TR::TreeTop *_currentEvaluationTreeTop;
TR::Block *_currentEvaluationBlock;

uint32_t _prePrologueSize;

TR::Instruction *_implicitExceptionPoint;
bool areMergeableGuards(TR::Instruction *earlierGuard, TR::Instruction *laterGuard);

protected:

TR_BitVector *_localsThatAreStored;
int32_t _numLocalsWhenStoreAnalysisWasDone;
TR_HashTabInt _uncommmonedNodes;               // uncommoned nodes keyed by the original nodes
List<TR_Pair<TR::Node, int32_t> > _ialoadUnneeded;

public:

TR_ALLOC(TR_Memory::CodeGenerator)

inline TR::CodeGenerator *self();

TR_StackMemory trStackMemory();
TR_Memory *trMemory() { return _trMemory; }
TR_HeapMemory trHeapMemory() { return _trMemory; }
TR::Machine *machine() { return _machine; }
TR::Compilation *comp() { return _compilation; }
TR_FrontEnd *fe();
TR_Debug *getDebug();

void uncommonCallConstNodes();

void preLowerTrees();
void postLowerTrees() {}

TR::TreeTop *lowerTree(TR::Node *root, TR::TreeTop *tt);
void lowerTrees();
void lowerTreesWalk(TR::Node * parent, TR::TreeTop * treeTop, vcount_t visitCount);

void lowerTreeIfNeeded(TR::Node *node, int32_t childNumber, TR::Node *parent, TR::TreeTop *tt);

void lowerTreesPreTreeTopVisit(TR::TreeTop *tt, vcount_t visitCount);
void lowerTreesPostTreeTopVisit(TR::TreeTop *tt, vcount_t visitCount);

void lowerTreesPreChildrenVisit(TR::Node * parent, TR::TreeTop * treeTop, vcount_t visitCount);
void lowerTreesPostChildrenVisit(TR::Node * parent, TR::TreeTop * treeTop, vcount_t visitCount);

void lowerTreesPropagateBlockToNode(TR::Node *node);

void setUpForInstructionSelection();
void doInstructionSelection();
void createStackAtlas();

void beginInstructionSelection() {}
void endInstructionSelection() {}

bool use64BitRegsOn32Bit();


TR::Register *evaluate(TR::Node *node);

// Main entry point for code generation from IL trees.  Override this function
// to provide customized code generation functionality.
//
bool generateCodeFromIL();

uint32_t getPrePrologueSize() {return _prePrologueSize;}
uint32_t setPrePrologueSize(uint32_t s) {return (_prePrologueSize = s);}

TR_BitVector *getLiveLocals() {return _liveLocals;}
TR_BitVector *setLiveLocals(TR_BitVector *v) {return (_liveLocals = v);}

/**
* @brief Returns the first TR::Instruction in the stream of instructions for
*        this method.  This instruction's "previous" link should be NULL.
*
* @return The first instruction in this method; NULL if not yet set.
*/
TR::Instruction *getFirstInstruction() {return _firstInstruction;}

/**
* @brief Sets the first TR::Instruction in the stream of instructions for
*        this method.  This instruction's "previous" link should be NULL.
*
* @return The instruction being set.
*/
TR::Instruction *setFirstInstruction(TR::Instruction *fi) {return (_firstInstruction = fi);}

/**
* @brief Returns the last TR::Instruction in the stream of instructions for
*        this method.  This instruction's "next" link should be NULL.
*
* @return The last instruction in this method; NULL if not yet set.
*/
TR::Instruction *getAppendInstruction() {return _appendInstruction;}

/**
* @brief Sets the last TR::Instruction in the stream of instructions for
*        this method.  This instruction's "next" link should be NULL.
*
* @return The instruction being set.
*/
TR::Instruction *setAppendInstruction(TR::Instruction *ai) {return (_appendInstruction = ai);}

TR::TreeTop *getCurrentEvaluationTreeTop() {return _currentEvaluationTreeTop;}
TR::TreeTop *setCurrentEvaluationTreeTop(TR::TreeTop *tt) {return (_currentEvaluationTreeTop = tt);}

TR::Block *getCurrentEvaluationBlock() {return _currentEvaluationBlock;}
TR::Block *setCurrentEvaluationBlock(TR::Block *tt) {return (_currentEvaluationBlock = tt);}

TR::Instruction *getImplicitExceptionPoint() {return _implicitExceptionPoint;}
TR::Instruction *setImplicitExceptionPoint(TR::Instruction *p) {return (_implicitExceptionPoint = p);}

void setNextAvailableBlockIndex(int32_t blockIndex) {}
int32_t getNextAvailableBlockIndex() { return -1; }

bool supportsMethodEntryPadding() { return true; }
bool mustGenerateSwitchToInterpreterPrePrologue() { return false; }
bool buildInterpreterEntryPoint() { return false; }
void generateCatchBlockBBStartPrologue(TR::Node *node, TR::Instruction *fenceInstruction) { return; }
bool supportsUnneededLabelRemoval() { return true; }
bool allowSplitWarmAndColdBlocks() { return false; }

TR_HasRandomGenerator randomizer;

bool supportsAtomicAdd() {return false;}
bool hasTMEvaluator()    {return false;}

// --------------------------------------------------------------------------
// Infrastructure
//
TR_PersistentMemory * trPersistentMemory();

TR::SymbolReferenceTable * getSymRefTab() { return _symRefTab; }
TR::SymbolReference * getSymRef(TR_RuntimeHelper h);

TR::SymbolReferenceTable *symRefTab() { return _symRefTab; }
TR::Linkage *linkage() {return _bodyLinkage;}


// --------------------------------------------------------------------------
// Code Generator Phases
//
void generateCode();
void doRegisterAssignment(TR_RegisterKinds kindsToAssign);  // no virt
void doBinaryEncoding(); // no virt, no cast
void doPeephole() { return; } // no virt, no cast, default avail
bool hasComplexAddressingMode() { return false; } // no virt, default
void removeUnusedLocals();

void identifyUnneededByteConvNodes(TR::Node*, TR::TreeTop *, vcount_t, TR::DataType);
void identifyUnneededByteConvNodes();

bool afterRA() { return _afterRA; }
TR::CodeGenPhase& getCodeGeneratorPhase() {return _codeGenPhase;}

void prepareNodeForInstructionSelection(TR::Node*node);
void remapGCIndicesInInternalPtrFormat();
void processRelocations();

void findAndFixCommonedReferences();
void findCommonedReferences(TR::Node*node, TR::TreeTop *treeTop);
void processReference(TR::Node*reference, TR::Node*parent, TR::TreeTop *treeTop);
void spillLiveReferencesToTemps(TR::TreeTop *insertionTree, std::list<TR::SymbolReference*, TR::typed_allocator<TR::SymbolReference*, TR::Allocator> >::iterator firstAvailableSpillTemp);
void needSpillTemp(TR_LiveReference * cursor, TR::Node*parent, TR::TreeTop *treeTop);

friend void OMR::CodeGenPhase::performEmitSnippetsPhase(TR::CodeGenerator*, TR::CodeGenPhase *);
friend void OMR::CodeGenPhase::performCleanUpFlagsPhase(TR::CodeGenerator * cg, TR::CodeGenPhase * phase);

// --------------------------------------------------------------------------
// Hardware profiling
//
void createHWPRecords() {}

// --------------------------------------------------------------------------
// Tree evaluation
//
static TR_TreeEvaluatorFunctionPointer *getTreeEvaluatorTable() {return _nodeToInstrEvaluators;}

int32_t getEvaluationPriority(TR::Node*node);
int32_t whichNodeToEvaluate(TR::Node*first, TR::Node* second);      // Decide which of two nodes should be evaluated first.
int32_t whichChildToEvaluate(TR::Node*node);    // Decide which child of the given node should be evaluated first.

// Convert a multiply tree node to a shift if possible.
// Note that for a negative constant the negation of the shifted value is
// not done, the caller must insert it after evaluating the shift node.
// Returns "true" if the conversion was done.
//
bool convertMultiplyToShift(TR::Node*node);

// See if the tree represents an operation on a memory location.
//
bool isMemoryUpdate(TR::Node*node);

// Find the magic values for turning a divide by a constant into multiply and shift
void compute32BitMagicValues(int32_t  d, int32_t *m, int32_t *s);
void compute64BitMagicValues(int64_t  d, int64_t *m, int64_t *s);
uint64_t computeUnsigned64BitMagicValues(uint64_t d, int32_t *s, int32_t* a);

rcount_t incReferenceCount(TR::Node*node);
rcount_t decReferenceCount(TR::Node*node);
rcount_t recursivelyDecReferenceCount(TR::Node*node);
void evaluateChildrenWithMultipleRefCount(TR::Node*node);

void incRefCountForOpaquePseudoRegister(TR::Node * node, TR::CodeGenerator * cg, TR::Compilation * comp) {}

void startUsingRegister(TR::Register *reg);
void stopUsingRegister(TR::Register *reg);

void setCurrentBlockIndex(int32_t blockIndex) { } // no virt, default, cast
int32_t getCurrentBlockIndex() { return -1; } // no virt, default

TR::Instruction *lastInstructionBeforeCurrentEvaluationTreeTop()
{
return _lastInstructionBeforeCurrentEvaluationTreeTop;
}
void setLastInstructionBeforeCurrentEvaluationTreeTop(TR::Instruction *instr)
{
_lastInstructionBeforeCurrentEvaluationTreeTop = instr;
}

bool useClobberEvaluate();

bool canClobberNodesRegister(TR::Node* node, uint16_t count = 1,
TR_ClobberEvalData * data = NULL, bool ignoreRefCount = false);
bool isRegisterClobberable(TR::Register *reg, uint16_t count);

// ilgen
bool ilOpCodeIsSupported(TR::ILOpCodes); // no virt, default, cast



TR::Recompilation *allocateRecompilationInfo() { return NULL; }

/**
* @brief This determines if it is necessary to emit a prefetch instruction.
*        If so, it also emits the prefetch instruction.
*
* @param node The node being evaluated.
* @param targetRegister A register holding the address where the prefetch location is generated from.
*/
void insertPrefetchIfNecessary(TR::Node *node, TR::Register *targetRegister);

// --------------------------------------------------------------------------
// Capabilities
//
bool supports32bitAiadd() {return true;}  // no virt, default
bool supportsMergingGuards() {return false;} // no virt, default

// --------------------------------------------------------------------------
// Z only
//

TR::list<uint8_t*>& getBreakPointList() {return _breakPointList;}
void addBreakPointAddress(uint8_t *b) {_breakPointList.push_front(b);}

bool AddArtificiallyInflatedNodeToStack(TR::Node* n);

// Used to model register liveness without Future Use Count.
bool isInternalControlFlowReg(TR::Register *reg) {return false;}  // no virt, default
void startInternalControlFlow(TR::Instruction *instr) {} // no virt, default, cast
void endInternalControlFlow(TR::Instruction *instr) {} // no virt, default, cast



// --------------------------------------------------------------------------
// P only
//
intptrj_t hiValue(intptrj_t address); // no virt, 1 impl

// --------------------------------------------------------------------------
// Lower trees
//
void rematerializeCmpUnderTernary(TR::Node*node);
bool yankIndexScalingOp() {return false;} // no virt, default

void cleanupFlags(TR::Node*node);

bool shouldYankCompressedRefs() { return false; } // no virt, default, cast
bool materializesHeapBase() { return true; } // no virt, default, cast
bool canFoldLargeOffsetInAddressing() { return false; } // no virt, default, cast

void insertDebugCounters();


// --------------------------------------------------------------------------
// Instruction selection
//
void setUpStackSizeForCallNode(TR::Node * node);

// --------------------------------------------------------------------------
// Debug counters
//
TR::Instruction *generateDebugCounter(const char *name, int32_t delta = 1, int8_t fidelity = TR::DebugCounter::Undetermined);
TR::Instruction *generateDebugCounter(const char *name, TR::Register *deltaReg, int8_t fidelity = TR::DebugCounter::Undetermined);
TR::Instruction *generateDebugCounter(TR::Instruction *cursor, const char *name, int32_t delta = 1, int8_t fidelity = TR::DebugCounter::Undetermined, int32_t staticDelta = 1);
TR::Instruction *generateDebugCounter(TR::Instruction *cursor, const char *name, TR::Register *deltaReg, int8_t fidelity = TR::DebugCounter::Undetermined, int32_t staticDelta = 1);
TR::Instruction *generateDebugCounter(const char *name, TR::RegisterDependencyConditions &cond, int32_t delta = 1, int8_t fidelity = TR::DebugCounter::Undetermined, int32_t staticDelta = 1, TR::Instruction *cursor = NULL);
TR::Instruction *generateDebugCounter(const char *name, TR::Register *deltaReg, TR::RegisterDependencyConditions &cond, int8_t fidelity = TR::DebugCounter::Undetermined, int32_t staticDelta = 1, TR::Instruction *cursor = NULL);
TR::Instruction *generateDebugCounter(const char *name, TR_ScratchRegisterManager &srm, int32_t delta = 1, int8_t fidelity = TR::DebugCounter::Undetermined, int32_t staticDelta = 1, TR::Instruction *cursor = NULL);
TR::Instruction *generateDebugCounter(const char *name, TR::Register *deltaReg, TR_ScratchRegisterManager &srm, int8_t fidelity = TR::DebugCounter::Undetermined, int32_t staticDelta = 1, TR::Instruction *cursor = NULL);

TR::Instruction *generateDebugCounterBump(TR::Instruction *cursor, TR::DebugCounterBase *counter, int32_t delta, TR::RegisterDependencyConditions *cond){ return cursor; } // no virt, default, cast
TR::Instruction *generateDebugCounterBump(TR::Instruction *cursor, TR::DebugCounterBase *counter, TR::Register *deltaReg, TR::RegisterDependencyConditions *cond){ return cursor; } // no virt, default, cast
TR::Instruction *generateDebugCounterBump(TR::Instruction *cursor, TR::DebugCounterBase *counter, int32_t delta, TR_ScratchRegisterManager &srm){ return cursor; } // no virt, default, cast
TR::Instruction *generateDebugCounterBump(TR::Instruction *cursor, TR::DebugCounterBase *counter, TR::Register *deltaReg, TR_ScratchRegisterManager &srm){ return cursor; } // no virt, default, cast

// NOT USED?
bool supportsDebugCounters(TR::DebugCounterInjectionPoint injectionPoint){ return injectionPoint == TR::TR_BeforeCodegen; } // no virt, default

void incrementEventCounter(TR::Node *node, TR::SymbolReference *symRef, TR::CodeGenerator *cg) { TR_ASSERT(0,"not implemented\n");} // no virt, default

// --------------------------------------------------------------------------
// Linkage
//
void initializeLinkage(); // no virt, default, cast
TR::Linkage *createLinkage(TR_LinkageConventions lc); // no virt, default, cast
TR::Linkage *createLinkageForCompilation();

TR::Linkage *getLinkage() {return _bodyLinkage;}
TR::Linkage *setLinkage(TR::Linkage * l) {return (_bodyLinkage = l);}

TR::Linkage *getLinkage(TR_LinkageConventions lc);
TR::Linkage *setLinkage(TR_LinkageConventions lc, TR::Linkage * l) {return _linkages[lc] = l;}

// --------------------------------------------------------------------------
// Optimizer, code generator capabilities
//
int32_t getPreferredLoopUnrollFactor() {return -1;} // no virt, default

/**
* @brief Answers whether the provided recognized method should be inlined by an
*        inliner optimization.
* @param method : the recognized method to consider
* @return true if inlining should be suppressed; false otherwise
*/
bool suppressInliningOfRecognizedMethod(TR::RecognizedMethod method) {return false;}

// --------------------------------------------------------------------------
// Optimizer, not code generator
//
bool getGRACompleted() { return _flags4.testAny(GRACompleted); }
void setGRACompleted() { _flags4.set(GRACompleted); }

bool getSupportsProfiledInlining() { return _flags4.testAny(SupportsProfiledInlining);}
void setSupportsProfiledInlining() { _flags4.set(SupportsProfiledInlining);}
bool supportsInliningOfIsInstance() {return false;} // no virt, default
bool supportsPassThroughCopyToNewVirtualRegister() { return false; } // no virt, default

uint8_t getSizeOfCombinedBuffer() {return 0;} // no virt, default

bool doRematerialization() {return false;} // no virt, default

// --------------------------------------------------------------------------
// Architecture, not code generator
//
int16_t getMinShortForLongCompareNarrower() { return SHRT_MIN; } // no virt, default
int8_t getMinByteForLongCompareNarrower() { return SCHAR_MIN; } // no virt, default

bool branchesAreExpensive() { return true; } // no virt, default
bool opCodeIsNoOp(TR::ILOpCode &opCode); // no virt, 1 impl
bool opCodeIsNoOpOnThisPlatform(TR::ILOpCode &opCode) {return false;} // no virt

bool supportsSinglePrecisionSQRT() {return false;} // no virt
bool supportsFusedMultiplyAdd() {return false;} // no virt
bool supportsNegativeFusedMultiplyAdd() {return false;} // no virt

bool supportsComplexAddressing() {return false;} // no virt
bool canBeAffectedByStoreTagStalls() { return false; } // no virt, default

bool isMaterialized(TR::Node *); // no virt, cast
bool shouldValueBeInACommonedNode(int64_t) { return false; } // no virt, cast
bool materializesLargeConstants() { return false; }

bool canUseImmedInstruction(int64_t v) {return false;} // no virt
bool needsNormalizationBeforeShifts() { return false; } // no virt, cast

uint32_t getNumberBytesReadInaccessible() { return _numberBytesReadInaccessible; }
uint32_t getNumberBytesWriteInaccessible() { return _numberBytesWriteInaccessible; }

bool codegenSupportsUnsignedIntegerDivide() {return false;} // no virt
bool mulDecompositionCostIsJustified(int numOfOperations, char bitPosition[], char operationType[], int64_t value); // no virt

bool codegenSupportsLoadlessBNDCheck() {return false;} // no virt, cast

// called to determine if multiply decomposition exists in platform codegen so that codegen sequences are used
// instead of the IL transformed multiplies
bool codegenMulDecomposition(int64_t multiplier) {return false;} // no virt

// --------------------------------------------------------------------------
// FrontEnd, not code generator
//
bool getSupportsNewObjectAlignment() { return false; } // no virt
bool getSupportsTenuredObjectAlignment() { return false; } // no virt
bool isObjectOfSizeWorthAligning(uint32_t size) { return false; } // no virt

// J9
int32_t getInternalPtrMapBit() { return 31;} // no virt

uint32_t getMaxObjectSizeGuaranteedNotToOverflow() { return _maxObjectSizeGuaranteedNotToOverflow; }

// --------------------------------------------------------------------------
// OSR, not code generator
//
void addToOSRTable(TR::Instruction *);
void addToOSRTable(int32_t instructionPC, TR_ByteCodeInfo &bcInfo);

int genStoreForSymListArray(TR_Array<List<TR::SymbolReference> >* symListArray,
TR_OSRMethodData* osrMethodData, TR::TreeTop*& insertionPoint, TR::Node* callNode,
TR::Node* osrBufferNode, TR::Node* osrScratchBufferNode, TR::Node* osrFrameIndex,
int32_t& scratchBufferOffset);

void eliminateLoadsOfLocalsThatAreNotStored(TR::Node *node, int32_t childNum);

// --------------------------------------------------------------------------
// FE capability, not code generator
//
bool internalPointerSupportImplemented() {return false;} // no virt, cast
bool supportsInternalPointers();

// --------------------------------------------------------------------------
// Behaviour on a particular arch, not code generator
//
bool supportsLongRegAllocation() {return false;}  // no virt

// --------------------------------------------------------------------------
// GC
//
TR::GCStackAtlas *getStackAtlas() {return _stackAtlas;}
TR::GCStackAtlas *setStackAtlas(TR::GCStackAtlas *p) {return (_stackAtlas = p);}
TR_GCStackMap *getMethodStackMap() {return _methodStackMap;}
TR_GCStackMap *setMethodStackMap(TR_GCStackMap *m) {return (_methodStackMap = m);}
void addToAtlas(TR::Instruction *);
void buildGCMapsForInstructionAndSnippet(TR::Instruction *instr);
TR_GCStackMap *buildGCMapForInstruction(TR::Instruction *instr);
void buildRegisterMapForInstruction(TR_GCStackMap *map);
// IA32 only?
uint32_t getRegisterMapInfoBitsMask() {return 0;} // no virt, cast

// --------------------------------------------------------------------------
// Shrink wrapping
//
TR_BitVector *getPreservedRegsInPrologue() {return _preservedRegsInPrologue;}
TR_BitVector *setPreservedRegsInPrologue(TR_BitVector *v) {return (_preservedRegsInPrologue = v);}

int32_t getLowestSavedRegister() {return _lowestSavedReg;}
void setLowestSavedRegister(int32_t v) {_lowestSavedReg = v;}

bool processInstruction(TR::Instruction *instr, TR_BitVector **registerUsageInfo, int32_t &blockNum, int32_t &isFence, bool traceIt) {return false;} // no virt, cast
uint32_t isPreservedRegister(int32_t regIndex) { return 0; } // no virt, cast
bool isReturnInstruction(TR::Instruction *instr) { return false; } // no virt, cast
bool isBranchInstruction(TR::Instruction *instr) { return false; } // no virt, cast
int32_t isFenceInstruction(TR::Instruction *instr) { return false; } // no virt
bool isAlignmentInstruction(TR::Instruction *instr) { return false; } // no virt
bool isLabelInstruction(TR::Instruction *instr) { return false; } // no virt
TR::Instruction *splitEdge(TR::Instruction *cursor, bool isFallThrough, bool needsJump, TR::Instruction *newSplitLabel, TR::list<TR::Instruction*> *jmpInstrs, bool firstJump = false) { return NULL; } // no virt
TR::Instruction *splitBlockEntry(TR::Instruction *instr) { return NULL; } // no virt
int32_t computeRegisterSaveDescription(TR_BitVector *regs, bool populateInfo = false) { return 0; } // no virt
void processIncomingParameterUsage(TR_BitVector **registerUsageInfo, int32_t blockNum) { return; } // no virt
void updateSnippetMapWithRSD(TR::Instruction *cur, int32_t rsd) { return; } // no virt
bool isTargetSnippetOrOutOfLine(TR::Instruction *instr, TR::Instruction **start, TR::Instruction **end) { return false; }

// --------------------------------------------------------------------------
// Method frame building
//
uint32_t getLargestOutgoingArgSize()           {return _largestOutgoingArgSize;}
uint32_t setLargestOutgoingArgSize(uint32_t s) {return (_largestOutgoingArgSize = s);}

int32_t getFrameSizeInBytes() {return _frameSizeInBytes;}
int32_t setFrameSizeInBytes(int32_t fs) {return (_frameSizeInBytes = fs);}

int32_t getRegisterSaveDescription() {return _registerSaveDescription;}
int32_t setRegisterSaveDescription(int32_t rsd) {return (_registerSaveDescription = rsd);}

TR_InterferenceGraph *getLocalsIG() {return _localsIG;}
TR_InterferenceGraph *setLocalsIG(TR_InterferenceGraph *ig) {return (_localsIG = ig);}

bool isLeafMethod()      { return _flags1.testAny(IsLeafMethod); }
void setIsLeafMethod()   { _flags1.set(IsLeafMethod); }
void resetIsLeafMethod() { _flags1.reset(IsLeafMethod); }

// --------------------------------------------------------------------------
// Binary encoding code cache
//
uint32_t getEstimatedWarmLength()           {return _estimatedCodeLength;} // DEPRECATED
uint32_t setEstimatedWarmLength(uint32_t l) {return (_estimatedCodeLength = l);} // DEPRECATED

uint32_t getEstimatedCodeLength()           {return _estimatedCodeLength;}
uint32_t setEstimatedCodeLength(uint32_t l) {return (_estimatedCodeLength = l);}

uint8_t *getBinaryBufferStart()           {return _binaryBufferStart;}
uint8_t *setBinaryBufferStart(uint8_t *b) {return (_binaryBufferStart = b);}

uint8_t *getCodeStart();
uint8_t *getCodeEnd()                  {return _binaryBufferCursor;}
uint32_t getCodeLength();

uint8_t *getBinaryBufferCursor() {return _binaryBufferCursor;}
uint8_t *setBinaryBufferCursor(uint8_t *b) { return (_binaryBufferCursor = b); }

uint8_t *alignBinaryBufferCursor();

uint32_t getBinaryBufferLength() {return (uint32_t)(_binaryBufferCursor - _binaryBufferStart - _jitMethodEntryPaddingSize);} // cast explicitly

int32_t getEstimatedSnippetStart() {return _estimatedSnippetStart;}
int32_t setEstimatedSnippetStart(int32_t s) {return (_estimatedSnippetStart = s);}

int32_t getAccumulatedInstructionLengthError() {return _accumulatedInstructionLengthError;}
int32_t setAccumulatedInstructionLengthError(int32_t e) {return (_accumulatedInstructionLengthError = e);}
int32_t addAccumulatedInstructionLengthError(int32_t e) {return (_accumulatedInstructionLengthError += e);}

uint32_t getPreJitMethodEntrySize() {return _preJitMethodEntrySize;}
uint32_t setPreJitMethodEntrySize(uint32_t s) {return (_preJitMethodEntrySize = s);}

uint32_t getJitMethodEntryPaddingSize() {return _jitMethodEntryPaddingSize;}
uint32_t setJitMethodEntryPaddingSize(uint32_t s) {return (_jitMethodEntryPaddingSize = s);}

// --------------------------------------------------------------------------
// Code cache
//
TR::CodeCache * getCodeCache() { return _codeCache; }
void  setCodeCache(TR::CodeCache * codeCache) { _codeCache = codeCache; }
void  reserveCodeCache();
uint8_t * allocateCodeMemory(uint32_t size, bool isCold, bool isMethodHeaderNeeded=true);
uint8_t * allocateCodeMemory(uint32_t warmSize, uint32_t coldSize, uint8_t **coldCode, bool isMethodHeaderNeeded=true);
void  resizeCodeMemory();
void  registerAssumptions() {}

static void syncCode(uint8_t *codeStart, uint32_t codeSize);

void commitToCodeCache() { _committedToCodeCache = true; }
bool committedToCodeCache() { return _committedToCodeCache; }

// --------------------------------------------------------------------------
// Load extensions (Z)

TR::SparseBitVector & getExtendedToInt64GlobalRegisters()  { return _extendedToInt64GlobalRegisters; }

// --------------------------------------------------------------------------
// Live registers
//
void checkForLiveRegisters(TR_LiveRegisters *);
TR_LiveRegisters *getLiveRegisters(TR_RegisterKinds rk) {return _liveRegisters[rk];}
TR_LiveRegisters *setLiveRegisters(TR_LiveRegisters *p, TR_RegisterKinds rk) {return (_liveRegisters[rk] = p);}

void genLiveRealRegisters(TR_RegisterKinds rk, TR_RegisterMask r) {_liveRealRegisters[rk] |= r;}
void killLiveRealRegisters(TR_RegisterKinds rk, TR_RegisterMask r) {_liveRealRegisters[rk] &= ~r;}
TR_RegisterMask getLiveRealRegisters(TR_RegisterKinds rk) {return _liveRealRegisters[rk];}
void resetLiveRealRegisters(TR_RegisterKinds rk) {_liveRealRegisters[rk] = 0;}

uint8_t getSupportedLiveRegisterKinds() {return _supportedLiveRegisterKinds;}
void addSupportedLiveRegisterKind(TR_RegisterKinds rk) {_supportedLiveRegisterKinds |= (uint8_t)(1<<rk);}

// --------------------------------------------------------------------------
// VMThread (shouldn't be common codegen)
//
TR::Register *getVMThreadRegister() {return _vmThreadRegister;}
TR::Register *setVMThreadRegister(TR::Register *vmtr) {return (_vmThreadRegister = vmtr);}

TR::RealRegister *getRealVMThreadRegister() {return _realVMThreadRegister;}
void setRealVMThreadRegister(TR::RealRegister *defvmtr) {_realVMThreadRegister = defvmtr;}

// --------------------------------------------------------------------------
// GRA
//
void addSymbolAndDataTypeToMap(TR::Symbol *symbol, TR::DataType dt);
TR::DataType getDataTypeFromSymbolMap(TR::Symbol *symbol);

bool prepareForGRA(); // no virt, cast

uint32_t getGlobalRegister(TR_GlobalRegisterNumber n) {return _globalRegisterTable[n];}
uint32_t *setGlobalRegisterTable(uint32_t *p) {return (_globalRegisterTable = p);}

TR_GlobalRegisterNumber getGlobalRegisterNumber(uint32_t realReg) { return -1; } // no virt, cast

TR_GlobalRegisterNumber getFirstGlobalGPR() {return 0;}
TR_GlobalRegisterNumber getLastGlobalGPR()  {return _lastGlobalGPR;}
TR_GlobalRegisterNumber setLastGlobalGPR(TR_GlobalRegisterNumber n) {return (_lastGlobalGPR = n);}

TR_GlobalRegisterNumber getFirstGlobalHPR() {return _firstGlobalHPR;}
TR_GlobalRegisterNumber setFirstGlobalHPR(TR_GlobalRegisterNumber n) {return (_firstGlobalHPR = n);}
TR_GlobalRegisterNumber getLastGlobalHPR() {return _lastGlobalHPR;}
TR_GlobalRegisterNumber setLastGlobalHPR(TR_GlobalRegisterNumber n) {return (_lastGlobalHPR = n);}

TR_GlobalRegisterNumber getGlobalHPRFromGPR (TR_GlobalRegisterNumber n) {return 0;} // no virt, cast
TR_GlobalRegisterNumber getGlobalGPRFromHPR (TR_GlobalRegisterNumber n) {return 0;} // no virt

TR_GlobalRegisterNumber getFirstGlobalFPR() {return _lastGlobalGPR + 1;}
TR_GlobalRegisterNumber setFirstGlobalFPR(TR_GlobalRegisterNumber n) {return (_firstGlobalFPR = n);}
TR_GlobalRegisterNumber getLastGlobalFPR() {return _lastGlobalFPR;}
TR_GlobalRegisterNumber setLastGlobalFPR(TR_GlobalRegisterNumber n)  {return (_lastGlobalFPR = n);}

TR_GlobalRegisterNumber getFirstOverlappedGlobalFPR()                          { return _firstOverlappedGlobalFPR    ;}
TR_GlobalRegisterNumber setFirstOverlappedGlobalFPR(TR_GlobalRegisterNumber n) { return _firstOverlappedGlobalFPR = n;}
TR_GlobalRegisterNumber getLastOverlappedGlobalFPR()                           { return _lastOverlappedGlobalFPR     ;}
TR_GlobalRegisterNumber setLastOverlappedGlobalFPR(TR_GlobalRegisterNumber n)  { return _lastOverlappedGlobalFPR = n ;}

TR_GlobalRegisterNumber getFirstGlobalAR() {return _firstGlobalAR;}
TR_GlobalRegisterNumber setFirstGlobalAR(TR_GlobalRegisterNumber n) {return (_firstGlobalAR = n);}
TR_GlobalRegisterNumber getLastGlobalAR() {return _lastGlobalAR;}
TR_GlobalRegisterNumber setLastGlobalAR(TR_GlobalRegisterNumber n) {return (_lastGlobalAR = n);}

TR_GlobalRegisterNumber getFirstGlobalVRF() {return _firstGlobalVRF;}
TR_GlobalRegisterNumber setFirstGlobalVRF(TR_GlobalRegisterNumber n) {return (_firstGlobalVRF = n);}
TR_GlobalRegisterNumber getLastGlobalVRF() {return _lastGlobalVRF;}
TR_GlobalRegisterNumber setLastGlobalVRF(TR_GlobalRegisterNumber n) {return (_lastGlobalVRF= n);}

TR_GlobalRegisterNumber getFirstOverlappedGlobalVRF()                          {return _firstOverlappedGlobalVRF     ;}
TR_GlobalRegisterNumber setFirstOverlappedGlobalVRF(TR_GlobalRegisterNumber n) {return _firstOverlappedGlobalVRF = n ;}
TR_GlobalRegisterNumber getLastOverlappedGlobalVRF()                           {return _lastOverlappedGlobalVRF      ;}
TR_GlobalRegisterNumber setLastOverlappedGlobalVRF(TR_GlobalRegisterNumber n)  {return _lastOverlappedGlobalVRF = n  ;}

bool hasGlobalVRF() { return _firstGlobalVRF != -1 && _lastGlobalVRF != -1; }

void setLast8BitGlobalGPR(TR_GlobalRegisterNumber n) { _last8BitGlobalGPR = n;}

uint16_t getNumberOfGlobalRegisters();

#ifdef TR_HOST_S390
uint16_t getNumberOfGlobalGPRs();
#else
uint16_t getNumberOfGlobalGPRs() {return _lastGlobalGPR + 1;}
#endif
uint16_t getNumberOfGlobalFPRs() {return _lastGlobalFPR - _lastGlobalGPR;}
uint16_t getNumberOfGlobalVRFs() {return _lastGlobalVRF - _firstGlobalVRF;}

uint8_t getGlobalGPRPartitionLimit() {return _globalGPRPartitionLimit;}
uint8_t setGlobalGPRPartitionLimit(uint8_t l) {return (_globalGPRPartitionLimit = l);}

uint8_t getGlobalFPRPartitionLimit() {return _globalFPRPartitionLimit;}
uint8_t setGlobalFPRPartitionLimit(uint8_t l) {return (_globalFPRPartitionLimit = l);}

bool isGlobalGPR(TR_GlobalRegisterNumber n) {return n <= _lastGlobalGPR;}
bool isGlobalHPR(TR_GlobalRegisterNumber n) {return (n >= _firstGlobalHPR && n <= _lastGlobalHPR);}

bool isAliasedGRN(TR_GlobalRegisterNumber n);
TR_GlobalRegisterNumber getOverlappedAliasForGRN(TR_GlobalRegisterNumber n);

void setOverlapOffsetBetweenAliasedGRNs(TR_GlobalRegisterNumber n)
{
TR_ASSERT(n >= 0, "Offset for aliased global register numbers must be positive. Currently: %d", n);
_overlapOffsetBetweenFPRandVRFgrns = n;
}

TR_GlobalRegisterNumber getOverlapOffsetBetweenAliasedGRNs()
{
return _overlapOffsetBetweenFPRandVRFgrns;
}

bool isGlobalVRF(TR_GlobalRegisterNumber n);
bool isGlobalFPR(TR_GlobalRegisterNumber n);

bool is8BitGlobalGPR(TR_GlobalRegisterNumber n) {return n <= _last8BitGlobalGPR;}

TR_GlobalRegisterNumber getLinkageGlobalRegisterNumber(int8_t linkageRegisterIndex, TR::DataType type){ return -1; } // no virt, cast
TR_BitVector *getGlobalGPRsPreservedAcrossCalls(){ return NULL; } // no virt, cast
TR_BitVector *getGlobalFPRsPreservedAcrossCalls(){ return NULL; } // no virt, cast

int32_t getFirstBit(TR_BitVector &bv);
TR_GlobalRegisterNumber pickRegister(TR_RegisterCandidate *, TR::Block * *, TR_BitVector & availableRegisters, TR_GlobalRegisterNumber & highRegisterNumber, TR_LinkHead<TR_RegisterCandidate> *candidates); // no virt
TR_RegisterCandidate *findCoalescenceForRegisterCopy(TR::Node *node, TR_RegisterCandidate *rc, bool *isUnpreferred);
TR_GlobalRegisterNumber findCoalescenceRegisterForParameter(TR::Node *callNode, TR_RegisterCandidate *rc, uint32_t childIndex, bool *isUnpreferred);
TR_RegisterCandidate *findUsedCandidate(TR::Node *node, TR_RegisterCandidate *rc, TR_BitVector *visitedNodes);

bool allowGlobalRegisterAcrossBranch(TR_RegisterCandidate *, TR::Node * branchNode); // no virt
void removeUnavailableRegisters(TR_RegisterCandidate * rc, TR::Block * * blocks, TR_BitVector & availableRegisters) {} // no virt
void setUnavailableRegistersUsage(TR_Array<TR_BitVector>  & liveOnEntryUsage, TR_Array<TR_BitVector>   & liveOnExitUsage) {} // no virt

int32_t getMaximumNumberOfGPRsAllowedAcrossEdge(TR::Node *) { return INT_MAX; } // no virt
int32_t getMaximumNumberOfFPRsAllowedAcrossEdge(TR::Node *) { return INT_MAX; } // no virt
int32_t getMaximumNumberOfVRFsAllowedAcrossEdge(TR::Node *) { return INT_MAX; } // no virt
int32_t getMaximumNumberOfGPRsAllowedAcrossEdge(TR::Block *block); // no virt
int32_t getMaximumNumbersOfAssignableGPRs() { return INT_MAX; } // no virt, cast
int32_t getMaximumNumbersOfAssignableFPRs() { return INT_MAX; } // no virt, cast
int32_t getMaximumNumbersOfAssignableVRs()  { return INT_MAX; } // no virt, cast
virtual bool willBeEvaluatedAsCallByCodeGen(TR::Node *node, TR::Compilation *comp){ return true;}
bool isGlobalRegisterAvailable(TR_GlobalRegisterNumber, TR::DataType) { return true; } // no virt

bool areAssignableGPRsScarce(); // no virt, 1 impl

TR_Array<TR::Register *>& getRegisterArray() {return _registerArray;}

bool needToAvoidCommoningInGRA() {return false;} // no virt

bool considerTypeForGRA(TR::Node *node) {return true;} // no virt
bool considerTypeForGRA(TR::DataType dt) {return true;} // no virt
bool considerTypeForGRA(TR::SymbolReference *symRef) {return true;} // no virt

void enableLiteralPoolRegisterForGRA () {} // no virt
bool excludeInvariantsFromGRAEnabled() { return false; } // no virt

TR_BitVector *getBlocksWithCalls();

// --------------------------------------------------------------------------
// Debug
//
TR::RegisterIterator *getGPRegisterIterator() {return _gpRegisterIterator;}
TR::RegisterIterator *setGPRegisterIterator(TR::RegisterIterator *iter) {return (_gpRegisterIterator = iter);}

TR::RegisterIterator *getFPRegisterIterator() {return _fpRegisterIterator;}
TR::RegisterIterator *setFPRegisterIterator(TR::RegisterIterator *iter) {return (_fpRegisterIterator = iter);}

// X86 only
uint32_t estimateBinaryLength(TR::MemoryReference *) { return 0; } // no virt

#ifdef DEBUG
static void shutdown(TR_FrontEnd *fe, TR::FILE *logFile);
static void dumpSpillStats(TR_FrontEnd *fe);
static void incNumSpilledRegisters()        {_totalNumSpilledRegisters++;}
static void incNumRematerializedConstants() {_totalNumRematerializedConstants++;}
static void incNumRematerializedLocals()    {_totalNumRematerializedLocals++;}
static void incNumRematerializedStatics()   {_totalNumRematerializedStatics++;}
static void incNumRematerializedIndirects() {_totalNumRematerializedIndirects++;}
static void incNumRematerializedAddresses() {_totalNumRematerializedAddresses++;}
static void incNumRematerializedXMMRs()     {_totalNumRematerializedXMMRs++;}
#endif

void dumpDataSnippets(TR::FILE *outFile) {}
void dumpTargetAddressSnippets(TR::FILE *outFile) {}

// --------------------------------------------------------------------------
// Register assignment tracing
//
bool getTraceRAOption(uint32_t mask);
void traceRAInstruction(TR::Instruction *instr);
void tracePreRAInstruction(TR::Instruction *instr);
void tracePostRAInstruction(TR::Instruction *instr);
void traceRegAssigned(TR::Register *virtReg, TR::Register *realReg);
void traceRegAssigned(TR::Register *virtReg, TR::Register *realReg, TR_RegisterAssignmentFlags flags);
void traceRegFreed(TR::Register *virtReg, TR::Register *realReg);
void traceRegInterference(TR::Register *virtReg, TR::Register *interferingVirtual, int32_t distance);
void traceRegWeight(TR::Register *realReg, uint32_t weight);
void traceRegisterAssignment(const char *format, ...);

void setRegisterAssignmentFlag(TR_RegisterAssignmentFlagBits b) {_regAssignFlags.set(b);}
void resetRegisterAssignmentFlag(TR_RegisterAssignmentFlagBits b) {_regAssignFlags.set(b);}
bool testAnyRegisterAssignmentFlags(TR_RegisterAssignmentFlagBits b) {return _regAssignFlags.testAny(b);}
void clearRegisterAssignmentFlags() {_regAssignFlags.clear();}

// --------------------------------------------------------------------------
// Spill
//
TR::list<TR_BackingStore*>& getSpill4FreeList() {return _spill4FreeList;}
TR::list<TR_BackingStore*>& getSpill8FreeList() {return _spill8FreeList;}
TR::list<TR_BackingStore*>& getSpill16FreeList() {return _spill16FreeList;}
TR::list<TR_BackingStore*>& getInternalPointerSpillFreeList() {return _internalPointerSpillFreeList;}
TR::list<TR_BackingStore*>& getCollectedSpillList() {return _collectedSpillList;}
TR::list<TR_BackingStore*>& getAllSpillList() {return _allSpillList;}

TR::list<TR::Register*> *getSpilledRegisterList() {return _spilledRegisterList;}
TR::list<TR::Register*> *setSpilledRegisterList(TR::list<TR::Register*> *r) {return _spilledRegisterList = r;}
TR::list<TR::Register*> *getFirstTimeLiveOOLRegisterList() {return _firstTimeLiveOOLRegisterList;}
TR::list<TR::Register*> *setFirstTimeLiveOOLRegisterList(TR::list<TR::Register*> *r) {return _firstTimeLiveOOLRegisterList = r;}

TR_BackingStore *allocateSpill(bool containsCollectedReference, int32_t *offset, bool reuse=true);
TR_BackingStore *allocateSpill(int32_t size, bool containsCollectedReference, int32_t *offset, bool reuse=true);
TR_BackingStore *allocateInternalPointerSpill(TR::AutomaticSymbol *pinningArrayPointer);
void freeSpill(TR_BackingStore *spill, int32_t size, int32_t offset);
void jettisonAllSpills(); // Call between register assigner passes

// --------------------------------------------------------------------------
// Out of line register allocation (x86)
//
TR::list<OMR::RegisterUsage*> *getReferencedRegisterList() {return _referencedRegistersList;}
TR::list<OMR::RegisterUsage*> *setReferencedRegisterList(TR::list<OMR::RegisterUsage*> *r) {return _referencedRegistersList = r;}

void recordSingleRegisterUse(TR::Register *reg);
void startRecordingRegisterUsage();
TR::list<OMR::RegisterUsage*> *stopRecordingRegisterUsage();

int32_t setCurrentPathDepth(int32_t d) { return _currentPathDepth = d; }
int32_t getCurrentPathDepth() { return _currentPathDepth; }

// --------------------------------------------------------------------------
// Virtual register allocation
//
TR::Register * allocateRegister(TR_RegisterKinds rk = TR_GPR);
TR::Register * allocateCollectedReferenceRegister();
TR::Register * allocateSinglePrecisionRegister(TR_RegisterKinds rk = TR_FPR);
TR::Register * allocate64bitRegister();

TR::RegisterPair * allocate64bitRegisterPair(TR::Register * lo = 0, TR::Register * ho = 0);
TR::RegisterPair * allocateRegisterPair(TR::Register * lo = 0, TR::Register * ho = 0);
TR::RegisterPair * allocateSinglePrecisionRegisterPair(TR::Register * lo = 0, TR::Register * ho = 0);
TR::RegisterPair * allocateFloatingPointRegisterPair(TR::Register * lo = 0, TR::Register * ho = 0);



TR::SymbolReference * allocateLocalTemp(TR::DataType dt = TR::Int32, bool isInternalPointer = false);

// --------------------------------------------------------------------------
// Relocations
//
TR::list<TR::Relocation*>& getRelocationList() {return _relocationList;}
TR::list<TR::Relocation*>& getAOTRelocationList() {return _aotRelocationList;}
TR::list<TR::StaticRelocation>& getStaticRelocations() { return _staticRelocationList; }

void addRelocation(TR::Relocation *r);
void addAOTRelocation(TR::Relocation *r, const char *generatingFileName, uintptr_t generatingLineNumber, TR::Node *node, TR::AOTRelocationPositionRequest where = TR::AOTRelocationAtBack);
void addAOTRelocation(TR::Relocation *r, TR::RelocationDebugInfo *info, TR::AOTRelocationPositionRequest where = TR::AOTRelocationAtBack);
void addStaticRelocation(const TR::StaticRelocation &relocation);

void addProjectSpecializedRelocation(uint8_t *location,
uint8_t *target,
uint8_t *target2,
TR_ExternalRelocationTargetKind kind,
char *generatingFileName,
uintptr_t generatingLineNumber,
TR::Node *node) {}
void addProjectSpecializedPairRelocation(uint8_t *location1,
uint8_t *location2,
uint8_t *target,
TR_ExternalRelocationTargetKind kind,
char *generatingFileName,
uintptr_t generatingLineNumber,
TR::Node *node) {}
void addProjectSpecializedRelocation(TR::Instruction *instr,
uint8_t *target,
uint8_t *target2,
TR_ExternalRelocationTargetKind kind,
char *generatingFileName,
uintptr_t generatingLineNumber,
TR::Node *node) {}

void apply8BitLabelRelativeRelocation(int32_t * cursor, TR::LabelSymbol * label); // no virt
void apply12BitLabelRelativeRelocation(int32_t * cursor, TR::LabelSymbol * label, bool isCheckDisp = true); // no virt
void apply16BitLabelRelativeRelocation(int32_t * cursor, TR::LabelSymbol * label); // no virt
void apply16BitLabelRelativeRelocation(int32_t * cursor, TR::LabelSymbol * label,int8_t d, bool isInstrOffset = false); // no virt
void apply24BitLabelRelativeRelocation(int32_t * cursor, TR::LabelSymbol *); // no virt
void apply16BitLoadLabelRelativeRelocation(TR::Instruction *, TR::LabelSymbol *, TR::LabelSymbol *, int32_t); // no virt
void apply32BitLoadLabelRelativeRelocation(TR::Instruction *, TR::LabelSymbol *, TR::LabelSymbol *, int32_t);  // no virt
void apply64BitLoadLabelRelativeRelocation(TR::Instruction *, TR::LabelSymbol *); // no virt
void apply32BitLabelRelativeRelocation(int32_t * cursor, TR::LabelSymbol *); // no virt
void apply32BitLabelTableRelocation(int32_t * cursor, TR::LabelSymbol *); // no virt

TR::list<TR_Pair<TR_ResolvedMethod,TR::Instruction> *> &getJNICallSites() { return _jniCallSites; }  // registerAssumptions()

bool needClassAndMethodPointerRelocations() { return false; }
bool needRelocationsForStatics() { return false; }

// --------------------------------------------------------------------------
// Snippets
//
int32_t setEstimatedLocationsForSnippetLabels(int32_t estimatedSnippetStart);
TR::list<TR::Snippet*>& getSnippetList() {return _snippetList;}
void addSnippet(TR::Snippet *s);

// Local snippet sharing facility: most RISC platforms can make use of it. The platform
// specific code generators should override isSnippetMatched if they choose to use it.
TR::LabelSymbol * lookUpSnippet(int32_t snippetKind, TR::SymbolReference *symRef);
bool isSnippetMatched(TR::Snippet *snippet, int32_t snippetKind, TR::SymbolReference *symRef) {return false;} // no virt, cast

// called to emit any constant data snippets.  The platform specific code generators
// should override these methods if they use constant data snippets.
//
void emitDataSnippets() {}
virtual bool hasDataSnippets() {return false;} // no virt, cast
int32_t setEstimatedLocationsForDataSnippetLabels(int32_t estimatedSnippetStart) {return 0;}

// called to emit any target address snippets.  The platform specific code generators
// should override these methods if they use target address snippets.
//
void emitTargetAddressSnippets() {}
bool hasTargetAddressSnippets() {return false;} // no virt, cast
int32_t setEstimatedLocationsForTargetAddressSnippetLabels(int32_t estimatedSnippetStart) {return 0;}

// --------------------------------------------------------------------------
// Register pressure
//
void estimateRegisterPressure(TR::Node *node, int32_t &registerPressure, int32_t &maxRegisterPressure, int32_t maxRegisters, TR_BitVector *valuesInGlobalRegs, bool isCold, vcount_t visitCount, TR::SymbolReference *symRef, bool &symRefIsLive, bool checkForIMuls, bool &vmThreadUsed);
int32_t estimateRegisterPressure(TR::Block *block, vcount_t visitCount, int32_t maxStaticFrequency, int32_t maxFrequency, bool &vmThreadUsed, int32_t numGlobalRegs = 0, TR_BitVector *valuesInGlobalRegs = NULL, TR::SymbolReference *symRef = NULL, bool checkForIMuls = false);

#include "codegen/RegisterPressureSimulatorInner.hpp"

// --------------------------------------------------------------------------
// Internal control flow
//
// Used to track the internal control flow depth level while compiling.
// Updated the CodeGenerator.hpp in x86, s390, and PPC so they reference this common code.
int32_t _internalControlFlowNestingDepth;
int32_t _internalControlFlowSafeNestingDepth;
TR::Instruction *_instructionAtEndInternalControlFlow;
int32_t internalControlFlowNestingDepth() {return _internalControlFlowNestingDepth;}
int32_t internalControlFlowSafeNestingDepth() { return _internalControlFlowSafeNestingDepth; }
void incInternalControlFlowNestingDepth() {_internalControlFlowNestingDepth++;}
void decInternalControlFlowNestingDepth() {_internalControlFlowNestingDepth--;}
bool insideInternalControlFlow() {return (_internalControlFlowNestingDepth > _internalControlFlowSafeNestingDepth);}
void setInternalControlFlowNestingDepth(int32_t depth) { _internalControlFlowNestingDepth = depth; }
void setInternalControlFlowSafeNestingDepth(int32_t safeDepth) { _internalControlFlowSafeNestingDepth = safeDepth; }
TR::Instruction* getInstructionAtEndInternalControlFlow() { return _instructionAtEndInternalControlFlow; }

// --------------------------------------------------------------------------
// Non-linear register assigner

bool getUseNonLinearRegisterAssigner() { return _enabledFlags.testAny(UseNonLinearRegisterAssigner); }
void setUseNonLinearRegisterAssigner() { _enabledFlags.set(UseNonLinearRegisterAssigner); }

bool isFreeSpillListLocked() { return _enabledFlags.testAny(LockFreeSpillList); }
void lockFreeSpillList() {_enabledFlags.set(LockFreeSpillList);}
void unlockFreeSpillList() {_enabledFlags.reset(LockFreeSpillList);}

bool getEnableRegisterUsageTracking() { return _enabledFlags.testAny(TrackRegisterUsage); }
void setEnableRegisterUsageTracking() { _enabledFlags.set(TrackRegisterUsage); }
void resetEnableRegisterUsageTracking() {_enabledFlags.reset(TrackRegisterUsage);}

// --------------------------------------------------------------------------
// Register assignment
//
TR_RegisterKinds prepareRegistersForAssignment(); // no virt
void addToUnlatchedRegisterList(TR::RealRegister *reg);
void freeUnlatchedRegisters();

// --------------------------------------------------------------------------
// Listing
//
uint32_t _indentation;


// --------------------------------------------------------------------------
// Code patching
//
// Used to find out whether there is an appropriate instruction space as vgdnop space
int32_t sizeOfInstructionToBePatched(TR::Instruction *vgdnop);
int32_t sizeOfInstructionToBePatchedHCRGuard(TR::Instruction *vgdnop);
// Used to find which instruction is an appropriate instruction space as vgdnop space
TR::Instruction* getInstructionToBePatched(TR::Instruction *vgdnop);
// Used to find the guard instruction where a given guard will actually patch
// currently can only return a value other than vgdnop for HCR guards
TR::Instruction* getVirtualGuardForPatching(TR::Instruction *vgdnop);

void jitAddPicToPatchOnClassUnload(void *classPointer, void *addressToBePatched) {}
void jitAdd32BitPicToPatchOnClassUnload(void *classPointer, void *addressToBePatched) {}
void jitAddPicToPatchOnClassRedefinition(void *classPointer, void *addressToBePatched, bool unresolved = false) {}
void jitAdd32BitPicToPatchOnClassRedefinition(void *classPointer, void *addressToBePatched, bool unresolved = false) {}
void jitAddUnresolvedAddressMaterializationToPatchOnClassRedefinition(void *firstInstruction) {} //J9
bool wantToPatchClassPointer(const TR_OpaqueClassBlock *allegedClassPointer, const TR::Node *forNode) { return false; } //J9
bool wantToPatchClassPointer(const TR_OpaqueClassBlock *allegedClassPointer, const uint8_t *inCodeAt) { return false; } //J9

// --------------------------------------------------------------------------
// Unclassified
//

// P now
bool isRotateAndMask(TR::Node *node) { return false; } // no virt

TR::Instruction *generateNop(TR::Node *node, TR::Instruction *instruction=0, TR_NOPKind nopKind=TR_NOPStandard); // no virt, cast
bool isOutOfLineHotPath() { TR_ASSERT(0, "isOutOfLineHotPath is only implemented for 390 and ppc"); return false;} // no virt

//Rather confusingly not used -only- in BCD related codegen.
//... has leaked into non-BCD code.
bool traceBCDCodeGen();
void traceBCDEntry(char *str, TR::Node *node);
void traceBCDExit(char *str, TR::Node *node);

TR_BitVector *getLiveButMaybeUnreferencedLocals() {return _liveButMaybeUnreferencedLocals;}
TR_BitVector *setLiveButMaybeUnreferencedLocals(TR_BitVector *v) {return (_liveButMaybeUnreferencedLocals = v);}

TR::AheadOfTimeCompile *getAheadOfTimeCompile() {return _aheadOfTimeCompile;}
TR::AheadOfTimeCompile *setAheadOfTimeCompile(TR::AheadOfTimeCompile *p) {return (_aheadOfTimeCompile = p);}

// J9, X86
bool canTransformUnsafeCopyToArrayCopy() { return false; } // no virt
bool canTransformUnsafeSetMemory() { return false; }

bool canNullChkBeImplicit(TR::Node *); // no virt, cast
bool canNullChkBeImplicit(TR::Node *, bool doChecks);

bool IsInMemoryType(TR::DataType type) { return false; }

bool nodeMayCauseException(TR::Node *node) { return false; } // no virt

// Should these be in codegen?
bool isSupportedAdd(TR::Node *addr);
bool nodeMatches(TR::Node *addr1, TR::Node *addr2, bool addressesUnderSameTreeTop=false);
bool additionsMatch(TR::Node *addr1, TR::Node *addr2, bool addressesUnderSameTreeTop=false);
bool addressesMatch(TR::Node *addr1, TR::Node *addr2, bool addressesUnderSameTreeTop=false);
// Z codegen
bool uniqueAddressOccurrence(TR::Node *addr1, TR::Node *addr2, bool addressesUnderSameTreeTop);

TR_StorageOverlapKind storageMayOverlap(TR::Node *node1, size_t length1, TR::Node *node2, size_t length2);

// arrayTranslateTableRequiresAlignment returns a mask if alignent required, otherwise 0
// For example, if a page is 4096 bytes and array translation required page alignment, 4095 would
// be returned.
// arrayTranslateMinimumNumberOfElements returns the minimum number of elements that need
// to be processed for it to be worth it to execute the 'translate' built-in function
// arrayTranslateAndTestMinimumNumberOfIterations returns the minimum number of iterations
// that the loop must run for the transformation to be worthwhile.
int32_t arrayTranslateTableRequiresAlignment(bool isByteSource, bool isByteTarget)  { return 0; } // no virt

// These methods used to return a default value of INT_MAX. However, in at least one place,
// and quite possibly elsewhere, the optimizer tests for
//
//    arrayTranslateAndTestMinimumNumberOfIterations >= freq1 / freq2
//
// by instead using integer arithmetic like this:
//
//    arrayTranslateAndTestMinimumNumberOfIterations * freq2 >= freq1.
//
// In such a test, the default INT_MAX causes a signed overflow, which is undefined behaviour.
// We appear to have gotten relatively benign wrapping behaviour, but the wrapped result is
// -freq2 when freq2 is even, and INT_MAX - freq2 + 1 when freq2 is odd, which means that we
// would make a decision based solely on the low bit of freq2.
//
// Rather than ensure that all callers use floating-point division for such tests, we can
// make sure here that the default may be safely multiplied by a block frequency. The new
// default of 10001 is still unmeasurably large as a frequency ratio.
//
// The proper thing should be for the optimizer to perform these sorts of transformations
// regardless of the expected number of iterations, because the transformed IL provides
// strictly more information to codegen, which can then generate the best sequence at its
// discretion, fabricating a loop if necessary. But at least in the case of idiom
// recognition's MemCpy pattern, we want Design 94472 for this.
int32_t arrayTranslateMinimumNumberOfElements(bool isByteSource, bool isByteTarget); // no virt

// TO TransformUtil.  Make platform specific
int32_t arrayTranslateAndTestMinimumNumberOfIterations(); // no virt
static int32_t defaultArrayTranslateMinimumNumberOfIterations(const char *methodName);
static bool useOldArrayTranslateMinimumNumberOfIterations()
{
static bool useOldValue = feGetEnv("TR_oldArrayTranslateMinimumNumberOfIterations") != NULL;
return useOldValue;
}

// the following functions evaluate whether a codegen for the node or for static
// symbol reference requires entry in the literal pool
bool arithmeticNeedsLiteralFromPool(TR::Node *node) { return false; } // no virt
bool bitwiseOpNeedsLiteralFromPool(TR::Node *parent, TR::Node *child) { return false; } // no virt
bool bndsChkNeedsLiteralFromPool(TR::Node *node) { return false; } // no virt
bool constLoadNeedsLiteralFromPool(TR::Node *node) { return false; } // no virt, cast
void setOnDemandLiteralPoolRun(bool answer) {} // no virt, cast
bool isLiteralPoolOnDemandOn () { return false; } // no virt, cast
bool supportsOnDemandLiteralPool() { return false; } // no virt, cast
bool supportsDirectIntegralLoadStoresFromLiteralPool() { return false; } // no virt
bool supportsHighWordFacility() { return false; } // no virt, default, cast

bool inlineDirectCall(TR::Node *node, TR::Register *&resultReg) { return false; }

// J9 only, move to trj9
TR_OpaqueClassBlock* getMonClass(TR::Node* monNode);

void setCurrentBlock(TR::Block *b);
TR::Block *getCurrentBlock() { return _currentBlock; }

bool hasCCInfo()                          { return (_flags2.testAny(HasCCInfo));}
void setHasCCInfo(bool v)                 { _flags2.set(HasCCInfo, v); }
bool hasCCOverflow()                      { return (_flags2.testAny(HasCCOverflow));}
void setHasCCOverflow(bool v)             { _flags2.set(HasCCOverflow, v); }
bool hasCCSigned()                        { return (_flags2.testAny(HasCCSigned));}
void setHasCCSigned(bool v)               { _flags2.set(HasCCSigned, v); }
bool hasCCZero()                          { return (_flags2.testAny(HasCCZero));}
void setHasCCZero(bool v)                 { _flags2.set(HasCCZero, v); }
bool hasCCCarry()                         { return (_flags2.testAny(HasCCCarry));}
void setHasCCCarry(bool v)                { _flags2.set(HasCCCarry, v); }

bool hasCCCompare()                       { return (_flags3.testAny(HasCCCompare));}
void setHasCCCompare(bool v)                { _flags3.set(HasCCCompare, v); }

bool requiresCarry()                      { return (_flags3.testAny(RequiresCarry));}
void setRequiresCarry(bool v)             { _flags3.set(RequiresCarry, v); }
bool computesCarry()                      { return (_flags3.testAny(ComputesCarry));}
void setComputesCarry(bool v)             { _flags3.set(ComputesCarry, v); }

TR::RealRegister **_unlatchedRegisterList; // dynamically allocated

bool alwaysUseTrampolines() { return _enabledFlags.testAny(AlwaysUseTrampolines); }
void setAlwaysUseTrampolines() {_enabledFlags.set(AlwaysUseTrampolines);}

bool shouldBuildStructure() { return _enabledFlags.testAny(ShouldBuildStructure); }
void setShouldBuildStructure() {_enabledFlags.set(ShouldBuildStructure);}


bool enableRefinedAliasSets();
void setEnableRefinedAliasSets() {_enabledFlags.set(EnableRefinedAliasSets);}

// --------------------------------------------------------------------------

TR::Node *createOrFindClonedNode(TR::Node *node, int32_t numChildren);

void zeroOutAutoOnEdge(TR::SymbolReference * liveAutoSym, TR::Block *block, TR::Block *succBlock, TR::list<TR::Block*> *newBlocks, TR_ScratchList<TR::Node> *fsdStores);

bool constantAddressesCanChangeSize(TR::Node *node);
bool profiledPointersRequireRelocation();
bool needGuardSitesEvenWhenGuardRemoved();
bool supportVMInternalNatives();
bool supportsNativeLongOperations();

TR::DataType IntJ() { return TR::Compiler->target.is64Bit() ? TR::Int64 : TR::Int32; }

// will a BCD left shift always leave the sign code unchanged and thus allow it to be propagated through and upwards
bool propagateSignThroughBCDLeftShift(TR::DataType type) { return false; } // no virt

bool supportsLengthMinusOneForMemoryOpts() {return false;} // no virt, cast

// Java, likely Z
bool supportsTrapsInTMRegion() { return true; } // no virt

// Allows a platform code generator to assert that a particular node operation will use 64 bit values
// that are not explicitly present in the node datatype.
bool usesImplicit64BitGPRs(TR::Node *node) { return false; } // no virt

// General utility?
static bool treeContainsCall(TR::TreeTop * treeTop);

// IA32 only?
int32_t arrayInitMinimumNumberOfBytes() {return 8;} // no virt

TR::Instruction *saveOrRestoreRegisters(TR_BitVector *regs, TR::Instruction *cursor, bool doSaves);

void addCountersToEdges(TR::Block *block);

bool getSupportsBitOpCodes() {return false;} // no virt, default

bool getMappingAutomatics() {return _flags1.testAny(MappingAutomatics);}
void setMappingAutomatics() {_flags1.set(MappingAutomatics);}

bool getSupportsDirectJNICalls() {return _flags1.testAny(SupportsDirectJNICalls);} // no virt
bool supportsDirectJNICallsForAOT() { return false;} // no virt, default

void setSupportsDirectJNICalls() {_flags1.set(SupportsDirectJNICalls);}

bool getSupportsGlRegDeps() {return _flags1.testAny(SupportsGlRegDeps);}
void setSupportsGlRegDeps() {_flags1.set(SupportsGlRegDeps);}

bool getSupportsVectorRegisters() {return _flags1.testAny(SupportsVectorRegisters);}
void setSupportsVectorRegisters() {_flags1.set(SupportsVectorRegisters);}

bool getSupportsGlRegDepOnFirstBlock() {return _flags1.testAny(SupportsGlRegDepOnFirstBlock);}
void setSupportsGlRegDepOnFirstBlock() {_flags1.set(SupportsGlRegDepOnFirstBlock);}

bool getDisableLongGRA() {return _flags1.testAny(DisableLongGRA);}
void setDisableLongGRA() {_flags1.set(DisableLongGRA);}

bool getDisableFpGRA() {return _flags2.testAny(DisableFpGRA);}
void setDisableFpGRA() {_flags2.set(DisableFpGRA);}

bool usesRegisterMaps() {return _flags1.testAny(UsesRegisterMaps);}
void setUsesRegisterMaps() {_flags1.set(UsesRegisterMaps);}

bool getSupportsDivCheck() {return _flags1.testAny(SupportsDivCheck);}
void setSupportsDivCheck() {_flags1.set(SupportsDivCheck);}

bool getSupportsPrimitiveArrayCopy() {return _flags2.testAny(SupportsPrimitiveArrayCopy);}
void setSupportsPrimitiveArrayCopy() {_flags2.set(SupportsPrimitiveArrayCopy);}

bool getSupportsReferenceArrayCopy() {return _flags1.testAny(SupportsReferenceArrayCopy);}
void setSupportsReferenceArrayCopy() {_flags1.set(SupportsReferenceArrayCopy);}

bool getSupportsEfficientNarrowIntComputation() {return _flags2.testAny(SupportsEfficientNarrowIntComputation);}
void setSupportsEfficientNarrowIntComputation() {_flags2.set(SupportsEfficientNarrowIntComputation);}

bool getSupportsEfficientNarrowUnsignedIntComputation() {return _flags4.testAny(SupportsEfficientNarrowUnsignedIntComputation);}
void setSupportsEfficientNarrowUnsignedIntComputation() {_flags4.set(SupportsEfficientNarrowUnsignedIntComputation);}

bool getSupportsArrayTranslateAndTest() {return _flags2.testAny(SupportsArrayTranslateAndTest);}
void setSupportsArrayTranslateAndTest() {_flags2.set(SupportsArrayTranslateAndTest);}

bool getSupportsReverseLoadAndStore() {return _flags2.testAny(SupportsReverseLoadAndStore);}
void setSupportsReverseLoadAndStore() {_flags2.set(SupportsReverseLoadAndStore);}

bool getSupportsArrayTranslateTRxx() {return _flags2.testAny(SupportsArrayTranslate);}
void setSupportsArrayTranslateTRxx() {_flags2.set(SupportsArrayTranslate);}

bool getSupportsArrayTranslateTRTO255() {return _flags4.testAny(SupportsArrayTranslateTRTO255);}
void setSupportsArrayTranslateTRTO255() {_flags4.set(SupportsArrayTranslateTRTO255);}

bool getSupportsArrayTranslateTRTO() {return _flags4.testAny(SupportsArrayTranslateTRTO);}
void setSupportsArrayTranslateTRTO() {_flags4.set(SupportsArrayTranslateTRTO);}

bool getSupportsArrayTranslateTROTNoBreak() {return _flags4.testAny(SupportsArrayTranslateTROTNoBreak);}
void setSupportsArrayTranslateTROTNoBreak() {_flags4.set(SupportsArrayTranslateTROTNoBreak);}

bool getSupportsArrayTranslateTROT() {return _flags4.testAny(SupportsArrayTranslateTROT);}
void setSupportsArrayTranslateTROT() {_flags4.set(SupportsArrayTranslateTROT);}

bool getSupportsEncodeUtf16LittleWithSurrogateTest() { return false; }
bool getSupportsEncodeUtf16BigWithSurrogateTest() { return false; }

bool supportsZonedDFPConversions() {return _enabledFlags.testAny(SupportsZonedDFPConversions);}
void setSupportsZonedDFPConversions() {_enabledFlags.set(SupportsZonedDFPConversions);}

bool supportsIntDFPConversions() {return _enabledFlags.testAny(SupportsIntDFPConversions);}
void setSupportsIntDFPConversions() {_enabledFlags.set(SupportsIntDFPConversions);}

bool supportsFastPackedDFPConversions() {return _enabledFlags.testAny(SupportsFastPackedDFPConversions);}
void setSupportsFastPackedDFPConversions() {_enabledFlags.set(SupportsFastPackedDFPConversions);}

bool getSupportsArraySet() {return _flags1.testAny(SupportsArraySet);}
void setSupportsArraySet() {_flags1.set(SupportsArraySet);}

bool getSupportsArrayCmp() {return _flags1.testAny(SupportsArrayCmp);}
void setSupportsArrayCmp() {_flags1.set(SupportsArrayCmp);}

bool getSupportsArrayCmpSign() {return _flags3.testAny(SupportsArrayCmpSign);}
void setSupportsArrayCmpSign() {_flags3.set(SupportsArrayCmpSign);}

bool getSupportsSearchCharString() {return _flags3.testAny(SupportsSearchCharString);}
void setSupportsSearchCharString() {_flags3.set(SupportsSearchCharString);}

bool getSupportsTranslateAndTestCharString() {return _flags3.testAny(SupportsTranslateAndTestCharString);}
void setSupportsTranslateAndTestCharString() {_flags3.set(SupportsTranslateAndTestCharString);}

bool getSupportsTestCharComparisonControl() {return _flags3.testAny(SupportsTestCharComparisonControl);}
void setSupportsTestCharComparisonControl() {_flags3.set(SupportsTestCharComparisonControl);}

bool getMethodContainsBinaryCodedDecimal() { return _flags3.testAny(MethodContainsBinaryCodedDecimal);}
void setMethodContainsBinaryCodedDecimal() { _flags3.set(MethodContainsBinaryCodedDecimal);}

bool getAccessStaticsIndirectly() {return _flags1.testAny(AccessStaticsIndirectly);}
void setAccessStaticsIndirectly(bool b) {_flags1.set(AccessStaticsIndirectly, b);}

bool getSupportsBigDecimalLongLookasideVersioning() { return _flags3.testAny(SupportsBigDecimalLongLookasideVersioning);}
void setSupportsBigDecimalLongLookasideVersioning() { _flags3.set(SupportsBigDecimalLongLookasideVersioning);}

bool getSupportsBDLLHardwareOverflowCheck() { return _flags3.testAny(SupportsBDLLHardwareOverflowCheck);}
void setSupportsBDLLHardwareOverflowCheck() { _flags3.set(SupportsBDLLHardwareOverflowCheck);}

bool getSupportsDoubleWordCAS() { return _flags3.testAny(SupportsDoubleWordCAS);}
void setSupportsDoubleWordCAS() { _flags3.set(SupportsDoubleWordCAS);}

bool getSupportsDoubleWordSet() { return _flags3.testAny(SupportsDoubleWordSet);}
void setSupportsDoubleWordSet() { _flags3.set(SupportsDoubleWordSet);}

bool getSupportsTMDoubleWordCASORSet() { return _flags3.testAny(SupportsTMDoubleWordCASORSet);}
void setSupportsTMDoubleWordCASORSet() { _flags3.set(SupportsTMDoubleWordCASORSet);}

bool getSupportsTMHashMapAndLinkedQueue() { return _flags4.testAny(SupportsTMHashMapAndLinkedQueue);}
void setSupportsTMHashMapAndLinkedQueue() { _flags4.set(SupportsTMHashMapAndLinkedQueue);}

bool getSupportsAtomicLoadAndAdd() { return _flags4.testAny(SupportsAtomicLoadAndAdd);}
void setSupportsAtomicLoadAndAdd() { _flags4.set(SupportsAtomicLoadAndAdd);}

bool getSupportsTM() { return _flags4.testAny(SupportsTM);}
void setSupportsTM() { _flags4.set(SupportsTM);}

bool getSupportsLM() { return _flags4.testAny(SupportsLM);}
void setSupportsLM() { _flags4.set(SupportsLM);}

virtual bool getSupportsTLE();

virtual bool getSupportsIbyteswap();

virtual bool getSupportsBitPermute();

bool getSupportsAutoSIMD() { return _flags4.testAny(SupportsAutoSIMD);}
void setSupportsAutoSIMD() { _flags4.set(SupportsAutoSIMD);}

bool getSupportsOpCodeForAutoSIMD(TR::ILOpCode, TR::DataType) { return false; }

bool removeRegisterHogsInLowerTreesWalk() { return _flags3.testAny(RemoveRegisterHogsInLowerTreesWalk);}
void setRemoveRegisterHogsInLowerTreesWalk() { _flags3.set(RemoveRegisterHogsInLowerTreesWalk);}
void resetRemoveRegisterHogsInLowerTreesWalk() {_flags3.reset(RemoveRegisterHogsInLowerTreesWalk);}

bool getJNILinkageCalleeCleanup() {return _flags1.testAny(JNILinkageCalleeCleanup);}
void setJNILinkageCalleeCleanup() {_flags1.set(JNILinkageCalleeCleanup);}

bool getHasResumableTrapHandler() {return _flags1.testAny(HasResumableTrapHandler);}
void setHasResumableTrapHandler() {_flags1.set(HasResumableTrapHandler);}

bool performsChecksExplicitly() {return _flags1.testAny(PerformsExplicitChecks);}
void setPerformsChecksExplicitly() {_flags1.set(PerformsExplicitChecks);}

bool spillsFPRegistersAcrossCalls() {return _flags1.testAny(SpillsFPRegistersAcrossCalls);}
void setSpillsFPRegistersAcrossCalls() {_flags1.set(SpillsFPRegistersAcrossCalls);}

bool getSupportsJavaFloatSemantics() {return _flags1.testAny(SupportsJavaFloatSemantics);}
void setSupportsJavaFloatSemantics() {_flags1.set(SupportsJavaFloatSemantics);}

bool getSupportsInliningOfTypeCoersionMethods() {return _flags1.testAny(SupportsInliningOfTypeCoersionMethods);}
void setSupportsInliningOfTypeCoersionMethods() {_flags1.set(SupportsInliningOfTypeCoersionMethods);}

bool getSupportsPartialInlineOfMethodHooks() {return _flags1.testAny(SupportsPartialInlineOfMethodHooks);}
void setSupportsPartialInlineOfMethodHooks() {_flags1.set(SupportsPartialInlineOfMethodHooks);}

bool getSupportsMergedAllocations() {return _flags1.testAny(SupportsMergedAllocations);}
void setSupportsMergedAllocations() {_flags1.set(SupportsMergedAllocations);}

bool getSupportsInlinedAtomicLongVolatiles() {return _flags1.testAny(SupportsInlinedAtomicLongVolatiles);}
void setSupportsInlinedAtomicLongVolatiles() {_flags1.set(SupportsInlinedAtomicLongVolatiles);}
bool getInlinedGetCurrentThreadMethod() {return _flags3.testAny(InlinedGetCurrentThreadMethod);}
void setInlinedGetCurrentThreadMethod() {_flags3.set(InlinedGetCurrentThreadMethod);}

bool considerAllAutosAsTacticalGlobalRegisterCandidates()    {return _flags1.testAny(ConsiderAllAutosAsTacticalGlobalRegisterCandidates);}
void setConsiderAllAutosAsTacticalGlobalRegisterCandidates() {_flags1.set(ConsiderAllAutosAsTacticalGlobalRegisterCandidates);}

bool getSupportsScaledIndexAddressing() { return _flags1.testAny(SupportsScaledIndexAddressing); }
void setSupportsScaledIndexAddressing() { _flags1.set(SupportsScaledIndexAddressing); }

bool isAddressScaleIndexSupported(int32_t scale) { return false; } // no virt

bool getSupportsConstantOffsetInAddressing(int64_t value);
bool getSupportsConstantOffsetInAddressing() { return _flags3.testAny(SupportsConstantOffsetInAddressing); }
void setSupportsConstantOffsetInAddressing() { _flags3.set(SupportsConstantOffsetInAddressing); }

bool getSupportsCompactedLocals() {return _flags1.testAny(SupportsCompactedLocals);}
void setSupportsCompactedLocals() {_flags1.set(SupportsCompactedLocals);}

bool getSupportsFastCTM() {return _flags1.testAny(SupportsFastCTM);}
void setSupportsFastCTM() {_flags1.set(SupportsFastCTM);}

bool getSupportsCurrentTimeMaxPrecision() {return _flags2.testAny(SupportsCurrentTimeMaxPrecision);}
void setSupportsCurrentTimeMaxPrecision() {_flags2.set(SupportsCurrentTimeMaxPrecision);}

bool getSupportsAlignedAccessOnly() { return _flags3.testAny(SupportsAlignedAccessOnly); }
void setSupportsAlignedAccessOnly() { _flags3.set(SupportsAlignedAccessOnly); }

bool usesRegisterPairsForLongs()    {return _flags1.testAny(UsesRegisterPairsForLongs);}
void setUsesRegisterPairsForLongs() {_flags1.set(UsesRegisterPairsForLongs);}

bool getSupportsIDivAndIRemWithThreeChildren() {return _flags2.testAny(SupportsIDivAndIRemWithThreeChildren);}
void setSupportsIDivAndIRemWithThreeChildren() {_flags2.set(SupportsIDivAndIRemWithThreeChildren);}

bool getSupportsLDivAndLRemWithThreeChildren() {return _flags2.testAny(SupportsLDivAndLRemWithThreeChildren);}
void setSupportsLDivAndLRemWithThreeChildren() {_flags2.set(SupportsLDivAndLRemWithThreeChildren);}

bool getSupportsLoweringConstIDiv() {return _flags2.testAny(SupportsLoweringConstIDiv);}
void setSupportsLoweringConstIDiv() {_flags2.set(SupportsLoweringConstIDiv);}

bool getSupportsLoweringConstLDiv() {return _flags2.testAny(SupportsLoweringConstLDiv);}
void setSupportsLoweringConstLDiv() {_flags2.set(SupportsLoweringConstLDiv);}

bool getSupportsLoweringConstLDivPower2() {return _flags2.testAny(SupportsLoweringConstLDivPower2);}
void setSupportsLoweringConstLDivPower2() {_flags2.set(SupportsLoweringConstLDivPower2);}

bool getSupportsVirtualGuardNOPing() { return _flags2.testAny(SupportsVirtualGuardNOPing); }
void setSupportsVirtualGuardNOPing() { _flags2.set(SupportsVirtualGuardNOPing); }

bool getSupportsNewInstanceImplOpt() { return _flags2.testAny(SupportsNewInstanceImplOpt); }
void setSupportsNewInstanceImplOpt() { _flags2.set(SupportsNewInstanceImplOpt); }

bool getHasDoubleWordAlignedStack() { return _flags2.testAny(HasDoubleWordAlignedStack); }
void setHasDoubleWordAlignedStack() { _flags2.set(HasDoubleWordAlignedStack); }

bool getSupportsReadOnlyLocks() {return _flags2.testAny(SupportsReadOnlyLocks);}
void setSupportsReadOnlyLocks() {_flags2.set(SupportsReadOnlyLocks);}

bool getSupportsPostProcessArrayCopy() {return _flags2.testAny(SupportsPostProcessArrayCopy);}
void setSupportsPostProcessArrayCopy() {_flags2.set(SupportsPostProcessArrayCopy);}

bool getOptimizationPhaseIsComplete() {return _flags4.testAny(OptimizationPhaseIsComplete);}
void setOptimizationPhaseIsComplete() {_flags4.set(OptimizationPhaseIsComplete);}

bool getSupportsBCDToDFPReduction() {return _flags4.testAny(SupportsBCDToDFPReduction);}
void setSupportsBCDToDFPReduction() {_flags4.set(SupportsBCDToDFPReduction);}

bool getSupportsTestUnderMask() {return _flags4.testAny(SupportsTestUnderMask);}
void setSupportsTestUnderMask() {_flags4.set(SupportsTestUnderMask);}

bool getSupportsRuntimeInstrumentation() {return _flags4.testAny(SupportsRuntimeInstrumentation);}
void setSupportsRuntimeInstrumentation() {_flags4.set(SupportsRuntimeInstrumentation);}

bool isOutOfLineColdPath() {return (_outOfLineColdPathNestedDepth > 0) ? true : false;}
void incOutOfLineColdPathNestedDepth(){_outOfLineColdPathNestedDepth++;}
void decOutOfLineColdPathNestedDepth(){_outOfLineColdPathNestedDepth--;}

bool getMethodModifiedByRA() {return _flags2.testAny(MethodModifiedByRA);}
void setMethodModifiedByRA() {_flags2.set(MethodModifiedByRA);}
void resetMethodModifiedByRA() {_flags2.reset(MethodModifiedByRA);}

bool getEnforceStoreOrder() {return _flags2.testAny(EnforceStoreOrder);}
void setEnforceStoreOrder() {_flags2.set(EnforceStoreOrder);}

bool getDisableNullCheckOfArrayLength() { return _flags3.testAny(CompactNullCheckOfArrayLengthDisabled); }
void setDisableNullCheckOfArrayLength() { _flags3.set(CompactNullCheckOfArrayLengthDisabled); }

bool getSupportsShrinkWrapping() { return _flags3.testAny(SupportsShrinkWrapping); }
void setSupportsShrinkWrapping() { _flags3.set(SupportsShrinkWrapping); }

bool getShrinkWrappingDone() { return _flags3.testAny(ShrinkWrappingDone); }
void setShrinkWrappingDone() { _flags3.set(ShrinkWrappingDone); }

bool getUsesLoadStoreMultiple() { return _flags3.testAny(UsesLoadStoreMultiple); }
void setUsesLoadStoreMultiple() { _flags3.set(UsesLoadStoreMultiple); }

bool getSupportsStackAllocationOfArraylets() {return _flags3.testAny(SupportsStackAllocationOfArraylets);}
void setSupportsStackAllocationOfArraylets() {_flags3.set(SupportsStackAllocationOfArraylets);}

bool expandExponentiation() { return _flags3.testAny(ExpandExponentiation); }
void setExpandExponentiation() { _flags3.set(ExpandExponentiation); }

bool multiplyIsDestructive() { return _flags3.testAny(MultiplyIsDestructive); }
void setMultiplyIsDestructive() { _flags3.set(MultiplyIsDestructive); }

bool getIsInOOLSection() { return _flags4.testAny(IsInOOLSection); }
void toggleIsInOOLSection();

bool isInMemoryInstructionCandidate(TR::Node * node);

bool trackingInMemoryKilledLoads() {return _flags4.testAny(TrackingInMemoryKilledLoads);}
void setTrackingInMemoryKilledLoads() {_flags4.set(TrackingInMemoryKilledLoads);}
void resetTrackingInMemoryKilledLoads() {_flags4.reset(TrackingInMemoryKilledLoads);}

void setLmmdFailed() { _lmmdFailed = true;}

protected:

CodeGenerator();

enum // _flags1
{
MappingAutomatics                                  = 0x00000001,
SupportsDirectJNICalls                             = 0x00000002,
SupportsGlRegDeps                                  = 0x00000004,
SupportsDivCheck                                   = 0x00000008,
UsesRegisterMaps                                   = 0x00000010,
JNILinkageCalleeCleanup                            = 0x00000020,
HasResumableTrapHandler                            = 0x00000040,
IsLeafMethod                                       = 0x00000080,
SupportsPartialInlineOfMethodHooks                 = 0x00000100,
SupportsReferenceArrayCopy                         = 0x00000200,
SupportsJavaFloatSemantics                         = 0x00000400,
SupportsInliningOfTypeCoersionMethods              = 0x00000800,
// AVAILABLE                                       = 0x00001000,
SupportsVectorRegisters                            = 0x00002000,
SupportsGlRegDepOnFirstBlock                       = 0x00004000,
SupportsRemAsThirdChildOfDiv                       = 0x00008000,
HasNodePointerInEachInstruction                    = 0x00010000,
SupportsMergedAllocations                          = 0x00020000,
SupportsInlinedAtomicLongVolatiles                 = 0x00040000,
PerformsExplicitChecks                             = 0x00080000,
SpillsFPRegistersAcrossCalls                       = 0x00100000,
ConsiderAllAutosAsTacticalGlobalRegisterCandidates = 0x00200000,
//                                                 = 0x00400000,   // Available
SupportsScaledIndexAddressing                      = 0x00800000,
SupportsCompactedLocals                            = 0x01000000,
SupportsFastCTM                                    = 0x02000000,
UsesRegisterPairsForLongs                          = 0x04000000,
SupportsArraySet                                   = 0x08000000,
AccessStaticsIndirectly                            = 0x10000000,
SupportsArrayCmp                                   = 0x20000000,
DisableLongGRA                                     = 0x40000000,
DummyLastEnum1
};

enum // _flags2
{
SupportsIDivAndIRemWithThreeChildren                = 0x00000001,
SupportsLDivAndLRemWithThreeChildren                = 0x00000002,
SupportsPrimitiveArrayCopy                          = 0x00000004,
SupportsVirtualGuardNOPing                          = 0x00000008,
SupportsEfficientNarrowIntComputation               = 0x00000010,
SupportsNewInstanceImplOpt                          = 0x00000020,
SupportsLoweringConstIDiv                           = 0x00000040,
SupportsLoweringConstLDiv                           = 0x00000080,
SupportsArrayTranslate                              = 0x00000100,
HasDoubleWordAlignedStack                           = 0x00000200,
SupportsReadOnlyLocks                               = 0x00000400,
SupportsArrayTranslateAndTest                       = 0x00000800,
// AVAILABLE                                        = 0x00001000,
// AVAILABLE                                        = 0x00002000,
// AVAILABLE                                        = 0x00004000,
SupportsPostProcessArrayCopy                        = 0x00008000,
//                                                  = 0x00010000,   AVAILABLE FOR USE!!!
SupportsCurrentTimeMaxPrecision                     = 0x00020000,
HasCCSigned                                         = 0x00040000,
HasCCZero                                           = 0x00080000,
HasCCOverflow                                       = 0x00100000,
HasCCInfo                                           = 0x00200000,
SupportsReverseLoadAndStore                         = 0x00400000,
SupportsLoweringConstLDivPower2                     = 0x00800000,
DisableFpGRA                                        = 0x01000000,
// Available                                        = 0x02000000,
MethodModifiedByRA                                  = 0x04000000,
SchedulingInstrCleanupNeeded                        = 0x08000000,
// Available                                        = 0x10000000,
EnforceStoreOrder                                   = 0x20000000,
SupportsNewReferenceArrayCopy                       = 0x80000000,   // AVAILABLE FOR USE!!!!!!
DummyLastEnum2
};

enum // _flags3
{
//                                                  = 0x00000001,   AVAILABLE FOR USE!!!!!!
SupportsConstantOffsetInAddressing                  = 0x00000002,
SupportsAlignedAccessOnly                           = 0x00000004,
//                                                  = 0x00000008,   AVAILABLE FOR USE!!!!!!
CompactNullCheckOfArrayLengthDisabled               = 0x00000010,
SupportsArrayCmpSign                                = 0x00000020,
SupportsSearchCharString                            = 0x00000040,
SupportsTranslateAndTestCharString                  = 0x00000080,
SupportsTestCharComparisonControl                   = 0x00000100,
//                                                  = 0x00000200,   AVAILABLE FOR USE!!!!!!
//                                                  = 0x00000400,   AVAILABLE FOR USE!!!!!!
HasCCCarry                                          = 0x00000800,
//                                                  = 0x00001000,  AVAILABLE FOR USE!!!!!!
SupportsBigDecimalLongLookasideVersioning           = 0x00002000,
RemoveRegisterHogsInLowerTreesWalk                  = 0x00004000,
SupportsBDLLHardwareOverflowCheck                   = 0x00008000,
InlinedGetCurrentThreadMethod                       = 0x00010000,
RequiresCarry                                       = 0x00020000,
MethodContainsBinaryCodedDecimal                    = 0x00040000,  // wcode
ComputesCarry                                       = 0x00080000,
SupportsShrinkWrapping                              = 0x00100000,
ShrinkWrappingDone                                  = 0x00200000,
SupportsStackAllocationOfArraylets                  = 0x00400000,
//                                                  = 0x00800000,  AVAILABLE FOR USE!
SupportsDoubleWordCAS                               = 0x01000000,
SupportsDoubleWordSet                               = 0x02000000,
UsesLoadStoreMultiple                               = 0x04000000,
ExpandExponentiation                                = 0x08000000,
MultiplyIsDestructive                               = 0x10000000,
//                                                  = 0x20000000,  AVAILABLE FOR USE!
HasCCCompare                                        = 0x40000000,
SupportsTMDoubleWordCASORSet                        = 0x80000000,
DummyLastEnum
};

enum // flags4
{
//                                                  = 0x00000001,  AVAILABLE FOR USE!
//                                                  = 0x00000002,  AVAILABLE FOR USE!
//                                                  = 0x00000004,  AVAILABLE FOR USE!
OptimizationPhaseIsComplete                         = 0x00000008,
RequireRAPassAR                                     = 0x00000010,
IsInOOLSection                                      = 0x00000020,
SupportsBCDToDFPReduction                           = 0x00000040,
GRACompleted                                        = 0x00000080,
SupportsTestUnderMask                               = 0x00000100,
SupportsRuntimeInstrumentation                      = 0x00000200,
SupportsEfficientNarrowUnsignedIntComputation       = 0x00000400,
SupportsAtomicLoadAndAdd                            = 0x00000800,
//                                                  = 0x00001000, AVAILABLE FOR USE!
// AVAILABLE                                        = 0x00002000,
HasSignCleans                                       = 0x00004000,
SupportsArrayTranslateTRTO255                       = 0x00008000, //if (ca[i] < 256) ba[i] = (byte) ca[i]
SupportsArrayTranslateTROTNoBreak                   = 0x00010000, //ca[i] = (char) ba[i]
SupportsArrayTranslateTRTO                          = 0x00020000, //if (ca[i] < x) ba[i] = (byte) ca[i]; x is either 256 or 128
SupportsArrayTranslateTROT                          = 0x00040000, //if (ba[i] >= 0) ca[i] = (char) ba[i];
SupportsTM                                          = 0x00080000,
SupportsProfiledInlining                            = 0x00100000,
SupportsAutoSIMD                                = 0x00200000,  //vector support for autoVectorizatoon
// AVAILABLE                                        = 0x00400000,
// AVAILABLE                                        = 0x00800000,
//                                                  = 0x01000000,  NOW AVAILABLE
//                                                  = 0x02000000,  NOW AVAILABLE
//                                                  = 0x04000000,  NOW AVAILABLE
TrackingInMemoryKilledLoads                         = 0x08000000,
SupportsTMHashMapAndLinkedQueue                     = 0x10000000,
SupportsLM                                          = 0x20000000,

DummyLastEnum4
};

enum // enabledFlags
{
SupportsZonedDFPConversions      = 0x0001,
// Available                             ,
// Available                             ,
EnableRefinedAliasSets           = 0x0008,
AlwaysUseTrampolines             = 0x0010,
ShouldBuildStructure             = 0x0020,
LockFreeSpillList                = 0x0040,  // TAROK only (until it matures)
UseNonLinearRegisterAssigner     = 0x0080,  // TAROK only (until it matures)
TrackRegisterUsage               = 0x0100,  // TAROK only (until it matures)
//                               = 0x0200,  // AVAILABLE FOR USE!
//                               = 0x0400,  // AVAILABLE FOR USE!
SupportsIntDFPConversions        = 0x0800,
// Available                             ,
SupportsFastPackedDFPConversions = 0x2000,
// Available
};

TR::SymbolReferenceTable *_symRefTab;
TR::Linkage *_linkages[TR_NumLinkages];
TR::Linkage *_bodyLinkage;
TR::Register *_vmThreadRegister;
TR::RealRegister *_realVMThreadRegister;
TR::GCStackAtlas *_stackAtlas;
TR_GCStackMap *_methodStackMap;
TR::list<TR::Block*> _counterBlocks;
uint8_t *_binaryBufferStart;
uint8_t *_binaryBufferCursor;
TR::SparseBitVector _extendedToInt64GlobalRegisters;

TR_BitVector *_liveButMaybeUnreferencedLocals;
bool _lmmdFailed;
TR_BitVector *_assignedGlobalRegisters;

TR_LiveRegisters *_liveRegisters[NumRegisterKinds];
TR::AheadOfTimeCompile *_aheadOfTimeCompile;
uint32_t *_globalRegisterTable;

TR_InterferenceGraph *_localsIG;
TR_BitVector *_currentGRABlockLiveOutSet;
TR::Block *_currentBlock;
TR_BitVector *_preservedRegsInPrologue;

TR::list<TR::SymbolReference*> _availableSpillTemps;
TR::list<TR_LiveReference*> _liveReferenceList;
TR::list<TR::Snippet*> _snippetList;
TR_Array<TR::Register *> _registerArray;
TR::list<TR_BackingStore*> _spill4FreeList;
TR::list<TR_BackingStore*> _spill8FreeList;
TR::list<TR_BackingStore*> _spill16FreeList;
TR::list<TR_BackingStore*> _internalPointerSpillFreeList;
TR::list<TR_BackingStore*> _collectedSpillList;
TR::list<TR_BackingStore*> _allSpillList;
TR::list<TR::Relocation *> _relocationList;
TR::list<TR::Relocation *> _aotRelocationList;
TR::list<TR::StaticRelocation> _staticRelocationList;
TR::list<uint8_t*> _breakPointList;

TR::list<TR::SymbolReference*> _variableSizeSymRefPendingFreeList;
TR::list<TR::SymbolReference*> _variableSizeSymRefFreeList;
TR::list<TR::SymbolReference*> _variableSizeSymRefAllocList;

int32_t _accumulatorNodeUsage;

TR::list<TR::Register*> *_spilledRegisterList;
TR::list<TR::Register*> *_firstTimeLiveOOLRegisterList;
TR::list<OMR::RegisterUsage*> *_referencedRegistersList;
int32_t _currentPathDepth;
TR::list<TR::Node*> _nodesUnderComputeCCList;
TR::list<TR::Node*> _nodesToUncommonList;
TR::list<TR::Node*> _nodesSpineCheckedList;

TR::list<TR_Pair<TR_ResolvedMethod, TR::Instruction> *> _jniCallSites; // list of instrutions representing direct jni call sites

TR_Array<void *> _monitorMapping;

TR::list<TR::Node*> _compressedRefs;

int32_t _lowestSavedReg;

uint32_t _largestOutgoingArgSize;

uint32_t _estimatedCodeLength;
int32_t _estimatedSnippetStart;
int32_t _accumulatedInstructionLengthError;
int32_t _frameSizeInBytes;
int32_t _registerSaveDescription;
flags32_t _flags1;
flags32_t _flags2;
flags32_t _flags3;
flags32_t _flags4;
uint32_t _numberBytesReadInaccessible;
uint32_t _numberBytesWriteInaccessible;
uint32_t _maxObjectSizeGuaranteedNotToOverflow;

int32_t _outOfLineColdPathNestedDepth;

TR::CodeGenPhase _codeGenPhase;

TR::Instruction *_firstInstruction;
TR::Instruction *_appendInstruction;

TR_RegisterMask _liveRealRegisters[NumRegisterKinds];
TR_GlobalRegisterNumber _lastGlobalGPR;
TR_GlobalRegisterNumber _firstGlobalHPR;
TR_GlobalRegisterNumber _lastGlobalHPR;
TR_GlobalRegisterNumber _firstGlobalFPR;
TR_GlobalRegisterNumber _lastGlobalFPR;
TR_GlobalRegisterNumber _firstOverlappedGlobalFPR;
TR_GlobalRegisterNumber _lastOverlappedGlobalFPR;
TR_GlobalRegisterNumber _firstGlobalAR;
TR_GlobalRegisterNumber _lastGlobalAR;
TR_GlobalRegisterNumber _last8BitGlobalGPR;
TR_GlobalRegisterNumber _firstGlobalVRF;
TR_GlobalRegisterNumber _lastGlobalVRF;
TR_GlobalRegisterNumber _firstOverlappedGlobalVRF;
TR_GlobalRegisterNumber _lastOverlappedGlobalVRF;
TR_GlobalRegisterNumber _overlapOffsetBetweenFPRandVRFgrns;
uint8_t _supportedLiveRegisterKinds;
uint8_t _globalGPRPartitionLimit;
uint8_t _globalFPRPartitionLimit;
flags16_t _enabledFlags;

bool _afterRA;

// MOVE TO J9 Z CodeGenerator
// isTemporaryBased storageReferences just have a symRef but some other routines expect a node so use the below to fill in this symRef on this node
TR::Node *_dummyTempStorageRefNode;

public:
static TR_TreeEvaluatorFunctionPointer _nodeToInstrEvaluators[];

protected:

#ifdef DEBUG
static int _totalNumSpilledRegisters;         // For collecting statistics on spilling
static int _totalNumRematerializedConstants;
static int _totalNumRematerializedLocals;
static int _totalNumRematerializedStatics;
static int _totalNumRematerializedIndirects;
static int _totalNumRematerializedAddresses;
static int _totalNumRematerializedXMMRs;
#endif

bool _disableInternalPointers;

void addMonClass(TR::Node* monNode, TR_OpaqueClassBlock* clazz);

TR::RegisterIterator *_gpRegisterIterator;
TR::RegisterIterator *_fpRegisterIterator;
TR_RegisterAssignmentFlags _regAssignFlags;

uint32_t _preJitMethodEntrySize;
uint32_t _jitMethodEntryPaddingSize;

TR::Instruction *_lastInstructionBeforeCurrentEvaluationTreeTop;

TR_Stack<TR::MemoryReference *> _stackOfMemoryReferencesCreatedDuringEvaluation;

uint8_t *emitSnippets();

void addAllocatedRegisterPair(TR::RegisterPair * temp);
void addAllocatedRegister(TR::Register * temp);

private:

TR_BitVector *_blocksWithCalls;
void computeBlocksWithCalls();

TR::CodeCache * _codeCache;
bool _committedToCodeCache;

TR_Stack<TR::Node *> _stackOfArtificiallyInflatedNodes;

CS2::HashTable<TR::Symbol*, TR::DataType, TR::Allocator> _symbolDataTypeMap;
};

}

#endif
/*******************************************************************************
* Copyright (c) 2000, 2018 IBM Corp. and others
*
* This program and the accompanying materials are made available under
* the terms of the Eclipse Public License 2.0 which accompanies this
* distribution and is available at http://eclipse.org/legal/epl-2.0
* or the Apache License, Version 2.0 which accompanies this distribution
* and is available at https://www.apache.org/licenses/LICENSE-2.0.
*
* This Source Code may also be made available under the following Secondary
* Licenses when the conditions for such availability set forth in the
* Eclipse Public License, v. 2.0 are satisfied: GNU General Public License,
* version 2 with the GNU Classpath Exception [1] and GNU General Public
* License, version 2 with the OpenJDK Assembly Exception [2].
*
* [1] https://www.gnu.org/software/classpath/license.html
* [2] http://openjdk.java.net/legal/assembly-exception.html
*
* SPDX-License-Identifier: EPL-2.0 OR Apache-2.0 OR GPL-2.0 WITH Classpath-exception-2.0 OR LicenseRef-GPL-2.0 WITH Assembly-exception
*******************************************************************************/

#ifndef OMR_Z_CODEGENERATOR_INCL
#define OMR_Z_CODEGENERATOR_INCL

/*
* The following #define and typedef must appear before any #includes in this file
*/
#ifndef OMR_CODEGENERATOR_CONNECTOR
#define OMR_CODEGENERATOR_CONNECTOR
namespace OMR { namespace Z { class CodeGenerator; } }
namespace OMR { typedef OMR::Z::CodeGenerator CodeGeneratorConnector; }
#else
#error OMR::Z::CodeGenerator expected to be a primary connector, but an OMR connector is already defined
#endif

#include "compiler/codegen/OMRCodeGenerator.hpp"

#include <stddef.h>                                 // for size_t, NULL
#include <stdint.h>                                 // for int32_t, etc
#include <string.h>                                 // for memcmp
#include "codegen/FrontEnd.hpp"
#include "codegen/InstOpCode.hpp"
#include "codegen/LinkageConventionsEnum.hpp"
#include "codegen/Machine.hpp"                      // for Machine, etc
#include "codegen/RealRegister.hpp"                 // for RealRegister, etc
#include "codegen/RecognizedMethods.hpp"            // for RecognizedMethod
#include "codegen/RegisterConstants.hpp"
#include "codegen/ScratchRegisterManager.hpp"
#include "codegen/Snippet.hpp"                      // for Snippet
#include "codegen/TreeEvaluator.hpp"                // for TreeEvaluator
#include "compile/Compilation.hpp"                  // for Compilation, etc
#include "compile/ResolvedMethod.hpp"
#include "control/Options.hpp"
#include "control/Options_inlines.hpp"              // for TR::Options, etc
#include "cs2/arrayof.h"                            // for ArrayOf, etc
#include "cs2/hashtab.h"                            // for HashTable, etc
#include "env/CompilerEnv.hpp"
#include "env/CPU.hpp"                              // for Cpu
#include "env/jittypes.h"                           // for uintptrj_t
#include "env/PersistentInfo.hpp"                   // for PersistentInfo
#include "env/Processors.hpp"                       // for TR_Processor
#include "env/TRMemory.hpp"                         // for Allocator, etc
#include "il/Block.hpp"                             // for Block
#include "il/DataTypes.hpp"                         // for DataTypes, etc
#include "il/ILOpCodes.hpp"                         // for ILOpCodes, etc
#include "il/ILOps.hpp"                             // for TR::ILOpCode, etc
#include "il/Node.hpp"                              // for Node, etc
#include "il/Node_inlines.hpp"
#include "il/TreeTop.hpp"                           // for TreeTop
#include "il/TreeTop_inlines.hpp"                   // for TreeTop::getNode
#include "il/symbol/ResolvedMethodSymbol.hpp"
#include "infra/Array.hpp"                          // for TR_Array
#include "infra/Assert.hpp"                         // for TR_ASSERT
#include "infra/BitVector.hpp"                      // for TR_BitVector
#include "infra/Flags.hpp"                          // for flags32_t
#include "infra/List.hpp"                           // for List
#include "optimizer/DataFlowAnalysis.hpp"
#include "ras/Debug.hpp"                            // for TR_DebugBase
#include "runtime/Runtime.hpp"
#include "env/IO.hpp"


#include "il/symbol/LabelSymbol.hpp"
#include "z/codegen/S390OutOfLineCodeSection.hpp"
#include "codegen/BackingStore.hpp"
#include "codegen/Relocation.hpp"

#include "infra/TRlist.hpp"
class TR_BackingStore;
class TR_GCStackMap;
class TR_OpaquePseudoRegister;
class TR_PseudoRegister;
class TR_RegisterCandidate;
namespace TR { class S390ConstantDataSnippet; }
namespace TR { class S390ConstantInstructionSnippet; }
namespace TR { class S390EyeCatcherDataSnippet; }
namespace TR { class S390ImmInstruction; }
namespace TR { class S390LabelTableSnippet; }
namespace TR { class S390LookupSwitchSnippet; }
class TR_S390OutOfLineCodeSection;
namespace TR { class S390PrivateLinkage; }
class TR_S390ScratchRegisterManager;
namespace TR { class S390TargetAddressSnippet; }
namespace TR { class S390WritableDataSnippet; }
class TR_StorageReference;
namespace OMR { class Linkage; }
namespace TR { class CodeGenerator; }
namespace TR { class Instruction; }
namespace TR { class LabelSymbol; }
namespace TR { class MemoryReference; }
namespace TR { class Optimizer; }
namespace TR { class RegStarRef; }
namespace TR { class Register; }
namespace TR { class RegisterDependencyConditions; }
namespace TR { class RegisterIterator; }
namespace TR { class RegisterPair; }
namespace TR { class Symbol; }
namespace TR { class SymbolReference; }
namespace TR { class SystemLinkage; }

extern int64_t getIntegralValue(TR::Node* node);

#if defined(TR_TARGET_64BIT)
#define UPPER_4_BYTES(x) ((x) >> 32)
#define LOWER_4_BYTES(x) ((x) & 0xffffffff)
#else
// fake it to allow warning-free compilation on 32 bit systems
#define UPPER_4_BYTES(x) (x)
#define LOWER_4_BYTES(x) (x)
#endif

// Multi Code Cache Routines for checking whether an entry point is within reach of a BASRL.
#define NEEDS_TRAMPOLINE(target, rip, cg) (cg->alwaysUseTrampolines() || !CHECK_32BIT_TRAMPOLINE_RANGE(target,rip))

#define TR_MAX_MVO_PRECISION 31
#define TR_MAX_MVO_SIZE 16
#define TR_MAX_SRP_SIZE 16
#define TR_MAX_SRP_PRECISION 31
#define TR_MAX_SRP_SHIFT 31
#define TR_MAX_MVC_SIZE 256
#define TR_MAX_OC_NC_XC_SIZE 256
#define TR_MAX_CLC_SIZE 256
#define TR_MAX_SS1_SIZE 256
#define TR_MIN_SS_DISP 0
#define TR_MAX_SS_DISP 4095
#define TR_MIN_RX_DISP 0
#define TR_MAX_RX_DISP 4095

#define TR_MEMCPY_PAD_DST_LEN_INDEX 1
#define TR_MEMCPY_PAD_SRC_LEN_INDEX 3
#define TR_MEMCPY_PAD_MAX_LEN_INDEX 5
#define TR_MEMCPY_PAD_EQU_LEN_INDEX 6
#define MVCL_THRESHOLD 16777216

// functional thresholds
#define TR_MAX_FROM_TO_LOOP_STRING_SIZE (TR_MAX_SS1_SIZE)
#define TR_MAX_FROM_TO_TABLE_STRING_SIZE 1      // i.e. only TR one byte to one byte translations supported
#define TR_MAX_SIMD_LOOP_OPERAND_STRING_SIZE 8
#define TR_MAX_NUM_TALLY_TRIPLES_BY_LOOP 2
#define TR_MAX_NUM_TALLY_TRIPLES_BY_SIMD_LOOP 1
#define TR_MAX_NUM_TRANSLATE_QUADS_BY_LOOP 1    // note: for isInspectConvertingOp operations the length of the from/to strings is the # of 'all' replaces
#define TR_MAX_NUM_TRANSLATE_QUADS_BY_SIMD_LOOP 1
#define TR_MAX_SEARCH_STRING_SIZE (TR_MAX_SS1_SIZE)
#define TR_MAX_REPLACE_CHAR_STRING_SIZE 1
#define TR_MAX_BEFORE_AFTER_SIZE 1

// performance thresholds -- until SRST and similar instructions are used inline
#define TR_MAX_REPLACE_ALL_LOOP_PERF 150              // search > 1 byte (otherwise it is table lookup)
#define TR_MAX_TALLY_ALL_WIDE_LOOP_PERF 150           // search > 1 byte
#define TR_MIN_BM_SEARCH_PATTERN_SIZE 4
#define TR_MIN_BM_SEARCH_TEXT_SIZE 16

#define USE_CURRENT_DFP_ROUNDING_MODE (uint8_t)0x0


enum TR_MemCpyPadTypes
{
OneByte,
TwoByte,
ND_OneByte,
ND_TwoByte,
InvalidType
};

#define TR_INVALID_REGISTER -1


struct TR_S390BinaryEncodingData : public TR_BinaryEncodingData
{
int32_t estimate;
TR::Instruction *cursorInstruction;
TR::Instruction *jitTojitStart;
TR::Instruction *preProcInstruction;
int32_t loadArgSize;
};


namespace OMR
{
namespace Z
{
class OMR_EXTENSIBLE CodeGenerator : public OMR::CodeGenerator
{

public:

void preLowerTrees();

struct TR_BranchPreloadCallData
{
TR_ALLOC(TR_Memory::BranchPreloadCallData)
TR::Instruction *_callInstr;
TR::LabelSymbol *_callLabel;
TR::Block       *_callBlock;
TR::SymbolReference * _callSymRef;
int32_t        _frequency;

TR_BranchPreloadCallData() { _frequency = -1;}
TR_BranchPreloadCallData(TR::Instruction *i,TR::LabelSymbol *l,TR::Block *b,TR::SymbolReference *s, int32_t f)
:_callInstr(i),_callLabel(l),_callBlock(b),_callSymRef(s),_frequency(f){}
};

struct TR_BranchPreloadData
{
TR::Instruction *_returnInstr;
TR::LabelSymbol *_returnLabel;
TR::Block       *_returnBlock;
int32_t        _frequency;
bool           _insertBPPInEpilogue;

TR_BranchPreloadData() { _frequency = -1; _insertBPPInEpilogue = false; }
};

TR_BranchPreloadData _hottestReturn;
TR_BranchPreloadCallData _outlineCall;
TR_BranchPreloadCallData _outlineArrayCall;

TR::list<TR_BranchPreloadCallData*> *_callsForPreloadList;

TR::list<TR_BranchPreloadCallData*> * getCallsForPreloadList() { return _callsForPreloadList; }
void createBranchPreloadCallData(TR::LabelSymbol *callLabel, TR::SymbolReference * callSymRef, TR::Instruction * instr);

void insertInstructionPrefetches();
void insertInstructionPrefetchesForCalls(TR_BranchPreloadCallData * data);
bool hasWarmCallsBeforeReturn();

/**
* \brief Tells the optimzers and codegen whether a load constant node should be rematerialized.
*
* \details Large constants should be materialized (constant node should be commoned up)
* because loading them as immediate values is expensive.
*
* A constant is large if it's outside of the range specified by the largest negative
* const and smallest positive const. And the ranges are hardware platform dependent.
*/
bool materializesLargeConstants() { return true; }
bool shouldValueBeInACommonedNode(int64_t value);
int64_t getLargestNegConstThatMustBeMaterialized() {return ((-1ll) << 31) - 1;}   // min 32bit signed integer minus 1
int64_t getSmallestPosConstThatMustBeMaterialized() {return ((int64_t)0x000000007FFFFFFF) + 1;}   // max 32bit signed integer plus 1

void setNodeAddressOfCachedStaticTree(TR::Node *n) { _nodeAddressOfCachedStatic=n; }
TR::Node *getNodeAddressOfCachedStatic() { return _nodeAddressOfCachedStatic; }

TR::SparseBitVector & getBucketPlusIndexRegisters()  { return _bucketPlusIndexRegisters; }

// For hanging multiple loads from register symbols onto one common DEPEND
TR::Instruction *getCurrentDEPEND() {return _currentDEPEND; }
void setCurrentDEPEND(TR::Instruction *instr) { _currentDEPEND=instr; }

void changeRegisterKind(TR::Register * temp, TR_RegisterKinds rk);


TR::SymbolReference * _ARSaveAreaForTM;
void setARSaveAreaForTM(TR::SymbolReference * symRef) {_ARSaveAreaForTM = symRef;}
TR::SymbolReference * getARSaveAreaForTM() {return _ARSaveAreaForTM;}

void beginInstructionSelection();
void endInstructionSelection();

void checkIsUnneededIALoad(TR::Node *parent, TR::Node* node, TR::TreeTop *tt);
void lowerTreesWalk(TR::Node * parent, TR::TreeTop * treeTop, vcount_t visitCount);


void lowerTreeIfNeeded(TR::Node *node, int32_t childNumber, TR::Node *parent, TR::TreeTop *tt);

void lowerTreesPropagateBlockToNode(TR::Node *node);

CodeGenerator();
TR::Linkage *createLinkage(TR_LinkageConventions lc);

bool anyNonConstantSnippets();
bool anyLitPoolSnippets();

bool getSupportsEncodeUtf16BigWithSurrogateTest();

TR_S390ScratchRegisterManager* generateScratchRegisterManager(int32_t capacity = 8);

bool canTransformUnsafeCopyToArrayCopy();
bool supportsInliningOfIsInstance();
bool supports32bitAiadd() {return TR::Compiler->target.is64Bit();}

void setLabelHashTable(TR_HashTab *notPrintLabelHashTab) {_notPrintLabelHashTab = notPrintLabelHashTab;}
TR_HashTab * getLabelHashTable() {return _notPrintLabelHashTab;}

void addPICsListForInterfaceSnippet(TR::S390ConstantDataSnippet * ifcSnippet, TR::list<TR_OpaqueClassBlock*> * PICSlist);
TR::list<TR_OpaqueClassBlock*> * getPICsListForInterfaceSnippet(TR::S390ConstantDataSnippet * ifcSnippet);

void doInstructionSelection();
void doRegisterAssignment(TR_RegisterKinds kindsToAssign);

bool loadOrStoreAddressesMatch(TR::Node *node1, TR::Node *node2);

bool reliesOnAParticularSignEncoding(TR::Node *node);

void recordRegisterAssignment(TR::Register *assignedReg, TR::Register *virtualReg);

void doBinaryEncoding();
void doPeephole();
void setUseDefRegisters(bool resetRegs);

void AddFoldedMemRefToStack(TR::MemoryReference * mr);
void RemoveMemRefFromStack(TR::MemoryReference * mr);
void StopUsingEscapedMemRefsRegisters(int32_t topOfMemRefStackBeforeEvaluation);

bool supportsMergingGuards();

bool supportsDirectJNICallsForAOT() { return true;}

bool shouldYankCompressedRefs() { return true; }

TR::RegisterIterator *getARegisterIterator()                             {return  _aRegisterIterator;          }
TR::RegisterIterator *setARegisterIterator(TR::RegisterIterator *iter)    {return _aRegisterIterator = iter;  }

TR::RegisterIterator *getHPRegisterIterator()                            {return  _hpRegisterIterator;         }
TR::RegisterIterator *setHPRegisterIterator(TR::RegisterIterator *iter)   {return _hpRegisterIterator = iter; }

bool supportsJITFreeSystemStackPointer() { return false; }
TR::RegisterIterator *getVRFRegisterIterator()                           {return  _vrfRegisterIterator;        }
TR::RegisterIterator *setVRFRegisterIterator(TR::RegisterIterator *iter)  {return _vrfRegisterIterator = iter;}

bool supportsLengthMinusOneForMemoryOpts() {return true;}

bool supportsTrapsInTMRegion()
{
if(!TR::Compiler->target.isZOS())
return false;
return true;
}

bool inlineNDmemcpyWithPad(TR::Node * node, int64_t * maxLengthPtr = NULL);
bool codegenSupportsLoadlessBNDCheck() {return _processorInfo.supportsArch(TR_S390ProcessorInfo::TR_zEC12);}
TR::Register *evaluateLengthMinusOneForMemoryOps(TR::Node *,  bool , bool &lenMinusOne);

virtual TR_GlobalRegisterNumber getGlobalRegisterNumber(uint32_t realRegNum);

TR::RegisterPair* allocateArGprPair(TR::Register* lowRegister, TR::Register* highRegister);
void splitBaseRegisterPairsForRRMemoryInstructions(TR::Node *node, TR::RegisterPair * sourceReg, TR::RegisterPair * targetReg);
TR::RegisterDependencyConditions* createDepsForRRMemoryInstructions(TR::Node *node, TR::RegisterPair * sourceReg, TR::RegisterPair * targetReg, uint8_t extraDeps=0);

// Allocate a pair with inherent consecutive association
virtual TR::RegisterPair* allocateConsecutiveRegisterPair();
virtual TR::RegisterPair* allocateConsecutiveRegisterPair(TR::Register* lowRegister, TR::Register* highRegister);
virtual TR::RegisterPair* allocateFPRegisterPair();
virtual TR::RegisterPair* allocateFPRegisterPair(TR::Register* lowRegister, TR::Register* highRegister);

void processUnusedNodeDuringEvaluation(TR::Node *node);

void apply12BitLabelRelativeRelocation(int32_t * cursor, TR::LabelSymbol *, bool isCheckDisp = true);
void apply16BitLabelRelativeRelocation(int32_t * cursor, TR::LabelSymbol *, int8_t addressDifferenceDivisor, bool isInstrOffset = false);
void apply16BitLabelRelativeRelocation(int32_t * cursor, TR::LabelSymbol *);
void apply32BitLabelRelativeRelocation(int32_t * cursor, TR::LabelSymbol *);
void apply32BitLabelTableRelocation(int32_t * cursor, TR::LabelSymbol *);

void setUnavailableRegisters(TR::Block *b, TR_BitVector &unavailableRegisters);

void resetGlobalRegister(TR::RealRegister::RegNum reg, TR_BitVector &globalRegisters);
void setGlobalRegister(TR::RealRegister::RegNum reg, TR_BitVector &globalRegisters);
void setRegister(TR::RealRegister::RegNum reg, TR_BitVector &registers);
void removeUnavailableRegisters(TR_RegisterCandidate * rc, TR::Block * * blocks, TR_BitVector & availableRegisters);
void setUnavailableRegistersUsage(TR_Array<TR_BitVector>  &_liveOnEntryUsage, TR_Array<TR_BitVector>   &_liveOnExitUsage);

void genMemCpy(TR::MemoryReference *targetMR, TR::Node *targetNode, TR::MemoryReference *sourceMR, TR::Node *sourceNode, int64_t copySize);
void genMemClear(TR::MemoryReference *targetMR, TR::Node *targetNode, int64_t clearSize);

void genCopyFromLiteralPool(TR::Node *node, int32_t bytesToCopy, TR::MemoryReference *targetMR, size_t litPoolOffset, TR::InstOpCode::Mnemonic op = TR::InstOpCode::MVC);
int32_t biasDecimalFloatFrac(TR::DataType dt, int32_t frac);


bool isMemcpyWithPadIfFoldable(TR::Node *node, TR_MemCpyPadTypes type);
bool useMVCLForMemcpyWithPad(TR::Node *node, TR_MemCpyPadTypes type);
bool isValidCompareConst(int64_t compareConst);
bool isIfFoldable(TR::Node *node, int64_t compareConst);

bool mayImmedInstructionCauseOverFlow(TR::Node * node);

TR::Instruction *genLoadAddressToRegister(TR::Register *reg, TR::MemoryReference *origMR, TR::Node *node, TR::Instruction *preced=NULL);


bool useRippleCopyOrXC(size_t size, char *lit);

bool isCompressedClassPointerOfObjectHeader(TR::Node * node);



bool usesImplicit64BitGPRs(TR::Node *node);
bool nodeMayCauseException(TR::Node *node);

bool nodeRequiresATemporary(TR::Node *node);
bool endHintOnOperation(TR::Node *node);
bool endAccumulatorSearchOnOperation(TR::Node *node);
bool isStorageReferenceType(TR::Node *node);

TR::Register *allocateClobberableRegister(TR::Register *srcRegister);
/** Different from evaluateNode in that it returns a clobberable register */
TR::Register *gprClobberEvaluate(TR::Node *node, bool force_copy=false, bool ignoreRefCount = false);
TR::Register *fprClobberEvaluate(TR::Node *node);
TR_OpaquePseudoRegister *ssrClobberEvaluate(TR::Node *node, TR::MemoryReference *sourceMR);

//Convenience accessor methods
TR::Linkage *getS390Linkage();
TR::S390PrivateLinkage *getS390PrivateLinkage();
TR::SystemLinkage * getS390SystemLinkage();

TR::RealRegister *getStackPointerRealRegister(TR::Symbol *symbol = NULL);
TR::RealRegister *getEntryPointRealRegister();
TR::RealRegister *getReturnAddressRealRegister();

TR::RealRegister::RegNum getEntryPointRegister();
TR::RealRegister::RegNum getReturnAddressRegister();

TR::RealRegister *getExtCodeBaseRealRegister();
TR::RealRegister *getMethodMetaDataRealRegister();
TR::RealRegister *getLitPoolRealRegister();


void buildRegisterMapForInstruction(TR_GCStackMap *map);

// BCDCHK node
TR::Node * _currentCheckNode;
void setCurrentCheckNodeBeingEvaluated(TR::Node * n) { _currentCheckNode = n; }
TR::Node * getCurrentCheckNodeBeingEvaluated(){ return _currentCheckNode; }

// BCDCHK node exception handler label
TR::LabelSymbol* _currentBCDCHKHandlerLabel;
void setCurrentBCDCHKHandlerLabel(TR::LabelSymbol * l) { _currentBCDCHKHandlerLabel = l; }
TR::LabelSymbol* getCurrentBCDCHKHandlerLabel(){ return _currentBCDCHKHandlerLabel; }

TR::RegisterDependencyConditions * _currentBCDRegDeps;
void setCurrentCheckNodeRegDeps(TR::RegisterDependencyConditions * daaDeps) {_currentBCDRegDeps = daaDeps;}
TR::RegisterDependencyConditions * getCurrentCheckNodeRegDeps() {return _currentBCDRegDeps;}

// TODO : Do we need these? And if so there has to be a better way of tracking this this than what we're doing here.
/** Active counter used to track control flow basic blocks generated at instruction selection. */
int32_t _nextAvailableBlockIndex;
/** The index of the current basic block (used and updated during instruction selection). */
int32_t _currentBlockIndex;

void setNextAvailableBlockIndex(int32_t blockIndex) { _nextAvailableBlockIndex = blockIndex; }
int32_t getNextAvailableBlockIndex(){ return _nextAvailableBlockIndex; }
void incNextAvailableBlockIndex() { _nextAvailableBlockIndex++; }

void setCurrentBlockIndex(int32_t blockIndex) { _currentBlockIndex = blockIndex; }
int32_t getCurrentBlockIndex() { return _currentBlockIndex; }
int32_t arrayInitMinimumNumberOfBytes() {return 16;}

bool isStackBased(TR::MemoryReference *mr);

bool directLoadAddressMatch(TR::Node *load1, TR::Node *load2, bool trace);
bool isOutOf32BitPositiveRange(int64_t value, bool trace);
int32_t getMaskSize(int32_t leftMostNibble, int32_t nibbleCount);

void setAccumulatorNodeUsage(int32_t n) { _accumulatorNodeUsage = n; }
int32_t getAccumulatorNodeUsage()       { return _accumulatorNodeUsage; }
void incAccumulatorNodeUsage()          { _accumulatorNodeUsage++; }

bool possiblyConflictingNode(TR::Node *node);
bool foundConflictingNode(TR::Node *node, TR::list<TR::Node*> *conflictingAddressNodes);
void collectConflictingAddressNodes(TR::Node *parent, TR::Node *node, TR::list<TR::Node*> *conflictingAddressNodes);


template <class TR_AliasSetInterface> bool loadAndStoreMayOverlap(TR::Node *store, size_t storeSize, TR::Node *load, size_t loadSize, TR_AliasSetInterface &storeAliases);
bool loadAndStoreMayOverlap(TR::Node *store, size_t storeSize, TR::Node *load, size_t loadSize);

bool checkIfcmpxx(TR::Node *node);
bool checkSimpleLoadStore(TR::Node *loadNode, TR::Node *storeNode, TR::Block *block);
bool checkBitWiseChild(TR::Node *node);

TR_StorageDestructiveOverlapInfo getStorageDestructiveOverlapInfo(TR::Node *src, size_t srcLength, TR::Node *dst, size_t dstLength);

void setBranchOnCountFlag(TR::Node *node, vcount_t visitCount);

bool needs64bitPrecision(TR::Node *node);

virtual bool isUsing32BitEvaluator(TR::Node *node);
virtual bool getSupportsBitPermute();
int32_t getEstimatedExtentOfLitLoop()  {return _extentOfLitPool;}

int64_t setAvailableHPRSpillMask(int64_t i)  {return _availableHPRSpillMask = i;}
int64_t maskAvailableHPRSpillMask(int64_t i) {return _availableHPRSpillMask &= i;}
int64_t getAvailableHPRSpillMask()           {return _availableHPRSpillMask;}
int32_t getPreprologueOffset()               { return _preprologueOffset; }
int32_t setPreprologueOffset(int32_t offset) { return _preprologueOffset = offset; }

bool supportsBranchPreload()          {return _cgFlags.testAny(S390CG_enableBranchPreload);}
void setEnableBranchPreload()          {_cgFlags.set(S390CG_enableBranchPreload);}
void setDisableBranchPreload()          {_cgFlags.reset(S390CG_enableBranchPreload);}

bool supportsBranchPreloadForCalls()          {return _cgFlags.testAny(S390CG_enableBranchPreloadForCalls);}
void setEnableBranchPreloadForCalls()          {_cgFlags.set(S390CG_enableBranchPreloadForCalls);}
void setDisableBranchPreloadForCalls()          {_cgFlags.reset(S390CG_enableBranchPreloadForCalls);}

bool getExtCodeBaseRegisterIsFree()         {return _cgFlags.testAny(S390CG_extCodeBaseRegisterIsFree);}
void setExtCodeBaseRegisterIsFree(bool val) {return _cgFlags.set(S390CG_extCodeBaseRegisterIsFree, val);}

bool isOutOfLineHotPath() {return _cgFlags.testAny(S390CG_isOutOfLineHotPath);}
void setIsOutOfLineHotPath(bool val) {_cgFlags.set(S390CG_isOutOfLineHotPath, val);}

bool getExitPointsInMethod() {return _cgFlags.testAny(S390CG_doesExit);}
void setExitPointsInMethod(bool val) {return _cgFlags.set(S390CG_doesExit, val);}

bool isPrefetchNextStackCacheLine() {return _cgFlags.testAny(S390CG_prefetchNextStackCacheLine);}
void setPrefetchNextStackCacheLine(bool val) {return _cgFlags.set(S390CG_prefetchNextStackCacheLine, val);}

// Heap object prefetching
bool enableTLHPrefetching()    {return _cgFlags.testAny(S390CG_enableTLHPrefetching);}
void setEnableTLHPrefetching() { _cgFlags.set(S390CG_enableTLHPrefetching);}

// Query to codegen to know if regs are available or not
//
bool isExtCodeBaseFreeForAssignment();
bool isLitPoolFreeForAssignment();

// zGryphon HPR
TR::Instruction * upgradeToHPRInstruction(TR::Instruction * inst);
// REG ASSOCIATION
//
bool enableRegisterAssociations();
bool enableRegisterPairAssociation();

bool canBeAffectedByStoreTagStalls() { return true; }

// Local RA/GRA
TR_RegisterKinds prepareRegistersForAssignment();

// GRA
//
bool prepareForGRA();
TR_GlobalRegisterNumber getLinkageGlobalRegisterNumber(int8_t linkageRegisterIndex, TR::DataType type);

TR_BitVector _globalRegisterBitVectors[TR_numSpillKinds];
virtual TR_BitVector *getGlobalRegisters(TR_SpillKinds kind, TR_LinkageConventions lc){ return &_globalRegisterBitVectors[kind]; }
virtual void simulateNodeEvaluation(TR::Node * node, TR_RegisterPressureState * state, TR_RegisterPressureSummary * summary);

TR_BitVector *getGlobalGPRsPreservedAcrossCalls(){ return &_globalGPRsPreservedAcrossCalls; }
TR_BitVector *getGlobalFPRsPreservedAcrossCalls(){ return &_globalFPRsPreservedAcrossCalls; }

TR_GlobalRegisterNumber getGlobalHPRFromGPR (TR_GlobalRegisterNumber n);
TR_GlobalRegisterNumber getGlobalGPRFromHPR (TR_GlobalRegisterNumber n);

bool considerTypeForGRA(TR::Node *node);
bool considerTypeForGRA(TR::DataType dt);
bool considerTypeForGRA(TR::SymbolReference *symRef);

// Number of assignable GPRs
int32_t getMaximumNumberOfAssignableGPRs();

using OMR::CodeGenerator::getMaximumNumberOfGPRsAllowedAcrossEdge;
int32_t getMaximumNumberOfGPRsAllowedAcrossEdge(TR::Node *);
int32_t getMaximumNumberOfFPRsAllowedAcrossEdge(TR::Node *);
int32_t getMaximumNumberOfVRFsAllowedAcrossEdge(TR::Node *);
int32_t getMaximumNumberOfGPRsAllowedAcrossEdge(TR::Block *);

int32_t getMaximumNumbersOfAssignableGPRs();
int32_t getMaximumNumbersOfAssignableFPRs();
int32_t getMaximumNumbersOfAssignableVRs();
bool allowGlobalRegisterAcrossBranch(TR_RegisterCandidate *, TR::Node *);
void setRealRegisterAssociation(TR::Register     *reg,
TR::RealRegister::RegNum realNum);
bool isGlobalRegisterAvailable(TR_GlobalRegisterNumber i, TR::DataType dt);

// Used to model register liveness without Future Use Count.
virtual bool isInternalControlFlowReg(TR::Register *reg);
virtual void startInternalControlFlow(TR::Instruction *instr);
virtual void endInternalControlFlow(TR::Instruction *instr) { _internalControlFlowRegisters.clear(); }

bool doRematerialization() {return true;}

void dumpDataSnippets(TR::FILE *outFile);
void dumpTargetAddressSnippets(TR::FILE *outFile);

bool getSupportsBitOpCodes() { return true;}

bool getSupportsImplicitNullChecks() { return _cgFlags.testAny(S390CG_implicitNullChecks); }
void setSupportsImplicitNullChecks(bool b) { _cgFlags.set(S390CG_implicitNullChecks, b); }

bool getConditionalMovesEvaluationMode() { return _cgFlags.testAny(S390CG_conditionalMovesEvaluation); }
void setConditionalMovesEvaluationMode(bool b) { _cgFlags.set(S390CG_conditionalMovesEvaluation, b); }

bool getEnableRIOverPrivateLinkage() { return _cgFlags.testAny(S390CG_enableRIOverPrivateLinkage); }
void setEnableRIOverPrivateLinkage(bool b) { _cgFlags.set(S390CG_enableRIOverPrivateLinkage, b); }

bool getCondCodeShouldBePreserved() { return _cgFlags.testAny(S390CG_condCodeShouldBePreserved); }
void setCondCodeShouldBePreserved(bool b) { _cgFlags.set(S390CG_condCodeShouldBePreserved, b); }

uint8_t getFCondMoveBranchOpCond() { return fCondMoveBranchOpCond; }
void setFCondMoveBranchOpCond(TR::InstOpCode::S390BranchCondition b) { fCondMoveBranchOpCond = (getMaskForBranchCondition(b)) & 0xF; }

uint8_t getRCondMoveBranchOpCond() { return 0xF - fCondMoveBranchOpCond; }

/** Support for shrinkwrapping */
bool processInstruction(TR::Instruction *instr, TR_BitVector **registerUsageInfo, int32_t &blockNum, int32_t &isFence, bool traceIt); // virt
uint32_t isPreservedRegister(int32_t regIndex);
bool isReturnInstruction(TR::Instruction *instr);
bool isBranchInstruction(TR::Instruction *instr);
bool isLabelInstruction(TR::Instruction *instr);
int32_t isFenceInstruction(TR::Instruction *instr);
bool isAlignmentInstruction(TR::Instruction *instr);
TR::Instruction *splitEdge(TR::Instruction *cursor, bool isFallThrough, bool needsJump, TR::Instruction *newSplitLabel, TR::list<TR::Instruction*> *jmpInstrs, bool firstJump = false);
TR::Instruction *splitBlockEntry(TR::Instruction *instr);
int32_t computeRegisterSaveDescription(TR_BitVector *regs, bool populateInfo = false);
void processIncomingParameterUsage(TR_BitVector **registerUsageInfo, int32_t blockNum);
void updateSnippetMapWithRSD(TR::Instruction *cur, int32_t rsd);
bool isTargetSnippetOrOutOfLine(TR::Instruction *instr, TR::Instruction **start, TR::Instruction **end);
bool canUseImmedInstruction(int64_t v);
void ensure64BitRegister(TR::Register *reg);

virtual bool isAddMemoryUpdate(TR::Node * node, TR::Node * valueChild);

bool globalAccessRegistersSupported();

// AR mode
bool getRAPassAR() {return false;}
void setRAPassAR();
void resetRAPassAR();

#ifdef DEBUG
void dumpPreGPRegisterAssignment(TR::Instruction *);
void dumpPostGPRegisterAssignment(TR::Instruction *, TR::Instruction *);

// Internal local RA counters for self evaluation
void clearTotalSpills()        {_totalColdSpills=0;        _totalHotSpills=0;       }
void clearTotalRegisterXfers() {_totalColdRegisterXfers=0; _totalHotRegisterXfers=0;}
void clearTotalRegisterMoves() {_totalColdRegisterMoves=0; _totalHotRegisterMoves=0;}

// current RA block is only valid during register allocation pass and is only used for debug
TR::Block * getCurrentRABlock()           { return _curRABlock; }
void setCurrentRABlock(TR::Block * block) { _curRABlock = block; }

void incTotalSpills();
void incTotalRegisterXfers();
void incTotalRegisterMoves();
void printStats(int32_t);
#endif

TR_S390ProcessorInfo *getS390ProcessorInfo() {return &_processorInfo;}

TR_S390OutOfLineCodeSection *findS390OutOfLineCodeSectionFromLabel(TR::LabelSymbol *label);

TR::Instruction *generateNop(TR::Node *node, TR::Instruction *preced=0, TR_NOPKind nopKind=TR_NOPStandard);

/** \brief
*     Inserts padding (NOP) instructions in the instruction stream.
*
*  \param node
*     The node with which the generated instruction will be associated.
*
*  \param cursor
*     The instruction following which the padding instructions will be inserted.
*
*  \param size
*     The size in number of bytes of how much padding to insert. This value can be one of 0, 2, 4, or 6.
*
*  \param prependCursor
*     Determines whether the padding instructions will be inserted before or after \param cursor.
*
*  \return
*     The last padding instruction generated, or \param cursor if \param size is zero.
*/
TR::Instruction* insertPad(TR::Node* node, TR::Instruction* cursor, uint32_t size, bool prependCursor);

struct TR_S390ConstantDataSnippetKey
{
void * c;
uint32_t size;
TR::Node *node;
};

struct constantHashInfo
{
static CS2::HashValue Hash(const TR_S390ConstantDataSnippetKey & key, const CS2::HashValue hv = CS2::CS2_FNV_OFFSETBASIS)
{
return CS2::Hash_FNV((const unsigned char*)key.c,key.size, hv);
}
static bool Equal(const TR_S390ConstantDataSnippetKey & key1,const TR_S390ConstantDataSnippetKey & key2)
{
if(key1.size != key2.size)
return false;
if (key1.node != key2.node)
return false;
switch (key1.size)
{
case 4:
return *((int32_t *) key2.c) == *((int32_t *) key1.c);
case 8:
return *((int64_t *)key2.c) == *((int64_t *) key1.c);
case 2:
return *((int16_t *)key2.c) == *((int16_t *)key1.c);
case 256:
return memcmp(key1.c, key2.c, 256) == 0;
}
return false;
}
};

typedef CS2::HashTable<TR_S390ConstantDataSnippetKey,TR::S390ConstantDataSnippet *, TR::Allocator, constantHashInfo> TR_ConstantSnippetHash;
typedef TR_ConstantSnippetHash::Cursor TR_ConstHashCursor;


/** First Snippet (can be AddressSnippet or ConstantSnippet) */
TR::Snippet *getFirstSnippet();
// Constant Data List functions
int32_t setEstimatedOffsetForConstantDataSnippets(int32_t targetAddressSnippetSize);
int32_t setEstimatedLocationsForDataSnippetLabels(int32_t estimatedSnippetStart);
void emitDataSnippets();
bool hasDataSnippets() { return (_constantList.empty() && _writableList.empty() && _snippetDataList.empty() && _constantHash.IsEmpty()) ? false : true; }
TR::list<TR::S390ConstantDataSnippet*> &getConstantInstructionSnippets() { return _constantList; }
TR::list<TR::S390ConstantDataSnippet*> &getConstantDataStringSnippets() { return _constantList; }
TR_ConstHashCursor getConstantDataSnippets() { return _constantHashCur;}
TR::S390ConstantDataSnippet * getConstantDataSnippet(CS2::HashIndex hi) { return _constantHash.DataAt(hi);}


TR::S390ConstantDataSnippet * create64BitLiteralPoolSnippet(TR::DataType dt, int64_t value);
TR::S390ConstantDataSnippet * createLiteralPoolSnippet(TR::Node * node);
TR::S390ConstantInstructionSnippet *createConstantInstruction(TR::CodeGenerator * cg, TR::Node *node, TR::Instruction * instr);
TR::S390ConstantDataSnippet *findOrCreateConstant(TR::Node *, void *c, uint16_t size);
TR::S390ConstantDataSnippet *findOrCreate2ByteConstant(TR::Node *, int16_t c, bool isWarm = 0);
TR::S390ConstantDataSnippet *findOrCreate4ByteConstant(TR::Node *, int32_t c, bool isWarm = 0);
TR::S390ConstantDataSnippet *findOrCreate8ByteConstant(TR::Node *, int64_t c, bool isWarm = 0);
TR::S390ConstantDataSnippet *Create4ByteConstant(TR::Node *, int32_t c, bool writable);
TR::S390ConstantDataSnippet *Create8ByteConstant(TR::Node *, int64_t c, bool writable);
TR::S390ConstantDataSnippet *CreateConstant(TR::Node *, void *c, uint16_t size, bool writable);
TR::S390ConstantDataSnippet *getFirstConstantData();

TR::S390LabelTableSnippet *createLabelTable(TR::Node *, int32_t);

// Writable Data List functions
bool hasWritableDataSnippets() { return _writableList.empty() ? false : true; }
TR::S390WritableDataSnippet *CreateWritableConstant(TR::Node *);

// OutOfLineCodeSection List functions
TR::list<TR_S390OutOfLineCodeSection*> &getS390OutOfLineCodeSectionList() {return _outOfLineCodeSectionList;}
TR_S390OutOfLineCodeSection *findOutLinedInstructionsFromLabel(TR::LabelSymbol *label);



TR::Instruction *generateDebugCounterBump(TR::Instruction *cursor, TR::DebugCounterBase *counter, int32_t delta, TR::RegisterDependencyConditions *cond);
TR::Instruction *generateDebugCounterBump(TR::Instruction *cursor, TR::DebugCounterBase *counter, TR::Register *deltaReg, TR::RegisterDependencyConditions *cond);
TR::Instruction *generateDebugCounterBump(TR::Instruction *cursor, TR::DebugCounterBase *counter, int32_t delta, TR_ScratchRegisterManager &srm);
TR::Instruction *generateDebugCounterBump(TR::Instruction *cursor, TR::DebugCounterBase *counter, TR::Register *deltaReg, TR_ScratchRegisterManager &srm);


// Snippet Data functions
void addDataConstantSnippet(TR::S390ConstantDataSnippet * snippet);

// Target Address List functions
int32_t setEstimatedOffsetForTargetAddressSnippets();
int32_t setEstimatedLocationsForTargetAddressSnippetLabels(int32_t estimatedSnippetStart);
void emitTargetAddressSnippets();
bool hasTargetAddressSnippets() { return _targetList.empty() ? false : true; }
TR::S390LookupSwitchSnippet  *CreateLookupSwitchSnippet(TR::Node *,  TR::Snippet* s);
TR::S390TargetAddressSnippet *CreateTargetAddressSnippet(TR::Node *, TR::Snippet* s);
TR::S390TargetAddressSnippet *CreateTargetAddressSnippet(TR::Node *, TR::LabelSymbol * s);
TR::S390TargetAddressSnippet *CreateTargetAddressSnippet(TR::Node *, TR::Symbol* s);
TR::S390TargetAddressSnippet *findOrCreateTargetAddressSnippet(TR::Node *, uintptrj_t s);
TR::S390TargetAddressSnippet *getFirstTargetAddress();

// Transient Long Registers

TR_Array<TR::Register *> &getTransientLongRegisters() {return _transientLongRegisters;}
void freeAndResetTransientLongs();
uint32_t registerBitMask(int32_t reg)
{
return 1 << (reg-1);
}

bool opCodeIsNoOpOnThisPlatform(TR::ILOpCode &opCode);

bool internalPointerSupportImplemented();
// list of branch unconditional relative instructions used for patching entry point
// need to be updated with correct offset to sampling preprologue
TR::list<TR::Instruction*> _recompPatchInsnList;

bool supportsFusedMultiplyAdd() {return true;}
bool supportsSinglePrecisionSQRT() {return true;}
bool supportsLongRegAllocation()
{
return TR::Compiler->target.isZOS() && TR::Compiler->target.is32Bit();
}

bool isAddressScaleIndexSupported(int32_t scale) { if (scale <= 2) return true; return false; }
using OMR::CodeGenerator::getSupportsConstantOffsetInAddressing;
bool getSupportsConstantOffsetInAddressing(int64_t value);

void enableLiteralPoolRegisterForGRA ();
void setOnDemandLiteralPoolRun(bool val)     { _cgFlags.set(S390CG_literalPoolOnDemandOnRun, val); }
bool isLiteralPoolOnDemandOn () { return _cgFlags.testAny(S390CG_literalPoolOnDemandOnRun); }
virtual void setGlobalStaticBaseRegisterOn(bool val)     { _cgFlags.set(S390CG_globalStaticBaseRegisterOn, val); }
virtual bool isGlobalStaticBaseRegisterOn () { return _cgFlags.testAny(S390CG_globalStaticBaseRegisterOn); }
virtual void setGlobalStaticBaseRegisterOnFlag();
virtual void setGlobalPrivateStaticBaseRegisterOn(bool val)     { _cgFlags.set(S390CG_globalPrivateStaticBaseRegisterOn, val); }
virtual bool isGlobalPrivateStaticBaseRegisterOn () { return _cgFlags.testAny(S390CG_globalPrivateStaticBaseRegisterOn); }

bool isAddressOfStaticSymRefWithLockedReg(TR::SymbolReference *symRef);
bool isAddressOfPrivateStaticSymRefWithLockedReg(TR::SymbolReference *symRef);

bool canUseRelativeLongInstructions(int64_t value);
bool supportsOnDemandLiteralPool();

bool supportsDirectIntegralLoadStoresFromLiteralPool();

void setSupportsHighWordFacility(bool val)  { _cgFlags.set(S390CG_supportsHighWordFacility, val); }
bool supportsHighWordFacility()     { return _cgFlags.testAny(S390CG_supportsHighWordFacility); }

void setCanExceptByTrap(bool val) { _cgFlags.set(S390CG_canExceptByTrap, val); }
virtual bool canExceptByTrap()    { return _cgFlags.testAny(S390CG_canExceptByTrap); }

TR_BackingStore *getLocalF2ISpill();

TR_BackingStore *getLocalD2LSpill();

// The reusable temp slot is an 8-byte scalar temp that gets mapped to the stack
// the first time allocateReusableTempSlot() is called.  This temp is meant to
// be used for short periods of time.
TR::SymbolReference* allocateReusableTempSlot();
void freeReusableTempSlot();

bool isActiveCompareCC(TR::InstOpCode::Mnemonic opcd, TR::Register* tReg, TR::Register* sReg);
bool isActiveArithmeticCC(TR::Register* tstReg);
bool isActiveLogicalCC(TR::Node* ccNode, TR::Register* tstReg);

bool mulDecompositionCostIsJustified(int32_t numOfOperations, char bitPosition[], char operationType[], int64_t value);

bool canUseGoldenEagleImmediateInstruction( int32_t value )
{
return true;
};

bool canUseGoldenEagleImmediateInstruction( int64_t value )
{
return value >= GE_MIN_IMMEDIATE_VAL && value <= GE_MAX_IMMEDIATE_VAL;
};

/** Checks if the immediate value can fit in 32-bit immediate field that is unsigned. */
bool canUseGoldenEagleUnsignedImmediateInstruction(int64_t value)
{
return value == (value & GE_MAX_UNSIGNED_IMMEDIATE_VAL);
}

/** LL: Sign extended the specified number of high order bits in the register. */
bool signExtendedHighOrderBits( TR::Node * node, TR::Register * targetRegister, uint32_t numberOfBits );

/** LL: Sign extended the specified number of high order bits in the register, use for 64 bits. */
bool signExtendedHighOrderBits( TR::Node * node, TR::Register * targetRegister, TR::Register * srcRegister, uint32_t numberOfBits );

/** LL: Zero filled the specified number of high order bits in the register. */
bool clearHighOrderBits( TR::Node * node, TR::Register * targetRegister, uint32_t numberOfBits );

/** LL: Zero filled the specified number of high order bits in the register, use for 64 bits. */
bool clearHighOrderBits( TR::Node * node, TR::Register * targetRegister, TR::Register * srcRegister, uint32_t numberOfBits );

void replaceInst(TR::Instruction* old, TR::Instruction* curr);

void deleteInst(TR::Instruction* old);

bool ilOpCodeIsSupported(TR::ILOpCodes);

void setUsesZeroBasePtr( bool v = true );
bool getUsesZeroBasePtr();

int16_t getMinShortForLongCompareNarrower() { return 0; }
int8_t getMinByteForLongCompareNarrower() { return 0; }

bool IsInMemoryType(TR::DataType type);

int32_t arrayTranslateMinimumNumberOfElements(bool isByteSource, bool isByteTarget) { if (isByteSource) return 25; else return 5; }

int32_t arrayTranslateAndTestMinimumNumberOfIterations() { return 4; }

/** Yank the scaling opp up a tree in lowerTrees */
bool yankIndexScalingOp() {return true;}

bool excludeInvariantsFromGRAEnabled();

/**
* array translate on TRex and below requires that the translate table be page-aligned
* for TRTO/TRTT and double-word aligned for TROT/TROO
* array translate takes awhile to initialize - for very short translations, the original
* loop of instructions should be driven instead of the translate instruction
* LL: For Golden Eagle, TRTO/TRTT is also double-word aligned.
*/
int32_t arrayTranslateTableRequiresAlignment(bool isByteSource, bool isByteTarget)
{
return 7;
}

// LL: move to .cpp
bool arithmeticNeedsLiteralFromPool(TR::Node *node);

// LL: move to .cpp
bool bitwiseOpNeedsLiteralFromPool(TR::Node *parent, TR::Node *child);

bool bndsChkNeedsLiteralFromPool(TR::Node *child);

bool constLoadNeedsLiteralFromPool(TR::Node *node);
virtual bool isDispInRange(int64_t disp);

bool getSupportsOpCodeForAutoSIMD(TR::ILOpCode, TR::DataType);

TR::Instruction *_ccInstruction;
TR::Instruction* ccInstruction() { return _ccInstruction; }
void setCCInstruction(TR::Instruction* cc) { _ccInstruction = cc; }

#define TR_DEFAULT_DATA_SNIPPET_EXPONENT 7
int32_t constantDataSnippetExponent() { return TR_DEFAULT_DATA_SNIPPET_EXPONENT; } // 1 << 7 = 128 byte max size for each constantDataSnippet


private:
TR_BitVector _globalGPRsPreservedAcrossCalls;
TR_BitVector _globalFPRsPreservedAcrossCalls;

TR::S390ImmInstruction          *_returnTypeInfoInstruction;
int32_t                        _extentOfLitPool;  // excludes snippets
uint64_t                       _availableHPRSpillMask;

TR::list<TR::S390TargetAddressSnippet*> _targetList;

protected:
TR::list<TR::S390ConstantDataSnippet*>  _constantList;
TR::list<TR::S390ConstantDataSnippet*>  _snippetDataList;

/**
* _processorInfo contains the targeted hardware level for the compilation.
* This may be different than the real hardware the JIT compiler is currently running
* on, due to user specified options.
*/
TR_S390ProcessorInfo            _processorInfo;

private:
TR::list<TR::S390WritableDataSnippet*>  _writableList;
TR::list<TR_S390OutOfLineCodeSection*> _outOfLineCodeSectionList;

CS2::HashTable<TR::Register *, TR::RealRegister::RegNum, TR::Allocator> _previouslyAssignedTo;

TR_ConstantSnippetHash _constantHash;
TR_ConstHashCursor     _constantHashCur;

TR_HashTab * _notPrintLabelHashTab;
TR_HashTab * _interfaceSnippetToPICsListHashTab;

TR::RegisterIterator            *_aRegisterIterator;
TR::RegisterIterator            *_hpRegisterIterator;
TR::RegisterIterator            *_vrfRegisterIterator;

TR_Array<TR::Register *>        _transientLongRegisters;

/** For aggregate type GRA */
bool considerAggregateSizeForGRA(int32_t size);

#ifdef DEBUG
uint32_t _totalColdSpills;
uint32_t _totalColdRegisterXfers;
uint32_t _totalColdRegisterMoves;
uint32_t _totalHotSpills;
uint32_t _totalHotRegisterXfers;
uint32_t _totalHotRegisterMoves;
TR::Block * _curRABlock;
#endif

bool  TR_LiteralPoolOnDemandOnRun;


/** Saves the preprologue offset to allow JIT entry point alignment padding. */
int32_t _preprologueOffset;

TR_BackingStore* _localF2ISpill;

TR_BackingStore* _localD2LSpill;

uint8_t fCondMoveBranchOpCond;

TR::SymbolReference* _reusableTempSlot;

TR::list<TR::Register *> _internalControlFlowRegisters;

CS2::HashTable<ncount_t, bool, TR::Allocator> _nodesToBeEvaluatedInRegPairs;

protected:
flags32_t  _cgFlags;

/** Miscellaneous S390CG boolean flags. */
typedef enum
{
// Available                       = 0x00000001,
S390CG_extCodeBaseRegisterIsFree   = 0x00000002,
// Available                       = 0x00000004,
S390CG_addStorageReferenceHints    = 0x00000008,
S390CG_isOutOfLineHotPath          = 0x00000010,
S390CG_literalPoolOnDemandOnRun    = 0x00000020,
S390CG_prefetchNextStackCacheLine  = 0x00000040,
S390CG_doesExit                    = 0x00000080,
// Available                       = 0x00000100,
S390CG_implicitNullChecks          = 0x00000200,
S390CG_reusableSlotIsFree          = 0x00000400,
S390CG_conditionalMovesEvaluation  = 0x00000800,
S390CG_usesZeroBasePtr             = 0x00001000,
S390CG_enableRIOverPrivateLinkage  = 0x00002000,
S390CG_condCodeShouldBePreserved   = 0x00004000,
S390CG_enableBranchPreload         = 0x00008000,
S390CG_globalStaticBaseRegisterOn  = 0x00010000,
S390CG_supportsHighWordFacility    = 0x00020000,
S390CG_canExceptByTrap             = 0x00040000,
S390CG_enableTLHPrefetching        = 0x00080000,
S390CG_enableBranchPreloadForCalls = 0x00100000,
S390CG_globalPrivateStaticBaseRegisterOn = 0x00200000
} TR_S390CGFlags;
private:

TR::Node *_nodeAddressOfCachedStatic;
protected:

TR::SparseBitVector _bucketPlusIndexRegisters;
TR::Instruction *_currentDEPEND;
};

}

}


class TR_S390Peephole
{
public:
TR_S390Peephole(TR::Compilation* comp, TR::CodeGenerator *cg);

void perform();

private:
void printInfo(const char* info)
{
if (_outFile)
{
if ( !( !comp()->getOption(TR_TraceCG) && comp()->getOptions()->getTraceCGOption(TR_TraceCGPostBinaryEncoding) && comp()->getOptions()->getTraceCGOption(TR_TraceCGMixedModeDisassembly) )  )
{
trfprintf(_outFile, info);
}
}
}

void printInst()
{
if (_outFile)
{
if ( !( !comp()->getOption(TR_TraceCG) && comp()->getOptions()->getTraceCGOption(TR_TraceCGPostBinaryEncoding) && comp()->getOptions()->getTraceCGOption(TR_TraceCGMixedModeDisassembly) )  )
{
comp()->getDebug()->print(_outFile, _cursor);
}
}
}

bool LLCReduction();
bool LGFRReduction();
bool AGIReduction();
bool ICMReduction();
bool replaceGuardedLoadWithSoftwareReadBarrier();
bool LAReduction();
bool NILHReduction();
bool duplicateNILHReduction();
bool unnecessaryNILHReduction();
bool clearsHighBitOfAddressInReg(TR::Instruction *inst, TR::Register *reg);
bool branchReduction();
bool forwardBranchTarget();
bool seekRegInFutureMemRef(int32_t ,TR::Register *);
bool LRReduction();
bool ConditionalBranchReduction(TR::InstOpCode::Mnemonic branchOPReplacement);
bool CompareAndBranchReduction();
bool LoadAndMaskReduction(TR::InstOpCode::Mnemonic LZOpCode);
bool removeMergedNullCHK();
bool trueCompEliminationForCompareAndBranch();
bool trueCompEliminationForCompare();
bool trueCompEliminationForLoadComp();
bool attemptZ7distinctOperants();
bool DeadStoreToSpillReduction();
bool tryMoveImmediate();
bool isBarrierToPeepHoleLookback(TR::Instruction *current);

/** \brief
*     Attempts to reduce LHI R,0 instructions to XR R,R instruction to save 2 bytes of icache.
*
*  \return
*     true if the reduction was successful; false otherwise.
*/
bool ReduceLHIToXR();

// DAA related Peephole optimizations
bool DAARemoveOutlinedLabelNop   (bool hasPadding);
bool DAARemoveOutlinedLabelNopCVB(bool hasPadding);

bool DAAHandleMemoryReferenceSpill(bool hasPadding);

bool revertTo32BitShift();
bool inlineEXtargetHelper(TR::Instruction *, TR::Instruction *);
bool inlineEXtarget();
void markBlockThatModifiesRegister(TR::Instruction *, TR::Register *);
void reloadLiteralPoolRegisterForCatchBlock();

TR::Compilation * comp() { return TR::comp(); }

private:
TR_FrontEnd * _fe;
TR::FILE *_outFile;
TR::Instruction *_cursor;
TR::CodeGenerator *_cg;
};

class TR_S390ScratchRegisterManager : public TR_ScratchRegisterManager
{
public:

TR_S390ScratchRegisterManager(int32_t capacity, TR::CodeGenerator *cg) : TR_ScratchRegisterManager(capacity, cg) {}
};

#endif
